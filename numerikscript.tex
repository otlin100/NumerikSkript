\documentclass[12pt]{article}


\usepackage{algorithmic} %Für Pseudocode https://math-linux.com/latex-26/faq/latex-faq/article/how-to-write-algorithm-and-pseudocode-in-latex-usepackage-algorithm-usepackage-algorithmic
\usepackage{stmaryrd} %Für Widerspruchsblitz
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage{amsthm} %Für Theoreme und Beweise
\usepackage{graphicx} %Für Bilder

\newtheoremstyle{break}% name
  {}%         Space above, empty = `usual value'
  {}%         Space below
  {\normalfont}% Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\bfseries}% Thm head font
  {.}%        Punctuation after thm head
  {\newline}% Space after thm head: \newline = linebreak
  {}%         Thm head spec

\theoremstyle{break}

\renewcommand{\thesection}{\Roman{section}}
\counterwithout{subsection}{section}
%\renewcommand{\thesubsection}{\arabic{subsection}}

%Definiere Satz, Definition,...
\newtheorem{theorem}{Satz}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{korollar}[theorem]{Korollar}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{algorithm}[theorem]{Algorithmus}
\newtheorem{comment}[theorem]{Bemerkung}
\newtheorem*{comment*}{Bemerkung}
\newtheorem*{example*}{Beispiel}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{example}[theorem]{Beispiel}
\newtheorem{nothing}[theorem]{}

\author{Prof. Schaedle}
\title{Numerik 1}

\begin{document}
\maketitle

\newpage

\section{Numerische Integration}

\input{Kapitel_1/Abschnitt_1} %Einführung
\input{Kapitel_1/Abschnitt_2} %Ordnung von Quadraturformeln
\input{Kapitel_1/Abschnitt_3} %Quadraturfehler
\input{Kapitel_1/Abschnitt_4} %Quadratur mit hoher Ordnung
\input{Kapitel_1/Abschnitt_5} %Orthogonalpolynome
\input{Kapitel_1/Abschnitt_6} %Ein adaptives Programm
\input{Kapitel_1/Abschnitt_7} %Gauß- und Loballo Quadraturformeln

\section{Interpolation und Approximation}

\begin{description}
  \item[Problemstellung A]
    Zu gegebenen $(x_0, y_0), ...,(x_n, y_n)$ berechne Polynom $p$ vom Grad $\leq n$ mit $$p(x_j) = y_j, \quad j=0,...,n$$
  
  \item[Problemstellung B]
    $f:[a,b] \rightarrow \mathbb{R}$ gegeben. Finde einfach auszuwertende Funktion $p: [a,b] \rightarrow \mathbb{R}$, etwa ein Polynom, stückweises Polynom, rationale Funktion, sodass $f-p$ klein ist.
    \begin{enumerate}
      \item[i)] $f(x)=p(x)$ für endlich viele vorgegebene Punkte $x$
      \item[ii)] $\int_a^b (f(x)-p(x))^2 dx$ soll minimal sein.
      \item[iii)] $\max_{x \in [a,b]} \vert f(x) -p(x) \vert$ soll minimal sein.
    \end{enumerate}
\end{description}

\subsection{Newtonsche Interpolationsformel}

\begin{example}
\begin{description}\item \end{description}
\begin{description}
  \item n=1: \\
    $(x_0, y_0),(x_1,y_1)$, $p \in \mathcal{P}_1$ das beide Punkte verbindet.\\
    $$p(x) = y_0 + (x-x_0) \frac{y_1-y_0}{x_1-x_0}$$
  \item n=2: \\
    $(x_0, y_0),(x_1,y_1),(x_2,y_2)$ \\
    $$p(x) = y_0 + (x-x_0) \frac{y_1-y_0}{x_1-x_0} + a(x-x_0)(x-x_1)$$
    Bestimme $a$ so, dass $p(x_2) = y_2$
    \begin{flalign*}
    y_2 &\overset{!}{=} y_0 + (\overset{-x_1+x_1}{\check{x_2\thinspace-}}x_0) \frac{y_1-y_0}{x_1-x_0} + a(x_2-x_0)(x-x_1)&\\
    a(x_2-x_0)(x_2-x_1) &= y_2 - y_0 - (x_2-x_1) \frac{y_1-y_0}{x_1-x_0} - y_1 + y_0 &\\
    \Rightarrow a &= \frac{1}{x_2-x_0} \left( \frac{y_2-y_1}{x_2-x_1} - \frac{y_1-y_0}{x_1-x_0} \right) 
     \end{flalign*}
\end{description}
\end{example}

\begin{definition}[dividierte Differenzen]
Für $(x_0,y_0), (x_1, y_1), ..., (x_n, y_n)$ mit paarweise verschiedenen Stützstellen $x_j$ definieren wir
\begin{flalign*}
y[x_j] &:= y_j \quad \left( = \delta^0 y[x_j] \right) &\\
\delta y[x_j, x_{j+1}] &:= \frac{y_{j+1} - y_j}{x_{j+1}-x_j} = \frac{\delta^0 y[x_{j+1}]-\delta^0 y[x_{j}]}{x_{j+1} - x_j} &\\
\delta ^2 y[x_j, x_{j+1}, x_{j+2}] &:= \frac{\delta y[x_{j+1}, x_{j+2}]-\delta y[x_{j}, x_{j+1}]}{x_{j+2} - x_j} &\\
\delta ^k y[x_j, x_{j+1},..., x_{j+k}] &:= \frac{1}{x_{j+k}-x_j} \left( \delta^{k-1} y[x_{j+1}, ..., x_{j+k}] - \delta^{k-1} y[x_j, ..., x_{j+k-1}] \right)
\end{flalign*}
\underline{Schema:}\\
\begin{tabular}{ccccc}
 
$x_0$ & $y_0$& & &\\
 & & $\delta^1y[x_0, x_1]$ & &\\
$x_1$ & $y_1$ & &$\delta^2y[x_0, x_1, x_2]$ &\\
 & & $\delta^1y[x_1, x_2]$ & & $\delta^3y[x_0, x_1, x_2, x_3]$\\
$x_2$ & $y_2$ & & $\delta^2y[x_1, x_2, x_3]$ &\\
 & & $\delta^1y[x_2, x_3]$ & &\\
$x_3$ & $y_3$ & & &\\
 
\end{tabular}
\end{definition}

\begin{comment}
Falls die $x_i$ äquidistant, dh. $x_i = x_0+ih$ so ist: \\
\begin{flalign*}
\delta y[x_i, x_{i+1}] &= \frac{y_{i+1} - y_i}{h} =: \frac{1}{h} \Delta y_i &\\
\delta ^2 y[x_i, x_{i+1}, x_{i+2}] &= \frac{\frac{1}{h} \Delta y_{i+1} - \frac{1}{h} \Delta y_{i}}{2h} = \frac{1}{2h^2} \Delta ^2 y_i &\\
\delta ^k y[x_i, ..., x_{i+k}] &= \frac{1}{k!h^k} \Delta^k y_i,
\end{flalign*}
    wobei $\Delta^{k}y_i := \Delta^{k-1}y_{i+1} - \Delta^{k-1}y_i$.
\end{comment}

\begin{theorem}[Newtonsche Interpolationsformel]
Zu paarweise verschiedenen reellen $x_i$, $i=0,..., n$, existiert ein eindeutiges Polynom $p \in \mathcal{P}_n$ durch die Punkte $(x_i, y_i)$, $i=0,...,n$ (d.h. $p(x_i) = y_i$ für $i=1,...,n$). Es lässt sich berechnen durch:
\begin{flalign*}
p(x) &= y[x_0] + (x-x_0) \delta y[x_0,x_1] + ... + (x-x_0)(x-x_1)...(x-x_{n-1}) \delta ^n y[x_0, ..., x_n] &\\
&= \sum_{i=0}^n \prod_{j=0}^{i-1} (x-x_j) \delta^iy[x_0,...,x_i] 
\end{flalign*}
\begin{proof}[Beweis](Induktion)\phantom{\qedhere}
\begin{description}
  \item[IA] $n=1$ (und $n=2$) vgl. Beispiel (1.1)
  \item[IS] $n-1 \rightarrow n$\\
    $$p_0(x) = y[x_0] + (x-x_0) \delta y[x_1, x_0] + ... + (x-x_0)...(x-x_{n-2}) \delta ^{n-1}y[x_0,..., x_{n-1}]$$ 
    ist das eindeutige interpolierende Polynom mit 
    $$\text{deg}(p_0) \leq n-1$$
    zu $(x_0,y_0), (x_1, y_1), ..., (x_{n-1}, y_{n-1})$. \\
    Für den Ansatz
    $$p(x) = p_0(x) + a(x-x_0)(x-x_1)...(x-x_{n-1})$$
    ergibt die Forderung $p(x_n) = y_n$
    $$a = \frac{y_n-p_0(x_n)}{(x_n-x_0)(x_n-x_1)...(x_n-x_{n-1})}$$
    Da $a$ eindeutig ist, ist $p$ eindeutig.\\
    Es bleibt zu zeigen: $a = \delta^n y[x_0, ..., x_n]$\\
    Sei dazu ein Polynom $p_1(x)$, welches durch $(x_1, y_1), ..., (x_n, y_n)$ läuft, mit $\text{deg}(p_1) \leq n-1$. Nach Induktionsannahme gilt 
    \begin{flalign*}
    p_1(x) &= y[x_1] + (x-x_1) \delta^1y[x_1, x_2] + ... + (x-x_1)...(x-x_{n-1}) \delta ^{n-1}y[x_1, ..., x_n]&\\
    &=x^{n-1} \delta^{n-1}y[x_1, ..., x_n] + r
    \end{flalign*}
    mit $\text{deg}(r) \leq n-2$.\\
    Setze Polynom
    $$p(x) := \frac{x_n-x}{x_n-x_0} p_0(x) + \frac{x-x_0}{x_n-x_0}p_1(x)$$
    mit $\text{deg}(p) \leq n$ durch $(x_0, y_0), ..., (x_n, y_n)$. \\
    Das gilt, da: 
    \begin{flalign*}
    p(x_0) &= p_0(x_0) = y_0 &\\
    p(x_n) &= p_1(x_n) = y_n &\\
    \text{Für } i=1,...,n-1: &\\
    p(x_i) &= \frac{x_n-x_i}{x_n-x_0} \underbrace{p_0(x_i)}_{y_i} + \frac{x_i-x_0}{x_n-x_0} \underbrace{p_1(x_i)}_{y_i} = y_i
    \end{flalign*}
    Andererseits:
    $$p(x) = ax^n + r \quad \text{mit  deg}(r) \leq n-1$$
    Koeffizientenvergleich:
    \begin{align*}
    a &= - \frac{1}{x_n-x_0} \delta^{n-1}y[x_0, ..., x_{n-1}] + \frac{1}{x_n-x_0} \delta^{n-1}y[x_1, ..., x_{n}] &\\
    &= \delta^n y[x_0, ..., x_n]&\\\tag*{\qed}
    \end{align*}
    
\end{description}
\end{proof}
\end{theorem}

\begin{nothing}[Hornerschema]
Zur Auswertung des Interpolationspolynom $p$ an der Stelle $x$ verwendet man 
$$
p(x) = y[x_0] + (x-x_0) \left( \delta y[x_0, x_1] + (x-x_1) \left( \delta ^2 y[x_0, x_1, x_2] + (x-x_2) \left(  ... \left( \delta^n y[x_0, ..., x_n] \right) \right) \right) \right)
$$
\underline{Algorithmus:}
\begin{algorithmic}
\STATE $s = \delta^n y[x_0, ..., x_n]$
\FOR{$k = n-1, ...,0$}
\STATE $s = \delta^k y[x_0, ..., x_k] + (x-x_k) s$
\ENDFOR
\end{algorithmic}
\end{nothing}

\begin{example}
\begin{tabular}{||l|l|l|rrrr||}
\hline
$i$ & $x_i$ & $y_i$ & $\delta^1 y[x_0, x_1]$& $\delta^2 y[x_0, x_1, x_2]$& $\delta^3 y[x_0, .., x_3]$& $\delta^4 y[x_0, .., x_4]$\\
\hline
$0$ & $-1$ & $0$&&&&\\
& & & $\frac{1-0}{0-(-1)} = 1$&&&\\
$1$ & $0$ & $1$ & & $\frac{0-1}{2-(-1)} = -\frac{1}{3}$&&\\
& & & $0$ & & $\frac{\frac{2}{3} - (-\frac{1}{3})}{3-(-1)} = \frac{1}{4}$&\\
$2$ & $2$ & $1$ & & $\frac{2-0}{3-0} = \frac{2}{3}$ && $\frac{-\frac{2}{5} - \frac{1}{4}}{5-(-1)} = -\frac{13}{120}$\\
& & & $\frac{3-1}{3-2} = 2$ & & $\frac{-\frac{4}{3} - \frac{2}{3}}{5-0} = -\frac{2}{5}$&\\
$3$ & $3$ & $3$ & & $\frac{-2-2}{5-2} = -\frac{4}{3}$&&\\
& & & $\frac{-1-3}{5-3} = -2$&&&\\
$4$ & $5$ & $-1$&&&&\\
\hline
\end{tabular} \\\\
Das Interpolationspolynom ist also
$$p(x) = 0 + (x+1) *1 - \frac{1}{3}(x+1)(x) + \frac{1}{4}(x+1)x(x-2)+(x+1)x(x-2)(x-3)\left(-\frac{13}{120}\right)$$
bzw. nach Hornerschema
\begin{flalign*}
p(x) &= 0+(x+1)\left(1+ x\left(-\frac{1}{3} + (x-2)\left(\frac{1}{4} + (x-3)\left(-\frac{13}{120}\right)\right)\right)\right) &\\
\end{flalign*}
Werte $p(x)$ an der Stelle 1 aus:
\begin{flalign*}
-\frac{13}{120}* (-2) &= \frac{26}{120} &\\
\left( \frac{1}{4} + \frac{26}{120} \right)(-1) &= -\frac{56}{120} = -\frac{7}{15} &\\
\left( -\frac{7}{15} - \frac{1}{3}\right) 1 &= -\frac{12}{15} = -\frac{4}{5} &\\
\left(-\frac{4}{5} + 1\right)2 &= \frac{2}{5} = p(1) &\\
\end{flalign*}
\end{example}

\subsection{Fehler bei der Polynominterpolation}

\underline{Problem:} $f: \thinspace [a,b] \rightarrow \mathbb{R}$ werde interpoliert in Stützstellen $x_0, ..., x_n \in [a,b]$ durch $p \in \mathcal{P}_n$ mit $p(x_i) = f(x_i)$ für $i=0,...,n$. \\
Wie groß ist der Fehler $f(x)-p(x)$?

\begin{theorem}
Sei $f: [a,b] \rightarrow \mathbb{R}$ (n+1)-mal stetig differenzierbar, $p \in \mathcal{P}_n$ mit $p(x_i) = f(x_i)$ ($i=0,...,n$) das Interpolationspolynom zu paarweise verschiedenen Stützstellen $x_i \in [a,b]$ ($i=0,..., n$). Dann gilt: \\
\[\forall x \in [a,b] \exists \xi = \xi(x) \in (a,b): f(x)-p(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^n (x-x_i) \]
\begin{proof}[Beweis]
Siehe (9.4)
\end{proof}
\end{theorem}

\begin{example}[Berechnung von Logarithmentafeln: Briggs, 17. Jhd]
$f(x) = log_{10}(x), \quad x \in [55, 58]$\\
Wähle Stützstellen:\\
$x_0 = 55, \quad x_1 = 56, \quad x_2 = 57, \quad x_3=58$ \\
Es seien \[log_{10}(55), \medspace log_{10}(56), \medspace log_{10}(57) \text{ und } log_{10}(58)\] bereits bekannt. Berechne eine Näherung von \(f\) an bei \(log_{10}(56.5)\) \\
$\rightarrow$ Interpolationspolynom $p$:
\begin{flalign*}
log_{10}(65.5) &= 1.752048448 &\\
p(56.5) &= 1.75204845 &\\
f'(x) &= \frac{1}{ln(10)x} &\\
f''(x) &= -\frac{1}{ln(10)x^2} &\\
f^{(3)}(x) &= \frac{2}{ln(10)x^3} &\\
f^{(4)}(x) &= -\frac{6}{ln(10)x^4} &\\
\text{Für } x\in[55, 58]: &\\
\vert f^{(4)}(x)\vert &\leq \frac{6}{55^4 ln(10)}
\Rightarrow &\\
\vert log_{10}(56.5) - p(56.5) \vert &\leq 1.5 * 0.5 * 0.5 * 1.5 * \frac{6}{55^4ln(10) \frac{1}{4!}} &\\
&\approx 6.7 * 10^{-9}
\end{flalign*}
\end{example}

Für den Beweis von (9.1) wird folgendes Lemma benötigt:

\begin{lemma}
Sei $f \in C^n([a,b])$ und sei für paarweise verschiedene $x_i \in [a,b]$ ($i=0,...,n$) $y_i := f(x_i)$. Dann existiert $\xi \in (\min_i(x_i), \max_i(x_i))$, sodass 
$$\delta^ny[x_0,..., x_n] = \frac{f^{(n)}(\xi)}{n!}\quad (x_0 < x_1 < ... < x_n)$$
\begin{proof}[Beweis]
Sei $p$ ein Interpolationspolynom zu $(x_i, y_i)_{i=0}^n$. Setzt man $d:= p-f$, so gilt $d(x_i) = 0$ für $i=0,...,n$.\\
n-maliges anwenden des Mittelwertsatzes liefert paarweise verschiedene $\xi_i$, $(i=0, ...,n-1)$ mit $d'(\xi_i) = 0$ für $\xi_i \in (\min_j(x_j), \max_j(x_j))$. \\
Dasselbe Argument angewandt auf $d'$ liefert $\eta_0,..., \eta_{n-2}$ mit $d''(\eta_i) = 0$ für $i=0,..., n-2$.\\
Wiederhole dies bis:\\
Es existiert $\rho_0$ mit $d^{(n)}(\rho_0) = 0$\\
$\Rightarrow f^{(n)}(\rho_0) = p^{(n)}(\rho_0) = n! \delta^ny[x_0,..., x_n],$\\
da $\delta^ny[x_0, ..., x_n]$ der Koeffizient von $x^n$ in $p$ ist.
\end{proof}
\end{lemma}

\begin{comment*}
Für $n=1$ ist Lemma (9.3) der Mittelwertsatz (oder Satz von Rolle) aus Ana I:
$$ \exists \xi: \frac{f(x_1)-f(x_2)}{x_1-x_2} = f'(\xi)$$
\end{comment*}

\begin{nothing}[Beweis von (9.1)]
Sei $\bar{x} \in [a,b]$ beliebig.
\begin{description}
  \item[1. Fall] $\bar{x} = x_i$ für ein $i \in \{0,...,n\}$, so ist wegen $p(x_i) - f(x_i) = 0$ nichts zu zeigen.
  
  \item[2. Fall] $\bar{x} \neq x_i$ für alle $i \in \{0,...,n\}$. Sei $\bar{p}$ das Interpolationspolynom mit $\text{deg}(\bar{p}) \leq n+1$ zu $(x_i, f(x_i))_{i=0}^n$ und $(\bar{x}, f(\bar{x}))$. Die Newton'sche Interpolationsformel liefert dann
  \begin{align*}
  \bar{p}(x) &= p(x) + \prod_{i=0}^n(x-x_i)\delta^{n+1}y[x_0,...,x_n, \bar{x}] &\\
  &\underset{(9.3)}{=} p(x) + \prod_{i=0}^n(x-x_i) \frac{f^{(n+1)}(\xi)}{(n+1)!}
  \end{align*}
  Für $x = \bar{x}$ gilt $\bar{p}(\bar{x}) = f(\bar{x})$. Damit ist Satz (9.1) für $x\in[a,b]$ gezeigt.\qed
\end{description}
\end{nothing}

\underline{Fragen:}
\begin{itemize}
  \item Für welche Wahl der Stützstellen $x_i$ ($i=0,...,n$, n fest) ist 
    $$\max_{x\in[a,b]} \vert \prod_{i=0}^n(x-x_i)\vert$$
    minimal? (Siehe Abschnitt 10)
  \item Wie wirken sich Fehler in den Funktionsauswertungen (etwa Messfehler oder Rechenfehler) auf das Interpolationspolynom aus?
\end{itemize}

\begin{theorem}[Lagrange Interpolationsformel]
Das Interpolationspolynom $p$ zu $(x_i, y_i)_{i=0}^n$ ist gegeben durch 
$$p(x) = \sum_{i=0}^n y_il_i(x)$$
mit 
$$l_i(x) = \frac{\prod_{j=0,\thinspace j\neq i}^n (x-x_j) }{\prod_{j=0,\thinspace j\neq i}^n ( x_i-x_j)}$$
\begin{proof}[Beweis]
$\text{deg}(l_i) = n$, $l_i(x_j) = \left\{
\begin{array}{ll}
1 & \textrm{für } j=i \\
0 & \, \textrm{sonst} \\
\end{array}
\right.$ \\
$\Rightarrow p(x_j) = \sum_{i=0}^n y_il_i(x_j) = y_j$
\end{proof}
\end{theorem}

\begin{comment*}
Lagranges und Newtons Interpolationsformeln liefern beide das gleiche Polynom nur in unterschiedlichen Darstellungen.
\end{comment*}

\begin{definition}
$$\Lambda_n := \max_{x\in[a,b]} \sum_{i=0}^n\vert l_i(x) \vert$$
heißt die \textbf{Lebesgue Konstante} zu den Stützstellen $x_i$, $i=0,...,n$ auf dem Intervall $[a,b]$.
\end{definition}

Damit gilt:

\begin{theorem}
Sei $p$ das Interpolationspolynom (vom Grad $\leq n$) zu $(x_i, y_i)_{i=0}^n$ und $\tilde{p}$ das Interpolationspolynom zu $(x_i, \tilde{y}_i)_{i=0}^n$, so gilt:
$$ \max_{x \in [a,b]} \vert p(x) - \tilde{p}(x) \vert \leq \Lambda_n \max_{i=0,...,n}\vert y_i - \tilde{y}_i \vert $$
\begin{proof}[Beweis]
klar
\end{proof}
\end{theorem}

\begin{example}
\begin{description}\item \end{description}
\begin{itemize}
  \item Für äquidistante Stützstellen $x_i = a + i\frac{b-a}{n}$ ($i=0,...,n$) ist 
    \begin{align*}
    \Lambda_{10} &\approx 40 &\\
    \Lambda_{20} &\approx 3*10^4 &\\
    \Lambda_{40} &\approx 10^{10} &\\
    \Lambda_{n} &\approx \frac{2^n}{ln(n)*e*n} \quad \text{für } n \rightarrow \infty&\\
    \end{align*}
    $\Rightarrow$ Vorsicht bei Polynominterpolation mit vielen äquidistanten Stützstellen! \\
    In $\S$10 werden wir Stützstellen kennenlernen mit $\Lambda_n \leq 4$ für $n \leq 100$.
\end{itemize}
\end{example}

\begin{theorem}
Sei $f: [a,b] \rightarrow \mathbb{R}$ stetig, $p$ Interpolationspolynom zu $f$ in den Stützstellen $x_0,..., x_n \in [a,b]$. So gilt:
\[ \forall q \in \mathcal{P}_{n+1}: \quad \max_{x \in [a,b] } \vert f(x) -p(x) \vert \leq (1+ \Lambda_n) \max_{x \in [a,b]} \vert q(x) - f(x) \vert .\]
Hierbei ist $\Lambda_n$ die Lebesgue-Konstante zu $(x_i)_{i=0}^n$ auf $[a,b]$.
\begin{proof}[Beweis]
Sei $q \in \mathcal{P}$.
\[ f-p = (f-q) + (q-p)\]
$q$ ist das Interpolationspolynom zu sich selbst in den $x_0,..., x_n$. Nach (9.7) gilt für $y_i = f(x_i)$ $\tilde{y_i}  = q(x_i)$.
\begin{align*}
\max_{x \in [a,b]} \vert p(x) - q(x) \vert &\leq \Lambda_n \max_{i=0,...,n} \vert f(x_i) - q(x_i) \vert &\\
&\leq \Lambda_n \max_{x \in [a,b]} \vert f(x)-q(x) \vert &\\
\Rightarrow \max_{x \in [a,b]} \vert f(x) - p(x) \vert &\leq \max_{x \in [a,b]} \vert f(x)-q(x) \vert + \max_{x \in [a,b]} \vert p(x)-q(x) \vert &\\
&\leq (1+ \Lambda_n) \max_{x \in [a,b]} \vert q(x) - f(x) \vert 
\end{align*}
\end{proof}
\end{theorem}

\subsection{Tschebyscheff-Interpolation}

\underline{Ziel:} Interpoliere $f: [a,b] \rightarrow \mathbb{R}$ in "guten" Stützstellen. \\
Ohne Einschränkungen sei $[a,b] = [-1, 1]$

\begin{definition}
$T_n(x) = cos(n * arccos(x))$ für $x \in [-1, 1]$ heißt n-tes Tschebyscheff-Polynom.
\end{definition}

\begin{lemma}
$T_n(x)$ ist für $x \in [-1, 1]$ ein Polynom mit folgenden Eigenschaften:
\begin{enumerate}
  \item[i)] $T_0(x) = 1$, $T_1(x) = x$
  \item[ii)] $T_n(x) = 2^{n-1} x^n + r(x)$ mit $r_n \in \mathcal{P}_{n-1}$
  \item[iii)] $T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)$
  \item[iv)] $\forall x \in [-1, 1]: \quad \vert T_n(x) \vert \leq 1$
  \item[v)] $T_n(cos(\frac{k\pi}{n})) = (-1)^k, \quad k=0,...,n$
  \item[vi)] $T_n(cos(\frac{(2k+1)\pi}{2n})) = 0, \quad k=0,...,n-1$ 
\end{enumerate}
\begin{proof}[Beweis]\leavevmode
\begin{enumerate}
  \item[i)] klar, da $T_0(x) = cos(0) = 1$, $T_1(x) = cos(arccos(x)) = x, \quad x \in [a,b]$
  \item[iii)]$cos((n+1)\phi) + cos((n-1)\phi)$\\
    $= \medspace cos(n\phi)cos(\phi) - sin(n\phi)sin(\phi) + cos(n\phi)cos(-\phi) - sin(n\phi)sin(-\phi)$ \\
    $= \medspace 2cos(n\phi)cos(\phi)$
  \item[ii)] folgt aus i) und iii)
  \item[iv)] klar, da $cos: [-1, 1] \rightarrow \mathbb{R}$
  \item[v) + vi)] ebenfalls klar, da $T_n(cos(\frac{k\pi}{n})) = cos(n \frac{k\pi}{n}) = cos(k\pi) = (-1)^k$\\
  analog: $T_n(cos(\frac{(2k+1)\pi}{2n})) = cos(n \frac{(2k+1)\pi}{2n})  = 0$
\end{enumerate}
\end{proof}
\end{lemma}

\begin{example} \leavevmode
\begin{description}
  \item $T_2(x) = 2x^2 - 1$
  \item $T_3(x) = 2x(2x^2 -1) - x = 4x^3 -3x$
\end{description}
\end{example}

\begin{lemma}
Sei $q \in \mathcal{P}_n$, $q(x) = 2^{n-1} x^n + r(x)$ mit $r(x) \in \mathcal{P}_{n-1}, q \neq T_n$. Dann gilt
\[ \max_{x\in [-1, 1]} \vert q(x) \vert > \max_{x\in [-1, 1]} \vert T_n(x) \vert \quad(=1)\]

\begin{proof}[Beweis]\leavevmode
Annahme: $\forall x \in [-1, 1]: \quad \vert q(x) \vert \leq 1$
\begin{description}
  \item $T_n(1) = 1$
  \item $T_n(cos(\frac{\pi}{n})) = -1$
\end{description}
Nach dem Zwischenwertsatz hat $q-T_n$ eine Nullstelle im Intervall $[cos(\frac{\pi}{n}), 1]$. Falls ein "Randpunkt" $x$ eine Nullstelle ist, so handelt es sich um eine doppelte Nullstelle, da $q'(x) = 0 = T_n'(x)$. Ebenso existiert in $[cos(\frac{2\pi}{n}), cos(\frac{\pi}{n})]$ und allgemein in $[cos(\frac{(k+1)\pi}{n}), cos(\frac{k\pi}{n})]$ für $k=0,...,n-1$.\\
Nullstelle $\Rightarrow$ $q-T_n$ hat n Nullstellen.\\
Andererseits ist $q-T_n \in \mathcal{P}_{n-1} \quad \Rightarrow q-T_n \equiv 0 \quad \Rightarrow q = T_n \quad \lightning$
\end{proof}
\end{lemma}

\begin{theorem}
Unter allen Unterteilungen $\{x_0,...,x_n\}$ von $[-1,1]$ wird 
$$ \max_{x \in [-1, 1]} \vert (x-x_0)(x-x_1)...(x-x_n) \vert $$
minimal für $x_k = cos\left( \frac{2k+1}{n+1} \frac{\pi}{2}\right), \quad k=0,...,n$ (d.h. $x_k$ sind die Wurzeln von $T_{n+1}$)

\begin{proof}[Beweis]
Nach Lemma (10.4) wird $\max_{x \in [-1, 1]} \vert (x-x_0)(x-x_1)...(x-x_n) \vert$ minimal gdw.  $(x-x_0)...(x-x_n) = 2^{-n} T_{n+1}(x)$, d.h. falls $x_k$ Wurzeln von $T_{n+1}$ sind.
\end{proof}
\end{theorem}

\begin{theorem}
Die Lebesguekonstanten $\Lambda_n$ zu den Tschebyscheffknoten (Wurzeln von $T_{n+1}$) erfüllen
\begin{description}
  \item $\Lambda_n \leq 3$ für $n\leq 20$
  \item $\Lambda_n \leq 4$ für $n\leq 100$
  \item $\Lambda_n \approx \frac{2}{\pi} log(n)$ für $n \rightarrow \infty$
\end{description}
\begin{proof}[Beweis]
ohne Beweis.
\end{proof}
\end{theorem}
Nach Satz (9.9) liefert die Interpolation in den Wurzeln der Tschebyscheffpolynome eine fast optimale Polynominterpolation an $f$.\\
Dazu kommen Eigenschaften, die die Berechnung eines Interpolationspolynoms in den Tschebyscheffknoten (Wurzeln der Tschebyscheffpolynome) vereinfachen.

\begin{lemma}
Die Tschebyscheffpolynome sind orthogonal, bzgl. des Skalarprodukts 
\[ \langle f, g \rangle := \int_{-1}^1 f(x)g(x) \frac{1}{\sqrt{1-x^2}} dx\]

\begin{proof}[Beweis]
Übungsaufgabe
\end{proof}
\end{lemma}

\begin{lemma}
Die Tschebyscheffpolynome $T_k, k = 0, ..., n$ sind orthogonal bzgl. des Skalarprodukts (auf $\mathcal{P}_n$)
\[(f, g) := \sum_{l=0}^n f(x_l)g(x_l), \quad \text{wobei } x_0, ..., x_n \medspace \text{Wurzeln von } T_{n+1}(x) \]

\begin{proof}[Beweis]\leavevmode
\begin{align*}
T_k(x_l) &= \text{cos}(k *\text{arccos}( \text{cos} ( \frac{2l-1}{n+1} \frac{\pi}{2}))) &\\
&= \text{cos}( k \frac{2l+1}{n+1} \frac{\pi}{2}) &\\
&= \text{cos}(k (l+ \frac{1}{2})h )
\end{align*}
für $h = \frac{\pi}{n+1}$\\
Damit ist 
\begin{align*}
(T_k, T_j) &= \sum_{l=0}^n \text{cos}(k(l+\frac{1}{2})h) * \text{cos}(j(l+\frac{1}{2})h), &\\
\intertext{da $\text{cos}(x)\text{cos}(y) = \frac{1}{2}(\text{cos}(x+y) + \text{cos}(x-y))$}
&= \frac{1}{2} \sum_{l=0}^n \text{cos}((k+j)(l+\frac{1}{2})h) * \text{cos}((k-j)(l+\frac{1}{2})h) &\\
\intertext{Es gilt: $\text{cos}(x) = \text{Re} (e^{ix})$}
&= \frac{1}{2} \text{Re}\left( \sum_{l=0}^n e^{i(k+j)(l+\frac{1}{2})h} + e^{i(k-j)(l+\frac{1}{2})h}\right) &\\
&= \frac{1}{2} \text{Re}\left( \sum_{l=0}^n \left( e^{i(k+j)lh}e^{i(k+j)\frac{h}{2}} + e^{i(k-j)lh}e^{i(k-j)\frac{h}{2}}\right)\right) &\\
&= \frac{1}{2} \text{Re} \left( e^{i(k+j)\frac{h}{2}} \frac{e^{i(k+j)h(n+1)} -1}{ e^{i(k+j)h} - 1} + e^{i(k-j)\frac{h}{2}} \frac{e^{i(k-j)h(n+1)} -1}{ e^{i(k-j)h} - 1}\right), \quad \text{für }k \neq j &\\
\intertext{Es gilt $k(n+1) = \pi$}
&\overset{\text{Behauptung}}{=} \begin{cases}
0 & k \neq j \\
\frac{1}{2} (n+1) & k=j\neq 0 \\
(n+1) & k = j = 0
\end{cases}
\end{align*}
\begin{description}
  \item[Fall 1:] $k=j=0 \Rightarrow \frac{1}{2} \sum_{l=0}^n(1+1) = (n+1)$
  \item[Fall 2:] $k=j\neq0 \Rightarrow \frac{1}{2} \text{Re} \left( (n+1) + e^{ijh} \underbrace{\frac{e^{i2j\overbrace{(n+1)h}^{= \pi}}-1}{e^{i2jh}-1}}_{=0}\right) = \frac{1}{2} (n+1)$ 
  \item[Fall 3:] $k \neq j$:
    \begin{description}
      \item[Fall 1:] $k+j$ ist gerade $\Rightarrow$ $k-j$ ist gerade \\
        $\Rightarrow \medspace \frac{1}{2} \text{Re} \left(0+0\right) = 0$
      \item[Fall 2:] $k+j$ ist ungerade $\Rightarrow$ $k-j$ ist ungerade \\
        \begin{align*}
        &\Rightarrow \medspace  \frac{1}{2} \text{Re} \left( e^{i(k+j)\frac{h}{2}} \frac{-2}{ e^{i(k+j)h} - 1} + e^{i(k-j)\frac{h}{2}} \frac{-2}{ e^{i(k-j)h} - 1}\right)&\\
        &= \frac{1}{2} \text{Re} \left(\underbrace{\frac{-2}{ e^{i(k+j)\frac{h}{2}} + e^{-i(k+j)\frac{h}{2}}}}_{\text{rein imaginär}} +  \underbrace{\frac{-2}{ e^{i(k-j)\frac{h}{2}} - e^{-i(k-j)\frac{h}{2}}}}_{\text{rein imaginär}}\right)&\\
        &= 0
        \end{align*}
    \end{description}
\end{description}
\end{proof}
\end{lemma}

\begin{comment*}
$(\thinspace \cdot \thinspace, \thinspace \cdot \thinspace )$ ist ein Skalarprodukt auf $\mathcal{P}_n$, da 
\begin{enumerate}
  \item[i)] bilinear
  \item[ii)] symmetrisch
  \item[iii)] positiv definit \\
    $(f,f) = \sum_{l=0}^n f(x_l)^2 \geq 0$\\
    $(f,f) = 0 \overset{!}{\Rightarrow} f \equiv 0$\\
    $\sum_{l=0}^n f(x_l)^2 = 0 \Rightarrow \forall l: \medspace f(x_l) = 0$
    $\Leftrightarrow f \equiv 0$, da $\text{deg}(f) \leq n$ 
\end{enumerate}
\end{comment*}

\begin{theorem}
Sei $p$ das Interpolationspolynom zur Funktion $f$ in den Tschebyscheffknoten $x_0, \dots, x_n$ (Wurzeln von $T_{n+1}$), so gilt:
\begin{align*}
&p(x) = \frac{1}{2} c_0 + \sum_{j=1}^n c_j T_j(x), &\\
\intertext{wobei}
&c_k = \frac{2}{n+1} \sum_{l=0}^n f(x_l) \cos(k \frac{2l+1}{n+1}\frac{\pi}{2}), \quad \text{für $k=0,...,n$}
\end{align*}

\begin{proof}[Beweis]
Betrachte $(p, T_k)$
\begin{align*}
(p, T_k) &= \frac{1}{2} (T_0, T_k) + \sum_{l=1}^n c_l (T_l, T_k) &\\
&= \begin{cases}
c_k (T_k, T_k) & \text{für }k \neq 0 \\
\frac{1}{2} c_0 (T_0, T_0)  & \text{für }k = 0 
\end{cases}&\\
&\overset{(10.8)}{=} \frac{n+1}{2} c_k&\\
&= \frac{n+1}{2} \sum_{l=0}^n f(x_l) T_k(x_l) &\\
&= \frac{n+1}{2} \sum_{l=0}^n f(x_l) \cos\left(k \underbrace{\arccos\left(\cos\left(\frac{2l+1}{n+1}\frac{\pi}{2}\right)\right)}_{ = \frac{2l+1}{n+1}\frac{\pi}{2}}\right) &\\
\end{align*}
\end{proof}
\end{theorem}
$p(x)$ lässt sich als bei bekannten Koeffizienten $c_k$ leicht berechnen/auswerten.

\begin{theorem}[Clenshaw Algorithmus]
Sei $p \in \mathcal{P}_n$ durch die Koeffizienten $c_0, ..., c_n$ in der Form
\begin{align*}
&p(x) = \frac{1}{2} c_0 + \sum_{j=1}^n c_j T_j(x)&\\
\intertext{gegeben. Setzt man }
&d_{n+1} = d_{n+2} = 0 &\\
\intertext{und definiert für $x$}
&d_k = c_k + 2x d_{k+1} - d_{k+2}, \quad \text{für }k=n,n-1,...,1, 0&\\
\intertext{so gilt:}
&p(x) = \frac{1}{2} (d_0-d_2)&\\
\end{align*}
\begin{proof}[Beweis]
Verwende die Rekursionsformel aus (10.2) iii) ($T_{k+1} = 2x T_k + T_{k-1}$). Dann ist 
\begin{align*}
p(x) &= \frac{1}{2} c_0 + \sum_{l=1}^n c_l T_l(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-3} c_l T_l(x) + c_{n-2} T_{n-2}(x) + c_{n-1} T_{n-1}(x) + c_{n} T_{n}(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-3} c_l T_l(x) + (c_{n-2} - \underbrace{c_n}_{=d_n}) T_{n-2}(x) + \underbrace{(c_{n-1} + 2xc_n)}_{=d_{n-1}} T_{n-1}(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-4} c_l T_l(x) + (c_{n-3} - d_{n-1}) T_{n-3}(x) + \underbrace{(c_{n-2} - d_n + 2xd_{n-1})}_{=d_{n-2}} T_{n-2}(x) &\\
\intertext{induktiv erhält man}
&= (\frac{1}{2} c_0 - d_2) \underbrace{T_0(x)}_{=1} + \underbrace{(c_1 - d_3 + 2xd_2)}_{=d_1} \underbrace{T_1(x)}_{=x} &\\
&= \frac{1}{2} (\underbrace{c_0 - 2d_1x - d_2}_{=d_0} -d_2) &\\
&= \frac{1}{2} (d_0-d_2)
\end{align*}
\end{proof}
\end{theorem}

\begin{comment*}
Bei der Verwendung von Rekursionen ist es wichtig zu verstehen, wie sich Rundungsfehler auswirken. 
\begin{description}
  \item[Beispiel:]
    $x_{n+1} = 10x_n - 9, \quad x_0 = 1$ \\
    $ \Rightarrow \forall n \in \mathbb{N}: \medspace x_n = 1$ \\
    Was passiert bei fehlerhafter Startwerten $\tilde{x}_0 = 1+ \varepsilon$? \\
    $ \tilde{x}_{n+1} = 10 \tilde{x}_n - 9, \quad \tilde{x}_n = 1+ 10^n \varepsilon$
\end{description}
Der Clenshaw-Algorithmus ist stabil, wie im Folgenden gezeigt wird:
\end{comment*}

\begin{theorem}
Für den Clanshaw-Algorithmus mit Fehlern $\varepsilon_k$ in der Rekursion, d.h. für 
\begin{align*}
&\tilde{d}_{n+1} = \tilde{d}_{n+2} = 0 &\\
&\tilde{d}_k = c_k + 2x \tilde{d}_{k+1} - \tilde{d}_{k+2} + \varepsilon_k, \quad k=n, n-1, ..., 0 &\\
\intertext{Dabei ist $\varepsilon_k$ der Rundungsfehler in der k-ten Iteration. Für
$\tilde{p}(x) = \frac{1}{2} (\tilde{d}_0 - \tilde{d}_2)$ gilt:}
& \vert \tilde{p}(x) - p(x) \vert \leq \sum_{j=0}^n \vert \varepsilon_j \vert, \quad \text{für } \vert x \vert < 1,
\intertext{wobei $p(x)$ mit (10.10) berechnet wird.}
\end{align*}

\begin{proof}[Beweis]
Setze $\varepsilon_k := \tilde{d}_k - d_k$ (für $d_k$ aus (10.10)). Dann gilt:
\begin{align*}
& \varepsilon_k = \varepsilon_k + 2x \varepsilon_{k+1} - \varepsilon_{k+2}, \quad \text{für } k=n,...,0 &\\
&\varepsilon_{n+1} = 0 \quad \text{und} \quad \varepsilon_{n+2} = 0
\end{align*}
Mit Satz (10.10) gilt für $c_k = \varepsilon_k$ und $d_k = \varepsilon_k$:
\begin{align*}
& \frac{1}{2} (\varepsilon_0 -\varepsilon_2) = \frac{1}{2} \varepsilon_0 + \sum_{j=1}^n \varepsilon_j T_j(x) 
\intertext{Da $\vert T_j(x) \vert \leq 1$ für $x \in [1, 1]$ gilt:}
& \vert \tilde{p}(x) - p(x) \vert \overset{\Delta-UGL}{\leq} \frac{1}{2} \vert \varepsilon_0 \vert + \sum_{j=1}^n \vert \varepsilon_j \vert  
\end{align*}
\end{proof}
\end{theorem}

\begin{comment*}
Die Approximation einer Funktion durch die Summe von Tschebyscheffpolynomen wird im Computer zur Berechnung von Funktionen wie log, exp, sin, cos,... verwendet.
\end{comment*}

\begin{example}
\underline{Ziel:} Berechne $\ln(x)$ für $0 \leq x_{\min} < x \leq x_{\max}$. $x_{\min}, x_{\max}$ ist die kleinste/größte positive darstellbare Zahl auf dem gegebenen Computer. \\
$x \medspace \text{"="} \medspace \underbrace{\left[ 1, b_1, b_2, ..., b_M \right]}_{\text{"Mantisse"}} * 2^N, \quad b_j \in \{0,1\}$\\
d.h. $ x = 2^N (1+ b_1 \frac{1}{2} + b_2 \frac{1}{4} + ... + b_M \frac{1}{2^M}) = 2^N (1+t), \quad t \in (0,1)$ \\
$\ln(x) = \ln(1+t) + N \underbrace{\ln(2)}_{\text{Konstante}}$ \\
Das Problem $\ln(x)$ zu berechnen ist damit auf das Problem $\ln(1+t)$ für $t \in [0,1]$ zu berechnen reduziert worden. \\
Tschebyscheffinterpolation: $[-1, 1] \rightarrow [0,1], \medspace x \mapsto t = \frac{1+x}{2} \quad (\Leftrightarrow x=2t-1)$ \\
Für den Interpolationsfehler gilt:
\begin{align*}
\ln(1+ \frac{1+x}{2}) -p(x) &= \underbrace{\prod_{j=0}^n (x-x_j)}_{=2^{-n} \text{ für Tschebyscheff}} \frac{1}{(n+1)!} \frac{(-1)^{n-1} (n-1)!}{(1+\frac{1+\xi}{2})^n} \left(\frac{1}{2}\right)^n, \quad \xi \in [-1, 1] &\\
\Leftrightarrow \quad \vert \ln(1+ \frac{1+x}{2}) -p(x) \vert &= \frac{1}{4^n} \frac{1}{(n+1)^n}
\end{align*}
Für n=15 ist $\frac{1}{4^n} \frac{1}{(n+1)^n} \leq 10^{-11}$ \\
Berechnet werden also $c_0,..., c_{15}$ (einmal für alle Zeiten):
\begin{align*}
c_0 &= 0.75290562... &\\
c_1 &= 0.34... &\\
c_2 &= -0.029... &\\
c_3 &= 0.0036... &\\
c_4 &= -0.00004 &\\
\vert c_k \vert &\leq 10^{-9}, \quad \text{für }k >10 &\\
\end{align*}
Beobachtung: $c_k$ werden schnell klein.\\
Um eine Genauigkeit von $10^{-8}$ (einfache Genauigkeit) zu erreichen, benötigt man nur $c_0,..., c_9$.\\
Die Auswertung mit dem Clenshaw-Algorithmus benötigen wir 10 Multiplikation (vgl. Taylor $\log(1+t) = \sum_{k=1}^{\infty} \frac{(-1)^k}{k}t^k$)
\end{example}

\subsection{Hermit\'{e}-Interpolation}

Gegeben sind $(x_i, y_i, y_i')_{i=0}^n, \quad x_i \in [a,b]$ paarweise verschieden. Gesucht ist ein Polynom $p \in \mathcal{P}$, sodass
\begin{align*}
&p(x_i) = y_i \quad \text{und } &\\
&p'(x_i) = y_i', \quad \text{für } i=0,...,n. &\\
\end{align*}
\underline{Idee:} Lasse $\varepsilon \rightarrow 0$ laufen im Newtonschema:

\begin{tabular}{lll}
 
$x_0$ & $y_0$\\
 & & $\delta y[x_0, x_0+\varepsilon] = \frac{(y_0 + \varepsilon y_0') - y_0}{(x_0 +\varepsilon) - x_0} = y_0'$\\
$x_0+\varepsilon$ & $y_0 + \varepsilon y_0'$\\
 & & $\delta y[x_0+\varepsilon, x_1] \underset{\varepsilon \rightarrow 0}{\rightarrow} \delta y[x_0,x_1]$\\
$x_1$ & $y_1$\\
 & & $\delta y[x_1, x_1+\varepsilon] = y_1'$\\
$x_1+\varepsilon$ & $y_1+\varepsilon y_1'$\\
 
\end{tabular} \\ \\
Newtonsche Interpolationsformel:
\begin{align*}
p_{\varepsilon}(x) = y_0 &+ (x-x_0) \delta y[x_0, x_0+\varepsilon] &\\
&+ (x-x_0)(x-(x_0 + \varepsilon)) \delta^2y[x_0, x_0 + \varepsilon, x_1] &\\
&+ \dots &\\
&+ \left( \prod_{j=0}^{n-1} (x-x_j)(x-(x_j + \varepsilon))(x-x_1) \dots \delta^{2n+1} y[x_0,\dots, x_1]\right)
\end{align*}
damit ist:
\begin{align*}
p_{\varepsilon}(x_i) &= y_i &\\
p_{\varepsilon}(x_i + \varepsilon) &= y_i + \varepsilon y_i' &\\
\Rightarrow \quad y_i' &= \frac{p_{\varepsilon}(x_i + \varepsilon) - p_{\varepsilon}(x_i)}{\varepsilon} \underset{MWS}{=} p'_{\varepsilon}(\xi_i), \quad \text{für } \xi_i \in [x_i, x_i+\varepsilon]
\end{align*}
Für $\varepsilon \rightarrow 0$ definieren wir
\begin{align*}
\delta^k y[x_0, x_0, x_1, x_1, ...] &:= \lim_{\varepsilon \rightarrow 0} \delta^k y[x_0, x_0+\varepsilon, x_1, x_1+\varepsilon, ...] &\\
\end{align*}
und
\begin{align*}
p(x) &:= \lim_{\varepsilon \rightarrow 0} p_{\varepsilon}(x) &\\
&= y_0 + (x-x_0) \underbrace{\delta y[x_0, x_0]}_{y_0'} + (x-x_0)^2 \delta^2 y[x_0, x_0, x_1] &\\
&+ (x-x_0)^2(x-x_1) \delta^3 y[x_0, x_0, x_1, x_1] &\\
&+ ... + \prod_{j=0}^{n-1}(x-x_j)^2 (x-x_n) \delta^{2n-1}y[x_0,x_0, ..., x_n, x_n] &\\
p(x_i) &= \lim_{\varepsilon \rightarrow 0} p_{\varepsilon}(x_i) = y_i &\\
p'(x_i) &= \lim_{\varepsilon \rightarrow 0} p'_{\varepsilon}(x_i) = \lim_{\varepsilon \rightarrow 0}(\xi_{i, \varepsilon}) = y_i'
\intertext{für $\xi_{i, \varepsilon} \in [x_i, x_i+\varepsilon]$}
\end{align*}
\underline{Schema:} \\
\begin{tabular}{llllll}
 
$x_0$ & $y_0$\\
 & & $y_0'$\\
$x_0$ & $y_0 $ & & $ \delta^2[x_0, x_0, x_1]$\\
 & & $\delta y[x_0,x_1]$ & & $ \delta^3[x_0, x_0, x_1, x_1]$\\
$x_1$ & $y_1$ & & $ \delta^2[x_0, x_1, x_1]$ && $\dots$\\
 & & $y_1'$ && $\dots$\\
$x_1$ & $y_1$ & & $ \delta^2[x_1, x_1, x_2]$ && $\dots$\\
 & & $\delta y[x_1, x_2]$ && $\dots$\\
$x_2$ & $y_2$ && $\dots$\\
 & & $y_2'$\\
$x_2$ & $y_2$\\
 
\end{tabular} \\ \\
\underline{Eindeutigkeit:} \\
Annahme: $\exists q \in \mathcal{P}_{2n+1}$ mit $q(x_i) = y_i)$, $q'(x_i) = y_i'$\\
Dann ist $q-p \in \mathcal{P}_{2n+1}$ \\
$q-p$ besitzt doppelte Nullstelle in $x_i$ \\
$q-p = c \prod(x-x_i)^2$, da $\text{deg}\left(\prod_{i=0}^n (x-x_i)^2 \right) = 2n+2$ \\
$\Rightarrow \medspace c=0 \quad \Rightarrow \medspace q=p$ \\\\
Damit ist der folgende Satz bewiesen.

\begin{theorem}
Zu gegebenen $(x_i, y_i, y_i')_{i=0}^n$ mit $x_i \neq x_j$, falls $i \neq j$ existiert ein eindeutiges Polynom $p \in \mathcal{P}_{2n+1}$ mit $p(x_i) = y_i$ und $p'(x_i) = y_i'$ ($i=0,\dots, n$). $p$ kann mit Hilfe des Newtonschen Differenzenschemas mit doppelten eingeschriebenen Nullstellen (Knoten) berechnet werden.
\end{theorem}

\begin{theorem}[vgl. Satz (9.1)]
Sei $f: [a,b] \rightarrow \mathbb{R}$ $(2n+2)$-mal stetig differenzierbar ($f \in \mathcal{C}^{2n+2}([a,b], \mathbb{R})$), seien $x_0,..., x_n$ paarweise verschieden und sei $p$ Hermit\'{e}polynom aus (11.1) zu $(x_i,y_i, y_i')_{i=0}^n$. Dann gilt:
\[ \forall x \in [a,b] \exists \xi \in [a,b]: \medspace f(x)-p(x) = \prod_{j=0}^n (x-x_j)^2 \frac{f^{(2n+2)} (\xi)}{(2n+2)!} \]

\begin{proof}[Beweis]
Betrachte $\varepsilon \rightarrow 0$ für $p_{\varepsilon}(x)$ in der Fehlerformel (9.1):
\begin{align*}
f(x)-p(x) &= \prod_{j=0}^n (x-x_j)(x-(x_j+\varepsilon)) \frac{f^{(2n+2)}(\xi_{\varepsilon})}{(2n+2)!}, \quad \text{für } \xi_{\varepsilon} \in [a,b]
\end{align*}
Sei $\xi$ ein Häufungspunkt von $\{\xi_{\varepsilon}, \varepsilon > 0\}$. Dann existiert eine Nullfolge $(\varepsilon_k)_{k\in \mathbb{N}}$ mit $\xi_{\varepsilon_k} \rightarrow \xi$ für $k \rightarrow \infty$. $\Rightarrow$
\begin{align*}
f(x)-p(x) &= \lim_{k \rightarrow \infty} (f(x) - p_{\varepsilon_k}(x)) &\\
&= \prod_{j=0}^n (x-x_j)^2 \frac{f^{(2n+2)}(\xi)}{(2n+2)!}
\end{align*}
\end{proof}
\end{theorem}

\subsection{Spline-Interpolation}
Spline ist engl. für Holz- oder Metallfeder.\\
\underline{Theorie:} stammt von Schoenenberg aus dem Jahr 1946\\
\underline{Idee:} Suche 'glatte' Funktion $s$ durch vorgegebene Punkte $(x_i, y_i)_{i=0}^n$
\begin{enumerate}
  \item[i)] $s(x_i)= y_i$ ($i=0,...,n$)'Interpolationseigenschaft'
  \item[ii)] $s$ muss mind. 2-mal stetig differenzierbar sein und $ \int_a^b (s''(x))^2dx $ soll minimal sein. 'glatt'
\end{enumerate}
Dadurch vermeidet man Oszillationen, wie sie bei der Polynominterpolation hohen Grades entstehen.\\
Wir suchen also eine Funktion $s$, sodass für $\varepsilon \in \mathbb{R}$ und $h \in \mathcal{C}^1([a,b], \mathbb{R})$, $h(x_i) = 0 \medspace (i=,...,n)$ und 
\begin{align*}
\int_a^b (s''(x))^2 dx &\overset{!}{\leq} \int_a^b \left( (s(x) + \varepsilon h(x))'' \right)^2 dx &\\
&= \int_a^b (s''(x) + \varepsilon h''(x))^2 dx &\\
&= \int_a^b (s''(x))^2 dx + 2\varepsilon \int_a^b s''(x)h''(x) dx + \underbrace{\varepsilon^2\int_a^b (h''(x))^2dx}_{ \geq 0 } 
\end{align*}
Obige Ungleichung ist erfüllt, falls 
$$ \forall h \in \mathcal{C}^2([a,b]) \medspace \text{mit }h(x_i) = 0: \medspace \int_a^b h''(x) s''(x) dx = 0$$
Dabei gilt:
$$ \int_a^b h''(x) s''(x) dx = \left[ s''(x) h'(x) \right]_{x=a}^b - \int_a^b s'''(x) h'(x)dx$$
Falls $s'''(x) = \alpha_i$ für $x \in [x_{i-1}, x_i]$, dann ist 
\begin{align*}
\int_a^b s'''(x) h'(x) dx &= \sum_{i=1}^n \alpha_i \int_{x_{i-1}}^{x_i} h'(x) dx &\\
&= \sum_{i=1}^n \alpha_i \left(\underbrace{h(x_i)}_{= 0} - \underbrace{h(x_{i-1}}_{= 0}\right) &\\
&= 0
\end{align*}
$\Rightarrow$ Forderung: $\left[s''(x) h'(x) \right]_{x=a}^b = s''(b)h'(b) - s''(a)h'(a) \overset{!}{=} 0$

\begin{theorem}
Seien $f, s \in \mathcal{C}^2 ([a,b], \mathbb{R})$ zwei Funktionen, die in $a=x_0 < x_1 < ... < x_n = b$ dieselben Werte annehmen, d.h. 
\begin{align*}
f(x_i) = s(x_i) \medspace &(i=0,...,n)
\quad \text{und } \quad 
s\mid_{[x_{i-1}, x_i]} &\in \mathcal{P}_3 \quad \text{für } i=1,...,n
\intertext{Falls}
s''(a) [f'(a) - s'(a)] &= s''(b) [f'(b) - s'(b)], \quad (*) 
\intertext{so gilt:}
\int_a^b (s''(x))^2dx &\leq \int_a^b(f''(x))^2dx
\end{align*}
\begin{proof}[Beweis]
Obige Rechnung für $h=f-s$ und $\varepsilon = 1$, $h(x_i) = 0$ \\
$[s''(x) h'(x)]_{x=a}^b = 0 \Leftrightarrow (*)$
\end{proof}
\end{theorem}

\begin{comment}
Die Bedingung (*) kann erreicht werden durch 
\begin{enumerate}
  \item[a)] Vorgabe von $s'(a) = f'(a)$, $s'(b) = f'(b)$ \\
  Der dadurch bestimmte Spline heißt \textbf{eingespannter} Spline.
  \item[b)] Vorgabe von $s''(a) = 0 = s''(b)$ \\
  Der dadurch bestimmte Spline heißt \textbf{natürlicher} Spline. Dieser hat aber schlechtere Approximationseigenschaften. 
\end{enumerate}
\end{comment}
\end{document}
