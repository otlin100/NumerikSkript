\documentclass[12pt]{article}


\usepackage{algorithmic} %Für Pseudocode https://math-linux.com/latex-26/faq/latex-faq/article/how-to-write-algorithm-and-pseudocode-in-latex-usepackage-algorithm-usepackage-algorithmic
\usepackage{stmaryrd} %Für Widerspruchsblitz
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage{amsthm} %Für Theoreme und Beweise
\usepackage{graphicx} %Für Bilder
\usepackage{nicefrac}
\usepackage{hyperref} %Für klickbares Inhaltsverzeichnis

\renewcommand{\contentsname}{Inhaltsverzeichnis}
\hypersetup{ %Einstellungen für Links (im Inhaltsverzeichnis)
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newtheoremstyle{break}% name
  {}%         Space above, empty = `usual value'
  {}%         Space below
  {\normalfont}% Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\bfseries}% Thm head font
  {.}%        Punctuation after thm head
  {\newline}% Space after thm head: \newline = linebreak
  {}%         Thm head spec

\theoremstyle{break}

\renewcommand{\thesection}{\Roman{section}}
\counterwithout{subsection}{section}
%\renewcommand{\thesubsection}{\arabic{subsection}}

%Definiere Satz, Definition,...
\newtheorem{theorem}{Satz}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{korollar}[theorem]{Korollar}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{algorithm}[theorem]{Algorithmus}
\newtheorem{comment}[theorem]{Bemerkung}
\newtheorem*{comment*}{Bemerkung}
\newtheorem*{example*}{Beispiel}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{example}[theorem]{Beispiel}
\newtheorem{properties}[theorem]{Eigenschaften}
\newtheorem{nothing}[theorem]{}

\author{Prof. Schaedle}
\title{Mitschrift Numerik 1 }

\begin{document}
\maketitle

\newpage

\tableofcontents
\newpage

\section{Numerische Integration}

\input{Kapitel_1/Abschnitt_1} %Einführung
\input{Kapitel_1/Abschnitt_2} %Ordnung von Quadraturformeln
\input{Kapitel_1/Abschnitt_3} %Quadraturfehler
\input{Kapitel_1/Abschnitt_4} %Quadratur mit hoher Ordnung
\input{Kapitel_1/Abschnitt_5} %Orthogonalpolynome
\input{Kapitel_1/Abschnitt_6} %Ein adaptives Programm
\input{Kapitel_1/Abschnitt_7} %Gauß- und Lobatto Quadraturformeln

\section{Interpolation und Approximation}

\begin{description}
  \item[Problemstellung A]
    Zu gegebenen $(x_0, y_0), ...,(x_n, y_n)$ berechne Polynom $p$ vom Grad $\leq n$ mit $$p(x_j) = y_j, \quad j=0,...,n$$
  
  \item[Problemstellung B]
    $f:[a,b] \rightarrow \mathbb{R}$ gegeben. Finde einfach auszuwertende Funktion $p: [a,b] \rightarrow \mathbb{R}$, etwa ein Polynom, stückweises Polynom, rationale Funktion, sodass $f-p$ klein ist.
    \begin{enumerate}
      \item[i)] $f(x)=p(x)$ für endlich viele vorgegebene Punkte $x$
      \item[ii)] $\int_a^b (f(x)-p(x))^2 dx$ soll minimal sein.
      \item[iii)] $\max_{x \in [a,b]} \vert f(x) -p(x) \vert$ soll minimal sein.
    \end{enumerate}
\end{description}

\subsection{Newtonsche Interpolationsformel}

\begin{example}
\begin{description}\item \end{description}
\begin{description}
  \item n=1: \\
    $(x_0, y_0),(x_1,y_1)$, $p \in \mathcal{P}_1$ das beide Punkte verbindet.\\
    $$p(x) = y_0 + (x-x_0) \frac{y_1-y_0}{x_1-x_0}$$
  \item n=2: \\
    $(x_0, y_0),(x_1,y_1),(x_2,y_2)$ \\
    $$p(x) = y_0 + (x-x_0) \frac{y_1-y_0}{x_1-x_0} + a(x-x_0)(x-x_1)$$
    Bestimme $a$ so, dass $p(x_2) = y_2$
    \begin{flalign*}
    y_2 &\overset{!}{=} y_0 + (\overset{-x_1+x_1}{\check{x_2\thinspace-}}x_0) \frac{y_1-y_0}{x_1-x_0} + a(x_2-x_0)(x-x_1)&\\
    a(x_2-x_0)(x_2-x_1) &= y_2 - y_0 - (x_2-x_1) \frac{y_1-y_0}{x_1-x_0} - y_1 + y_0 &\\
    \Rightarrow a &= \frac{1}{x_2-x_0} \left( \frac{y_2-y_1}{x_2-x_1} - \frac{y_1-y_0}{x_1-x_0} \right) 
     \end{flalign*}
\end{description}
\end{example}

\begin{definition}[dividierte Differenzen]
Für $(x_0,y_0), (x_1, y_1), ..., (x_n, y_n)$ mit paarweise verschiedenen Stützstellen $x_j$ definieren wir
\begin{flalign*}
y[x_j] &:= y_j \quad \left( = \delta^0 y[x_j] \right) &\\
\delta y[x_j, x_{j+1}] &:= \frac{y_{j+1} - y_j}{x_{j+1}-x_j} = \frac{\delta^0 y[x_{j+1}]-\delta^0 y[x_{j}]}{x_{j+1} - x_j} &\\
\delta ^2 y[x_j, x_{j+1}, x_{j+2}] &:= \frac{\delta y[x_{j+1}, x_{j+2}]-\delta y[x_{j}, x_{j+1}]}{x_{j+2} - x_j} &\\
\delta ^k y[x_j, x_{j+1},..., x_{j+k}] &:= \frac{1}{x_{j+k}-x_j} \left( \delta^{k-1} y[x_{j+1}, ..., x_{j+k}] - \delta^{k-1} y[x_j, ..., x_{j+k-1}] \right)
\end{flalign*}
\underline{Schema:}\\
\begin{tabular}{ccccc}
 
$x_0$ & $y_0$& & &\\
 & & $\delta^1y[x_0, x_1]$ & &\\
$x_1$ & $y_1$ & &$\delta^2y[x_0, x_1, x_2]$ &\\
 & & $\delta^1y[x_1, x_2]$ & & $\delta^3y[x_0, x_1, x_2, x_3]$\\
$x_2$ & $y_2$ & & $\delta^2y[x_1, x_2, x_3]$ &\\
 & & $\delta^1y[x_2, x_3]$ & &\\
$x_3$ & $y_3$ & & &\\
 
\end{tabular}
\end{definition}

\begin{comment}
Falls die $x_i$ äquidistant, dh. $x_i = x_0+ih$ so ist: \\
\begin{flalign*}
\delta y[x_i, x_{i+1}] &= \frac{y_{i+1} - y_i}{h} =: \frac{1}{h} \Delta y_i &\\
\delta ^2 y[x_i, x_{i+1}, x_{i+2}] &= \frac{\frac{1}{h} \Delta y_{i+1} - \frac{1}{h} \Delta y_{i}}{2h} = \frac{1}{2h^2} \Delta ^2 y_i &\\
\delta ^k y[x_i, ..., x_{i+k}] &= \frac{1}{k!h^k} \Delta^k y_i,
\end{flalign*}
    wobei $\Delta^{k}y_i := \Delta^{k-1}y_{i+1} - \Delta^{k-1}y_i$.
\end{comment}

\begin{theorem}[Newtonsche Interpolationsformel]
Zu paarweise verschiedenen reellen $x_i$, $i=0,..., n$, existiert ein eindeutiges Polynom $p \in \mathcal{P}_n$ durch die Punkte $(x_i, y_i)$, $i=0,...,n$ (d.h. $p(x_i) = y_i$ für $i=1,...,n$). Es lässt sich berechnen durch:
\begin{flalign*}
p(x) &= y[x_0] + (x-x_0) \delta y[x_0,x_1] + ... + (x-x_0)(x-x_1)...(x-x_{n-1}) \delta ^n y[x_0, ..., x_n] &\\
&= \sum_{i=0}^n \prod_{j=0}^{i-1} (x-x_j) \delta^iy[x_0,...,x_i] 
\end{flalign*}
\begin{proof}[Beweis](Induktion)\phantom{\qedhere}
\begin{description}
  \item[IA] $n=1$ (und $n=2$) vgl. Beispiel (1.1)
  \item[IS] $n-1 \rightarrow n$\\
    $$p_0(x) = y[x_0] + (x-x_0) \delta y[x_1, x_0] + ... + (x-x_0)...(x-x_{n-2}) \delta ^{n-1}y[x_0,..., x_{n-1}]$$ 
    ist das eindeutige interpolierende Polynom mit 
    $$\text{deg}(p_0) \leq n-1$$
    zu $(x_0,y_0), (x_1, y_1), ..., (x_{n-1}, y_{n-1})$. \\
    Für den Ansatz
    $$p(x) = p_0(x) + a(x-x_0)(x-x_1)...(x-x_{n-1})$$
    ergibt die Forderung $p(x_n) = y_n$
    $$a = \frac{y_n-p_0(x_n)}{(x_n-x_0)(x_n-x_1)...(x_n-x_{n-1})}$$
    Da $a$ eindeutig ist, ist $p$ eindeutig.\\
    Es bleibt zu zeigen: $a = \delta^n y[x_0, ..., x_n]$\\
    Sei dazu ein Polynom $p_1(x)$, welches durch $(x_1, y_1), ..., (x_n, y_n)$ läuft, mit $\text{deg}(p_1) \leq n-1$. Nach Induktionsannahme gilt 
    \begin{flalign*}
    p_1(x) &= y[x_1] + (x-x_1) \delta^1y[x_1, x_2] + ... + (x-x_1)...(x-x_{n-1}) \delta ^{n-1}y[x_1, ..., x_n]&\\
    &=x^{n-1} \delta^{n-1}y[x_1, ..., x_n] + r
    \end{flalign*}
    mit $\text{deg}(r) \leq n-2$.\\
    Setze Polynom
    $$p(x) := \frac{x_n-x}{x_n-x_0} p_0(x) + \frac{x-x_0}{x_n-x_0}p_1(x)$$
    mit $\text{deg}(p) \leq n$ durch $(x_0, y_0), ..., (x_n, y_n)$. \\
    Das gilt, da: 
    \begin{flalign*}
    p(x_0) &= p_0(x_0) = y_0 &\\
    p(x_n) &= p_1(x_n) = y_n &\\
    \text{Für } i=1,...,n-1: &\\
    p(x_i) &= \frac{x_n-x_i}{x_n-x_0} \underbrace{p_0(x_i)}_{y_i} + \frac{x_i-x_0}{x_n-x_0} \underbrace{p_1(x_i)}_{y_i} = y_i
    \end{flalign*}
    Andererseits:
    $$p(x) = ax^n + r \quad \text{mit  deg}(r) \leq n-1$$
    Koeffizientenvergleich:
    \begin{align*}
    a &= - \frac{1}{x_n-x_0} \delta^{n-1}y[x_0, ..., x_{n-1}] + \frac{1}{x_n-x_0} \delta^{n-1}y[x_1, ..., x_{n}] &\\
    &= \delta^n y[x_0, ..., x_n]&\\\tag*{\qed}
    \end{align*}
    
\end{description}
\end{proof}
\end{theorem}

\begin{nothing}[Hornerschema]
Zur Auswertung des Interpolationspolynom $p$ an der Stelle $x$ verwendet man 
$$
p(x) = y[x_0] + (x-x_0) \left( \delta y[x_0, x_1] + (x-x_1) \left( \delta ^2 y[x_0, x_1, x_2] + (x-x_2) \left(  ... \left( \delta^n y[x_0, ..., x_n] \right) \right) \right) \right)
$$
\underline{Algorithmus:}
\begin{algorithmic}
\STATE $s = \delta^n y[x_0, ..., x_n]$
\FOR{$k = n-1, ...,0$}
\STATE $s = \delta^k y[x_0, ..., x_k] + (x-x_k) s$
\ENDFOR
\end{algorithmic}
\end{nothing}

\begin{example}
\begin{tabular}{||l|l|l|rrrr||}
\hline
$i$ & $x_i$ & $y_i$ & $\delta^1 y[x_0, x_1]$& $\delta^2 y[x_0, x_1, x_2]$& $\delta^3 y[x_0, .., x_3]$& $\delta^4 y[x_0, .., x_4]$\\
\hline
$0$ & $-1$ & $0$&&&&\\
& & & $\frac{1-0}{0-(-1)} = 1$&&&\\
$1$ & $0$ & $1$ & & $\frac{0-1}{2-(-1)} = -\frac{1}{3}$&&\\
& & & $0$ & & $\frac{\frac{2}{3} - (-\frac{1}{3})}{3-(-1)} = \frac{1}{4}$&\\
$2$ & $2$ & $1$ & & $\frac{2-0}{3-0} = \frac{2}{3}$ && $\frac{-\frac{2}{5} - \frac{1}{4}}{5-(-1)} = -\frac{13}{120}$\\
& & & $\frac{3-1}{3-2} = 2$ & & $\frac{-\frac{4}{3} - \frac{2}{3}}{5-0} = -\frac{2}{5}$&\\
$3$ & $3$ & $3$ & & $\frac{-2-2}{5-2} = -\frac{4}{3}$&&\\
& & & $\frac{-1-3}{5-3} = -2$&&&\\
$4$ & $5$ & $-1$&&&&\\
\hline
\end{tabular} \\\\
Das Interpolationspolynom ist also
$$p(x) = 0 + (x+1) *1 - \frac{1}{3}(x+1)(x) + \frac{1}{4}(x+1)x(x-2)+(x+1)x(x-2)(x-3)\left(-\frac{13}{120}\right)$$
bzw. nach Hornerschema
\begin{flalign*}
p(x) &= 0+(x+1)\left(1+ x\left(-\frac{1}{3} + (x-2)\left(\frac{1}{4} + (x-3)\left(-\frac{13}{120}\right)\right)\right)\right) &\\
\end{flalign*}
Werte $p(x)$ an der Stelle 1 aus:
\begin{flalign*}
-\frac{13}{120}* (-2) &= \frac{26}{120} &\\
\left( \frac{1}{4} + \frac{26}{120} \right)(-1) &= -\frac{56}{120} = -\frac{7}{15} &\\
\left( -\frac{7}{15} - \frac{1}{3}\right) 1 &= -\frac{12}{15} = -\frac{4}{5} &\\
\left(-\frac{4}{5} + 1\right)2 &= \frac{2}{5} = p(1) &\\
\end{flalign*}
\end{example}

\subsection{Fehler bei der Polynominterpolation}

\underline{Problem:} $f: \thinspace [a,b] \rightarrow \mathbb{R}$ werde interpoliert in Stützstellen $x_0, ..., x_n \in [a,b]$ durch $p \in \mathcal{P}_n$ mit $p(x_i) = f(x_i)$ für $i=0,...,n$. \\
Wie groß ist der Fehler $f(x)-p(x)$?

\begin{theorem}
Sei $f: [a,b] \rightarrow \mathbb{R}$ (n+1)-mal stetig differenzierbar, $p \in \mathcal{P}_n$ mit $p(x_i) = f(x_i)$ ($i=0,...,n$) das Interpolationspolynom zu paarweise verschiedenen Stützstellen $x_i \in [a,b]$ ($i=0,..., n$). Dann gilt: \\
\[\forall x \in [a,b] \exists \xi = \xi(x) \in (a,b): f(x)-p(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^n (x-x_i) \]
\begin{proof}[Beweis]
Siehe (9.4)
\end{proof}
\end{theorem}

\begin{example}[Berechnung von Logarithmentafeln: Briggs, 17. Jhd]
$f(x) = log_{10}(x), \quad x \in [55, 58]$\\
Wähle Stützstellen:\\
$x_0 = 55, \quad x_1 = 56, \quad x_2 = 57, \quad x_3=58$ \\
Es seien \[log_{10}(55), \medspace log_{10}(56), \medspace log_{10}(57) \text{ und } log_{10}(58)\] bereits bekannt. Berechne eine Näherung von \(f\) an bei \(log_{10}(56.5)\) \\
$\rightarrow$ Interpolationspolynom $p$:
\begin{flalign*}
log_{10}(56.5) &= 1.752048448 &\\
p(56.5) &= 1.75204845 &\\
f'(x) &= \frac{1}{ln(10)x} &\\
f''(x) &= -\frac{1}{ln(10)x^2} &\\
f^{(3)}(x) &= \frac{2}{ln(10)x^3} &\\
f^{(4)}(x) &= -\frac{6}{ln(10)x^4} &\\
\text{Für } x\in[55, 58]: &\\
\vert f^{(4)}(x)\vert &\leq \frac{6}{55^4 ln(10)}
\Rightarrow &\\
\vert log_{10}(56.5) - p(56.5) \vert &\leq 1.5 \cdot 0.5 \cdot 0.5 \cdot 1.5 \cdot \frac{6}{55^4ln(10) \frac{1}{4!}} &\\
&\approx 6.7 \cdot 10^{-9}
\end{flalign*}
\end{example}

Für den Beweis von (9.1) wird folgendes Lemma benötigt:

\begin{lemma}
Sei $f \in C^n([a,b])$ und sei für paarweise verschiedene $x_i \in [a,b]$ ($i=0,...,n$) $y_i := f(x_i)$. Dann existiert $\xi \in (\min_i(x_i), \max_i(x_i))$, sodass 
$$\delta^ny[x_0,..., x_n] = \frac{f^{(n)}(\xi)}{n!}\quad (x_0 < x_1 < ... < x_n)$$
\begin{proof}[Beweis]
Sei $p$ ein Interpolationspolynom zu $(x_i, y_i)_{i=0}^n$. Setzt man $d:= p-f$, so gilt $d(x_i) = 0$ für $i=0,...,n$.\\
n-maliges anwenden des Mittelwertsatzes liefert paarweise verschiedene $\xi_i$, $(i=0, ...,n-1)$ mit $d'(\xi_i) = 0$ für $\xi_i \in (\min_j(x_j), \max_j(x_j))$. \\
Dasselbe Argument angewandt auf $d'$ liefert $\eta_0,..., \eta_{n-2}$ mit $d''(\eta_i) = 0$ für $i=0,..., n-2$.\\
Wiederhole dies bis:\\
Es existiert $\rho_0$ mit $d^{(n)}(\rho_0) = 0$\\
$\Rightarrow f^{(n)}(\rho_0) = p^{(n)}(\rho_0) = n! \delta^ny[x_0,..., x_n],$\\
da $\delta^ny[x_0, ..., x_n]$ der Koeffizient von $x^n$ in $p$ ist.
\end{proof}
\end{lemma}

\begin{comment*}
Für $n=1$ ist Lemma (9.3) der Mittelwertsatz (oder Satz von Rolle) aus Ana I:
$$ \exists \xi: \frac{f(x_1)-f(x_2)}{x_1-x_2} = f'(\xi)$$
\end{comment*}

\begin{nothing}[Beweis von (9.1)]
Sei $\bar{x} \in [a,b]$ beliebig.
\begin{description}
  \item[1. Fall] $\bar{x} = x_i$ für ein $i \in \{0,...,n\}$, so ist wegen $p(x_i) - f(x_i) = 0$ nichts zu zeigen.
  
  \item[2. Fall] $\bar{x} \neq x_i$ für alle $i \in \{0,...,n\}$. Sei $\bar{p}$ das Interpolationspolynom mit $\text{deg}(\bar{p}) \leq n+1$ zu $(x_i, f(x_i))_{i=0}^n$ und $(\bar{x}, f(\bar{x}))$. Die Newton'sche Interpolationsformel liefert dann
  \begin{align*}
  \bar{p}(x) &= p(x) + \prod_{i=0}^n(x-x_i)\delta^{n+1}y[x_0,...,x_n, \bar{x}] &\\
  &\underset{(9.3)}{=} p(x) + \prod_{i=0}^n(x-x_i) \frac{f^{(n+1)}(\xi)}{(n+1)!}
  \end{align*}
  Für $x = \bar{x}$ gilt $\bar{p}(\bar{x}) = f(\bar{x})$. Damit ist Satz (9.1) für $x\in[a,b]$ gezeigt.\qed
\end{description}
\end{nothing}

\underline{Fragen:}
\begin{itemize}
  \item Für welche Wahl der Stützstellen $x_i$ ($i=0,...,n$, n fest) ist 
    $$\max_{x\in[a,b]} \left\vert \prod_{i=0}^n(x-x_i)\right\vert$$
    minimal? (Siehe Abschnitt 10)
  \item Wie wirken sich Fehler in den Funktionsauswertungen (etwa Messfehler oder Rechenfehler) auf das Interpolationspolynom aus?
\end{itemize}

\begin{theorem}[Lagrange Interpolationsformel]
Das Interpolationspolynom $p$ zu $(x_i, y_i)_{i=0}^n$ ist gegeben durch 
$$p(x) = \sum_{i=0}^n y_il_i(x)$$
mit 
$$l_i(x) = \frac{\prod_{j=0,\thinspace j\neq i}^n (x-x_j) }{\prod_{j=0,\thinspace j\neq i}^n ( x_i-x_j)}$$
\begin{proof}[Beweis]
$\text{deg}(l_i) = n$, $l_i(x_j) = \left\{
\begin{array}{ll}
1 & \textrm{für } j=i \\
0 & \, \textrm{sonst} \\
\end{array}
\right.$ \\
$\Rightarrow p(x_j) = \sum_{i=0}^n y_il_i(x_j) = y_j$
\end{proof}
\end{theorem}

\begin{comment*}
Lagranges und Newtons Interpolationsformeln liefern beide das gleiche Polynom nur in unterschiedlichen Darstellungen.
\end{comment*}

\begin{definition}
$$\Lambda_n := \max_{x\in[a,b]} \sum_{i=0}^n\vert l_i(x) \vert$$
heißt die \textbf{Lebesgue Konstante} zu den Stützstellen $x_i$, $i=0,...,n$ auf dem Intervall $[a,b]$.
\end{definition}

Damit gilt:

\begin{theorem}
Sei $p$ das Interpolationspolynom (vom Grad $\leq n$) zu $(x_i, y_i)_{i=0}^n$ und $\tilde{p}$ das Interpolationspolynom zu $(x_i, \tilde{y}_i)_{i=0}^n$, so gilt:
$$ \max_{x \in [a,b]} \vert p(x) - \tilde{p}(x) \vert \leq \Lambda_n \max_{i=0,...,n}\vert y_i - \tilde{y}_i \vert $$
\begin{proof}[Beweis]
klar
\end{proof}
\end{theorem}

\begin{example}
\begin{description}\item \end{description}
\begin{itemize}
  \item Für äquidistante Stützstellen $x_i = a + i\frac{b-a}{n}$ ($i=0,...,n$) ist 
    \begin{align*}
    \Lambda_{10} &\approx 40 &\\
    \Lambda_{20} &\approx 3 \cdot 10^4 &\\
    \Lambda_{40} &\approx 10^{10} &\\
    \Lambda_{n} &\approx \frac{2^n}{ \ln(n) \cdot e \cdot n} \quad \text{für } n \rightarrow \infty&\\
    \end{align*}
    $\Rightarrow$ Vorsicht bei Polynominterpolation mit vielen äquidistanten Stützstellen! \\
    In $\S$10 werden wir Stützstellen kennenlernen mit $\Lambda_n \leq 4$ für $n \leq 100$.
\end{itemize}
\end{example}

\begin{theorem}
Sei $f: [a,b] \rightarrow \mathbb{R}$ stetig, $p$ Interpolationspolynom zu $f$ in den Stützstellen $x_0,..., x_n \in [a,b]$. So gilt:
\[ \forall q \in \mathcal{P}_{n+1}: \quad \max_{x \in [a,b] } \vert f(x) -p(x) \vert \leq (1+ \Lambda_n) \max_{x \in [a,b]} \vert q(x) - f(x) \vert .\]
Hierbei ist $\Lambda_n$ die Lebesgue-Konstante zu $(x_i)_{i=0}^n$ auf $[a,b]$.
\begin{proof}[Beweis]
Sei $q \in \mathcal{P}$.
\[ f-p = (f-q) + (q-p)\]
$q$ ist das Interpolationspolynom zu sich selbst in den $x_0,..., x_n$. Nach (9.7) gilt für $y_i = f(x_i)$ $\tilde{y_i}  = q(x_i)$.
\begin{align*}
\max_{x \in [a,b]} \vert p(x) - q(x) \vert &\leq \Lambda_n \max_{i=0,...,n} \vert f(x_i) - q(x_i) \vert &\\
&\leq \Lambda_n \max_{x \in [a,b]} \vert f(x)-q(x) \vert &\\
\Rightarrow \max_{x \in [a,b]} \vert f(x) - p(x) \vert &\leq \max_{x \in [a,b]} \vert f(x)-q(x) \vert + \max_{x \in [a,b]} \vert p(x)-q(x) \vert &\\
&\leq (1+ \Lambda_n) \max_{x \in [a,b]} \vert q(x) - f(x) \vert 
\end{align*}
\end{proof}
\end{theorem}

\subsection{Tschebyscheff-Interpolation}

\underline{Ziel:} Interpoliere $f: [a,b] \rightarrow \mathbb{R}$ in "guten" Stützstellen. \\
Ohne Einschränkungen sei $[a,b] = [-1, 1]$

\begin{definition}
$T_n(x) = \cos(n \cdot \arccos(x))$ für $x \in [-1, 1]$ heißt n-tes Tschebyscheff-Polynom.
\end{definition}

\begin{lemma}
$T_n(x)$ ist für $x \in [-1, 1]$ ein Polynom mit folgenden Eigenschaften:
\begin{enumerate}
  \item[i)] $T_0(x) = 1$, $T_1(x) = x$
  \item[ii)] $T_n(x) = 2^{n-1} x^n + r(x)$ mit $r_n \in \mathcal{P}_{n-1}$
  \item[iii)] $T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)$
  \item[iv)] $\forall x \in [-1, 1]: \quad \vert T_n(x) \vert \leq 1$
  \item[v)] $T_n(\cos(\frac{k\pi}{n})) = (-1)^k, \quad k=0,...,n$
  \item[vi)] $T_n(\cos(\frac{(2k+1)\pi}{2n})) = 0, \quad k=0,...,n-1$ 
\end{enumerate}
\begin{proof}[Beweis]\leavevmode
\begin{enumerate}
  \item[i)] klar, da $T_0(x) = \cos(0) = 1$, $T_1(x) = \cos(\arccos(x)) = x, \quad x \in [a,b]$
  \item[iii)]$\cos(\left(n+1\right)\phi) + \cos(\left(n-1\right)\phi)$\\
    $= \medspace \cos(n\phi)\cos(\phi) - \sin(n\phi)\sin(\phi) + \cos(n\phi)\cos(-\phi) - \sin(n\phi)\sin(-\phi)$ \\
    $= \medspace 2\cos(n\phi)\cos(\phi)$
  \item[ii)] folgt aus i) und iii)
  \item[iv)] klar, da $\cos: [-1, 1] \rightarrow \mathbb{R}$
  \item[v) + vi)] ebenfalls klar, da $T_n(\cos(\frac{k\pi}{n})) = \cos(n \frac{k\pi}{n}) = \cos(k\pi) = (-1)^k$\\
  analog: $T_n(\cos(\frac{(2k+1)\pi}{2n})) = \cos(n \frac{(2k+1)\pi}{2n})  = 0$
\end{enumerate}
\end{proof}
\end{lemma}

\begin{example} \leavevmode
\begin{description}
  \item $T_2(x) = 2x^2 - 1$
  \item $T_3(x) = 2x(2x^2 -1) - x = 4x^3 -3x$
\end{description}
\end{example}

\begin{lemma}
Sei $q \in \mathcal{P}_n$, $q(x) = 2^{n-1} x^n + r(x)$ mit $r(x) \in \mathcal{P}_{n-1}, q \neq T_n$. Dann gilt
\[ \max_{x\in [-1, 1]} \vert q(x) \vert > \max_{x\in [-1, 1]} \vert T_n(x) \vert \quad(=1)\]

\begin{proof}[Beweis]\leavevmode
Annahme: $\forall x \in [-1, 1]: \quad \vert q(x) \vert \leq 1$
\begin{description}
  \item $T_n(1) = 1$
  \item $T_n(cos(\frac{\pi}{n})) = -1$
\end{description}
Nach dem Zwischenwertsatz hat $q-T_n$ eine Nullstelle im Intervall $[cos(\frac{\pi}{n}), 1]$. Falls ein "Randpunkt" $x$ eine Nullstelle ist, so handelt es sich um eine doppelte Nullstelle, da $q'(x) = 0 = T_n'(x)$. Ebenso existiert in $[cos(\frac{2\pi}{n}), cos(\frac{\pi}{n})]$ und allgemein in $[cos(\frac{(k+1)\pi}{n}), cos(\frac{k\pi}{n})]$ für $k=0,...,n-1$.\\
Nullstelle $\Rightarrow$ $q-T_n$ hat n Nullstellen.\\
Andererseits ist $q-T_n \in \mathcal{P}_{n-1} \quad \Rightarrow q-T_n \equiv 0 \quad \Rightarrow q = T_n \quad \lightning$
\end{proof}
\end{lemma}

\begin{theorem}
Unter allen Unterteilungen $\{x_0,...,x_n\}$ von $[-1,1]$ wird 
$$ \max_{x \in [-1, 1]} \vert (x-x_0)(x-x_1)...(x-x_n) \vert $$
minimal für $x_k = cos\left( \frac{2k+1}{n+1} \frac{\pi}{2}\right), \quad k=0,...,n$ (d.h. $x_k$ sind die Wurzeln von $T_{n+1}$)

\begin{proof}[Beweis]
Nach Lemma (10.4) wird $\max_{x \in [-1, 1]} \vert (x-x_0)(x-x_1)...(x-x_n) \vert$ minimal gdw.  $(x-x_0)...(x-x_n) = 2^{n} T_{n+1}(x)$, d.h. falls $x_k$ Wurzeln von $T_{n+1}$ sind.
\end{proof}
\end{theorem}

\begin{theorem}
Die Lebesguekonstanten $\Lambda_n$ zu den Tschebyscheffknoten (Wurzeln von $T_{n+1}$) erfüllen
\begin{description}
  \item $\Lambda_n \leq 3$ für $n\leq 20$
  \item $\Lambda_n \leq 4$ für $n\leq 100$
  \item $\Lambda_n \approx \frac{2}{\pi} log(n)$ für $n \rightarrow \infty$
\end{description}
\begin{proof}[Beweis]
ohne Beweis.
\end{proof}
\end{theorem}
Nach Satz (9.9) liefert die Interpolation in den Wurzeln der Tschebyscheffpolynome eine fast optimale Polynominterpolation an $f$.\\
Dazu kommen Eigenschaften, die die Berechnung eines Interpolationspolynoms in den Tschebyscheffknoten (Wurzeln der Tschebyscheffpolynome) vereinfachen.

\begin{lemma}
Die Tschebyscheffpolynome sind orthogonal, bzgl. des Skalarprodukts 
\[ \langle f, g \rangle := \int_{-1}^1 f(x)g(x) \frac{1}{\sqrt{1-x^2}} dx\]

\begin{proof}[Beweis]
Übungsaufgabe
\end{proof}
\end{lemma}

\begin{lemma}
Die Tschebyscheffpolynome $T_k, k = 0, ..., n$ sind orthogonal bzgl. des Skalarprodukts (auf $\mathcal{P}_n$)
\[(f, g) := \sum_{l=0}^n f(x_l)g(x_l), \quad \text{wobei } x_0, ..., x_n \medspace \text{Wurzeln von } T_{n+1}(x) \]

\begin{proof}[Beweis]\leavevmode
\begin{align*}
T_k(x_l) &= \text{cos}(k \cdot \text{arccos}( \text{cos} ( \frac{2l-1}{n+1} \frac{\pi}{2}))) &\\
&= \text{cos}( k \frac{2l+1}{n+1} \frac{\pi}{2}) &\\
&= \text{cos}(k (l+ \frac{1}{2})h )
\end{align*}
für $h = \frac{\pi}{n+1}$\\
Damit ist 
\begin{align*}
(T_k, T_j) &= \sum_{l=0}^n \text{cos}(k(l+\frac{1}{2})h) \cdot \text{cos}(j(l+\frac{1}{2})h), &\\
\intertext{da $\text{cos}(x)\text{cos}(y) = \frac{1}{2}(\text{cos}(x+y) + \text{cos}(x-y))$}
&= \frac{1}{2} \sum_{l=0}^n \text{cos}((k+j)(l+\frac{1}{2})h) \cdot \text{cos}((k-j)(l+\frac{1}{2})h) &\\
\intertext{Es gilt: $\text{cos}(x) = \text{Re} (e^{ix})$}
&= \frac{1}{2} \text{Re}\left( \sum_{l=0}^n e^{i(k+j)(l+\frac{1}{2})h} + e^{i(k-j)(l+\frac{1}{2})h}\right) &\\
&= \frac{1}{2} \text{Re}\left( \sum_{l=0}^n \left( e^{i(k+j)lh}e^{i(k+j)\frac{h}{2}} + e^{i(k-j)lh}e^{i(k-j)\frac{h}{2}}\right)\right) &\\
&= \frac{1}{2} \text{Re} \left( e^{i(k+j)\frac{h}{2}} \frac{e^{i(k+j)h(n+1)} -1}{ e^{i(k+j)h} - 1} + e^{i(k-j)\frac{h}{2}} \frac{e^{i(k-j)h(n+1)} -1}{ e^{i(k-j)h} - 1}\right), \quad \text{für }k \neq j &\\
\intertext{Es gilt $k(n+1) = \pi$}
&\overset{\text{Behauptung}}{=} \begin{cases}
0 & k \neq j \\
\frac{1}{2} (n+1) & k=j\neq 0 \\
(n+1) & k = j = 0
\end{cases}
\end{align*}
\begin{description}
  \item[Fall 1:] $k=j=0 \Rightarrow \frac{1}{2} \sum_{l=0}^n(1+1) = (n+1)$
  \item[Fall 2:] $k=j\neq0 \Rightarrow \frac{1}{2} \text{Re} \left( (n+1) + e^{ijh} \underbrace{\frac{e^{i2j\overbrace{(n+1)h}^{= \pi}}-1}{e^{i2jh}-1}}_{=0}\right) = \frac{1}{2} (n+1)$ 
  \item[Fall 3:] $k \neq j$:
    \begin{description}
      \item[Fall 1:] $k+j$ ist gerade $\Rightarrow$ $k-j$ ist gerade \\
        $\Rightarrow \medspace \frac{1}{2} \text{Re} \left(0+0\right) = 0$
      \item[Fall 2:] $k+j$ ist ungerade $\Rightarrow$ $k-j$ ist ungerade \\
        \begin{align*}
        &\Rightarrow \medspace  \frac{1}{2} \text{Re} \left( e^{i(k+j)\frac{h}{2}} \frac{-2}{ e^{i(k+j)h} - 1} + e^{i(k-j)\frac{h}{2}} \frac{-2}{ e^{i(k-j)h} - 1}\right)&\\
        &= \frac{1}{2} \text{Re} \left(\underbrace{\frac{-2}{ e^{i(k+j)\frac{h}{2}} + e^{-i(k+j)\frac{h}{2}}}}_{\text{rein imaginär}} +  \underbrace{\frac{-2}{ e^{i(k-j)\frac{h}{2}} - e^{-i(k-j)\frac{h}{2}}}}_{\text{rein imaginär}}\right)&\\
        &= 0
        \end{align*}
    \end{description}
\end{description}
\end{proof}
\end{lemma}

\begin{comment*}
$(\thinspace \cdot \thinspace, \thinspace \cdot \thinspace )$ ist ein Skalarprodukt auf $\mathcal{P}_n$, da 
\begin{enumerate}
  \item[i)] bilinear
  \item[ii)] symmetrisch
  \item[iii)] positiv definit \\
    $(f,f) = \sum_{l=0}^n f(x_l)^2 \geq 0$\\
    $(f,f) = 0 \overset{!}{\Rightarrow} f \equiv 0$\\
    $\sum_{l=0}^n f(x_l)^2 = 0 \Rightarrow \forall l: \medspace f(x_l) = 0$
    $\Leftrightarrow f \equiv 0$, da $\text{deg}(f) \leq n$ 
\end{enumerate}
\end{comment*}

\begin{theorem}
Sei $p$ das Interpolationspolynom zur Funktion $f$ in den Tschebyscheffknoten $x_0, \dots, x_n$ (Wurzeln von $T_{n+1}$), so gilt:
\begin{align*}
&p(x) = \frac{1}{2} c_0 + \sum_{j=1}^n c_j T_j(x), &\\
\intertext{wobei}
&c_k = \frac{2}{n+1} \sum_{l=0}^n f(x_l) \cos\left(k \frac{2l+1}{n+1}\frac{\pi}{2}\right), \quad \text{für $k=0,...,n$}
\end{align*}

\begin{proof}[Beweis]
Betrachte $(p, T_k)$
\begin{align*}
(p, T_k) &= \frac{1}{2} (T_0, T_k) + \sum_{l=1}^n c_l (T_l, T_k) &\\
&= \begin{cases}
c_k (T_k, T_k) & \text{für }k \neq 0 \\
\frac{1}{2} c_0 (T_0, T_0)  & \text{für }k = 0 
\end{cases}&\\
&\overset{(10.8)}{=} \frac{n+1}{2} c_k&\\
&= \frac{n+1}{2} \sum_{l=0}^n f(x_l) T_k(x_l) &\\
&= \frac{n+1}{2} \sum_{l=0}^n f(x_l) \cos\left(k \underbrace{\arccos\left(\cos\left(\frac{2l+1}{n+1}\frac{\pi}{2}\right)\right)}_{ = \frac{2l+1}{n+1}\frac{\pi}{2}}\right) &\\
\end{align*}
\end{proof}
\end{theorem}
$p(x)$ lässt sich als bei bekannten Koeffizienten $c_k$ leicht berechnen/auswerten.

\begin{theorem}[Clenshaw Algorithmus]
Sei $p \in \mathcal{P}_n$ durch die Koeffizienten $c_0, ..., c_n$ in der Form
\begin{align*}
&p(x) = \frac{1}{2} c_0 + \sum_{j=1}^n c_j T_j(x)&\\
\intertext{gegeben. Setzt man }
&d_{n+1} = d_{n+2} = 0 &\\
\intertext{und definiert für $x$}
&d_k = c_k + 2x d_{k+1} - d_{k+2}, \quad \text{für }k=n,n-1,...,1, 0&\\
\intertext{so gilt:}
&p(x) = \frac{1}{2} (d_0-d_2)&\\
\end{align*}
\begin{proof}[Beweis]
Verwende die Rekursionsformel aus (10.2) iii) ($T_{k+1} = 2x T_k + T_{k-1}$). Dann ist 
\begin{align*}
p(x) &= \frac{1}{2} c_0 + \sum_{l=1}^n c_l T_l(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-3} c_l T_l(x) + c_{n-2} T_{n-2}(x) + c_{n-1} T_{n-1}(x) + c_{n} T_{n}(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-3} c_l T_l(x) + (c_{n-2} - \underbrace{c_n}_{=d_n}) T_{n-2}(x) + \underbrace{(c_{n-1} + 2xc_n)}_{=d_{n-1}} T_{n-1}(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-4} c_l T_l(x) + (c_{n-3} - d_{n-1}) T_{n-3}(x) + \underbrace{(c_{n-2} - d_n + 2xd_{n-1})}_{=d_{n-2}} T_{n-2}(x) &\\
\intertext{induktiv erhält man}
&= \left(\frac{1}{2} c_0 - d_2\right) \underbrace{T_0(x)}_{=1} + \underbrace{(c_1 - d_3 + 2xd_2)}_{=d_1} \underbrace{T_1(x)}_{=x} &\\
&= \frac{1}{2} (\underbrace{c_0 - 2d_1x - d_2}_{=d_0} -d_2) &\\
&= \frac{1}{2} (d_0-d_2)
\end{align*}
\end{proof}
\end{theorem}

\begin{comment*}
Bei der Verwendung von Rekursionen ist es wichtig zu verstehen, wie sich Rundungsfehler auswirken. 
\begin{description}
  \item[Beispiel:]
    $x_{n+1} = 10x_n - 9, \quad x_0 = 1$ \\
    $ \Rightarrow \forall n \in \mathbb{N}: \medspace x_n = 1$ \\
    Was passiert bei fehlerhafter Startwerten $\tilde{x}_0 = 1+ \varepsilon$? \\
    $ \tilde{x}_{n+1} = 10 \tilde{x}_n - 9, \quad \tilde{x}_n = 1+ 10^n \varepsilon$
\end{description}
Der Clenshaw-Algorithmus ist stabil, wie im Folgenden gezeigt wird:
\end{comment*}

\begin{theorem}
Für den Clanshaw-Algorithmus mit Fehlern $\varepsilon_k$ in der Rekursion, d.h. für 
\begin{align*}
&\tilde{d}_{n+1} = \tilde{d}_{n+2} = 0 &\\
&\tilde{d}_k = c_k + 2x \tilde{d}_{k+1} - \tilde{d}_{k+2} + \varepsilon_k, \quad k=n, n-1, ..., 0 &\\
\intertext{Dabei ist $\varepsilon_k$ der Rundungsfehler in der k-ten Iteration. Für
$\tilde{p}(x) = \frac{1}{2} (\tilde{d}_0 - \tilde{d}_2)$ gilt:}
& \vert \tilde{p}(x) - p(x) \vert \leq \sum_{j=0}^n \vert \varepsilon_j \vert, \quad \text{für } \vert x \vert < 1,
\intertext{wobei $p(x)$ mit (10.10) berechnet wird.}
\end{align*}

\begin{proof}[Beweis]
Setze $\varepsilon_k := \tilde{d}_k - d_k$ (für $d_k$ aus (10.10)). Dann gilt:
\begin{align*}
& \varepsilon_k = \varepsilon_k + 2x \varepsilon_{k+1} - \varepsilon_{k+2}, \quad \text{für } k=n,...,0 &\\
&\varepsilon_{n+1} = 0 \quad \text{und} \quad \varepsilon_{n+2} = 0
\end{align*}
Mit Satz (10.10) gilt für $c_k = \varepsilon_k$ und $d_k = \varepsilon_k$:
\begin{align*}
& \frac{1}{2} (\varepsilon_0 -\varepsilon_2) = \frac{1}{2} \varepsilon_0 + \sum_{j=1}^n \varepsilon_j T_j(x) 
\intertext{Da $\vert T_j(x) \vert \leq 1$ für $x \in [1, 1]$ gilt:}
& \vert \tilde{p}(x) - p(x) \vert \overset{\Delta-UGL}{\leq} \frac{1}{2} \vert \varepsilon_0 \vert + \sum_{j=1}^n \vert \varepsilon_j \vert  
\end{align*}
\end{proof}
\end{theorem}

\begin{comment*}
Die Approximation einer Funktion durch die Summe von Tschebyscheffpolynomen wird im Computer zur Berechnung von Funktionen wie log, exp, sin, cos,... verwendet.
\end{comment*}

\begin{example}
\underline{Ziel:} Berechne $\ln(x)$ für $0 \leq x_{\min} < x \leq x_{\max}$. $x_{\min}, x_{\max}$ ist die kleinste/größte positive darstellbare Zahl auf dem gegebenen Computer. \\
$x \medspace \text{"="} \medspace \underbrace{\left[ 1, b_1, b_2, ..., b_M \right]}_{\text{"Mantisse"}} * 2^N, \quad b_j \in \{0,1\}$\\
d.h. $ x = 2^N (1+ b_1 \frac{1}{2} + b_2 \frac{1}{4} + ... + b_M \frac{1}{2^M}) = 2^N (1+t), \quad t \in (0,1)$ \\
$\ln(x) = \ln(1+t) + N \underbrace{\ln(2)}_{\text{Konstante}}$ \\
Das Problem $\ln(x)$ zu berechnen ist damit auf das Problem $\ln(1+t)$ für $t \in [0,1]$ zu berechnen reduziert worden. \\
Tschebyscheffinterpolation: $[-1, 1] \rightarrow [0,1], \medspace x \mapsto t = \frac{1+x}{2} \quad (\Leftrightarrow x=2t-1)$ \\
Für den Interpolationsfehler gilt:
\begin{align*}
\ln\left(1+ \frac{1+x}{2}\right) -p(x) &= \underbrace{\prod_{j=0}^n (x-x_j)}_{=2^{-n} \text{ für Tschebyscheff}} \frac{1}{(n+1)!} \frac{(-1)^{n-1} (n-1)!}{\left(1+\frac{1+\xi}{2}\right)^n} \left(\frac{1}{2}\right)^n, \quad \xi \in [-1, 1] &\\
\Leftrightarrow \quad \left\vert \ln\left(1+ \frac{1+x}{2}\right) -p(x) \right\vert &= \frac{1}{4^n} \frac{1}{(n+1)^n}
\end{align*}
Für n=15 ist $\frac{1}{4^n} \frac{1}{(n+1)^n} \leq 10^{-11}$ \\
Berechnet werden also $c_0,..., c_{15}$ (einmal für alle Zeiten):
\begin{align*}
c_0 &= 0.75290562... &\\
c_1 &= 0.34... &\\
c_2 &= -0.029... &\\
c_3 &= 0.0036... &\\
c_4 &= -0.00004 &\\
\vert c_k \vert &\leq 10^{-9}, \quad \text{für }k >10 &\\
\end{align*}
Beobachtung: $c_k$ werden schnell klein.\\
Um eine Genauigkeit von $10^{-8}$ (einfache Genauigkeit) zu erreichen, benötigt man nur $c_0,..., c_9$.\\
Die Auswertung mit dem Clenshaw-Algorithmus benötigen wir 10 Multiplikation $\left(\text{vgl. Taylor }\log(1+t) = \sum_{k=1}^{\infty} \frac{(-1)^k}{k}t^k\right)$.
\end{example}

\subsection{Hermit\'{e}-Interpolation}

Gegeben sind $(x_i, y_i, y_i')_{i=0}^n, \quad x_i \in [a,b]$ paarweise verschieden. Gesucht ist ein Polynom $p \in \mathcal{P}$, sodass
\begin{align*}
&p(x_i) = y_i \quad \text{und } &\\
&p'(x_i) = y_i', \quad \text{für } i=0,...,n. &\\
\end{align*}
\underline{Idee:} Lasse $\varepsilon \rightarrow 0$ laufen im Newtonschema:

\begin{tabular}{lll}
 
$x_0$ & $y_0$\\
 & & $\delta y[x_0, x_0+\varepsilon] = \frac{(y_0 + \varepsilon y_0') - y_0}{(x_0 +\varepsilon) - x_0} = y_0'$\\
$x_0+\varepsilon$ & $y_0 + \varepsilon y_0'$\\
 & & $\delta y[x_0+\varepsilon, x_1] \underset{\varepsilon \rightarrow 0}{\rightarrow} \delta y[x_0,x_1]$\\
$x_1$ & $y_1$\\
 & & $\delta y[x_1, x_1+\varepsilon] = y_1'$\\
$x_1+\varepsilon$ & $y_1+\varepsilon y_1'$\\
 
\end{tabular} \\ \\
Newtonsche Interpolationsformel:
\begin{align*}
p_{\varepsilon}(x) = y_0 &+ (x-x_0) \delta y[x_0, x_0+\varepsilon] &\\
&+ (x-x_0)(x-(x_0 + \varepsilon)) \delta^2y[x_0, x_0 + \varepsilon, x_1] &\\
&+ \dots &\\
&+ \left( \prod_{j=0}^{n-1} (x-x_j)(x-(x_j + \varepsilon))\right)(x-x_n)\delta^{2n+1} y[x_0,\dots, x_n + \varepsilon]
\end{align*}
damit ist:
\begin{align*}
p_{\varepsilon}(x_i) &= y_i &\\
p_{\varepsilon}(x_i + \varepsilon) &= y_i + \varepsilon y_i' &\\
\Rightarrow \quad y_i' &= \frac{p_{\varepsilon}(x_i + \varepsilon) - p_{\varepsilon}(x_i)}{\varepsilon} \underset{MWS}{=} p'_{\varepsilon}(\xi_i), \quad \text{für } \xi_i \in [x_i, x_i+\varepsilon]
\end{align*}
Für $\varepsilon \rightarrow 0$ definieren wir
\begin{align*}
\delta^k y[x_0, x_0, x_1, x_1, ...] &:= \lim_{\varepsilon \rightarrow 0} \delta^k y[x_0, x_0+\varepsilon, x_1, x_1+\varepsilon, ...] &\\
\end{align*}
und
\begin{align*}
p(x) &:= \lim_{\varepsilon \rightarrow 0} p_{\varepsilon}(x) &\\
&= y_0 + (x-x_0) \underbrace{\delta y[x_0, x_0]}_{y_0'} + (x-x_0)^2 \delta^2 y[x_0, x_0, x_1] &\\
&+ (x-x_0)^2(x-x_1) \delta^3 y[x_0, x_0, x_1, x_1] &\\
&+ ... + \prod_{j=0}^{n-1}(x-x_j)^2 (x-x_n) \delta^{2n-1}y[x_0,x_0, ..., x_n, x_n] &\\
p(x_i) &= \lim_{\varepsilon \rightarrow 0} p_{\varepsilon}(x_i) = y_i &\\
p'(x_i) &= \lim_{\varepsilon \rightarrow 0} p'_{\varepsilon}(x_i) = \lim_{\varepsilon \rightarrow 0}(\xi_{i, \varepsilon}) = y_i'
\intertext{für $\xi_{i, \varepsilon} \in [x_i, x_i+\varepsilon]$}
\end{align*}
\underline{Schema:} \\
\begin{tabular}{llllll}
 
$x_0$ & $y_0$\\
 & & $y_0'$\\
$x_0$ & $y_0 $ & & $ \delta^2[x_0, x_0, x_1]$\\
 & & $\delta y[x_0,x_1]$ & & $ \delta^3[x_0, x_0, x_1, x_1]$\\
$x_1$ & $y_1$ & & $ \delta^2[x_0, x_1, x_1]$ && $\dots$\\
 & & $y_1'$ && $\dots$\\
$x_1$ & $y_1$ & & $ \delta^2[x_1, x_1, x_2]$ && $\dots$\\
 & & $\delta y[x_1, x_2]$ && $\dots$\\
$x_2$ & $y_2$ && $\dots$\\
 & & $y_2'$\\
$x_2$ & $y_2$\\
 
\end{tabular} \\ \\
\underline{Eindeutigkeit:} \\
Annahme: $\exists q \in \mathcal{P}_{2n+1}$ mit $q(x_i) = y_i)$, $q'(x_i) = y_i'$\\
Dann ist $q-p \in \mathcal{P}_{2n+1}$ \\
$q-p$ besitzt doppelte Nullstelle in $x_i$ \\
$q-p = c \prod(x-x_i)^2$, da $\text{deg}\left(\prod_{i=0}^n (x-x_i)^2 \right) = 2n+2$ \\
$\Rightarrow \medspace c=0 \quad \Rightarrow \medspace q=p$ \\\\
Damit ist der folgende Satz bewiesen.

\begin{theorem}
Zu gegebenen $(x_i, y_i, y_i')_{i=0}^n$ mit $x_i \neq x_j$, falls $i \neq j$ existiert ein eindeutiges Polynom $p \in \mathcal{P}_{2n+1}$ mit $p(x_i) = y_i$ und $p'(x_i) = y_i'$ ($i=0,\dots, n$). $p$ kann mit Hilfe des Newtonschen Differenzenschemas mit doppelten eingeschriebenen Nullstellen (Knoten) berechnet werden.
\end{theorem}

\begin{theorem}[vgl. Satz (9.1)]
Sei $f: [a,b] \rightarrow \mathbb{R}$ $(2n+2)$-mal stetig differenzierbar ($f \in \mathcal{C}^{2n+2}([a,b], \mathbb{R})$), seien $x_0,..., x_n$ paarweise verschieden und sei $p$ Hermit\'{e}polynom aus (11.1) zu $(x_i,y_i, y_i')_{i=0}^n$. Dann gilt:
\[ \forall x \in [a,b] \exists \xi \in [a,b]: \medspace f(x)-p(x) = \prod_{j=0}^n (x-x_j)^2 \frac{f^{(2n+2)} (\xi)}{(2n+2)!} \]

\begin{proof}[Beweis]
Betrachte $\varepsilon \rightarrow 0$ für $p_{\varepsilon}(x)$ in der Fehlerformel (9.1):
\begin{align*}
f(x)-p(x) &= \prod_{j=0}^n (x-x_j)(x-(x_j+\varepsilon)) \frac{f^{(2n+2)}(\xi_{\varepsilon})}{(2n+2)!}, \quad \text{für } \xi_{\varepsilon} \in [a,b]
\end{align*}
Sei $\xi$ ein Häufungspunkt von $\{\xi_{\varepsilon}, \varepsilon > 0\}$. Dann existiert eine Nullfolge $(\varepsilon_k)_{k\in \mathbb{N}}$ mit $\xi_{\varepsilon_k} \rightarrow \xi$ für $k \rightarrow \infty$. $\Rightarrow$
\begin{align*}
f(x)-p(x) &= \lim_{k \rightarrow \infty} (f(x) - p_{\varepsilon_k}(x)) &\\
&= \prod_{j=0}^n (x-x_j)^2 \frac{f^{(2n+2)}(\xi)}{(2n+2)!}
\end{align*}
\end{proof}
\end{theorem}

\subsection{Spline-Interpolation}
Spline ist engl. für Holz- oder Metallfeder.\\
\underline{Theorie:} stammt von Schoenenberg aus dem Jahr 1946\\
\underline{Idee:} Suche 'glatte' Funktion $s$ durch vorgegebene Punkte $(x_i, y_i)_{i=0}^n$
\begin{enumerate}
  \item[i)] $s(x_i)= y_i$ ($i=0,...,n$)'Interpolationseigenschaft'
  \item[ii)] $s$ muss mind. 2-mal stetig differenzierbar sein und $ \int_a^b (s''(x))^2dx $ soll minimal sein. 'glatt'
\end{enumerate}
Dadurch vermeidet man Oszillationen, wie sie bei der Polynominterpolation hohen Grades entstehen.\\
Wir suchen also eine Funktion $s$, sodass für $\varepsilon \in \mathbb{R}$ und $h \in \mathcal{C}^2([a,b], \mathbb{R})$, $h(x_i) = 0 \medspace (i=0,\ldots,n)$ und 
\begin{align*}
\int_a^b (s''(x))^2 dx &\overset{!}{\leq} \int_a^b \left( (s(x) + \varepsilon h(x))'' \right)^2 dx &\\
&= \int_a^b (s''(x) + \varepsilon h''(x))^2 dx &\\
&= \int_a^b (s''(x))^2 dx + 2\varepsilon \int_a^b s''(x)h''(x) dx + \underbrace{\varepsilon^2\int_a^b (h''(x))^2dx}_{ \geq 0 } 
\end{align*}
Obige Ungleichung ist erfüllt, falls 
$$ \forall h \in \mathcal{C}^2([a,b]) \medspace \text{mit }h(x_i) = 0: \medspace \int_a^b h''(x) s''(x) dx = 0$$
Dabei gilt:
$$ \int_a^b h''(x) s''(x) dx = \left[ s''(x) h'(x) \right]_{x=a}^b - \int_a^b s'''(x) h'(x)dx$$
Falls $s'''(x) = \alpha_i$ für $x \in [x_{i-1}, x_i]$, dann ist 
\begin{align*}
\int_a^b s'''(x) h'(x) dx &= \sum_{i=1}^n \alpha_i \int_{x_{i-1}}^{x_i} h'(x) dx &\\
&= \sum_{i=1}^n \alpha_i \left(\underbrace{h(x_i)}_{= 0} - \underbrace{h(x_{i-1}}_{= 0}\right) &\\
&= 0
\end{align*}
$\Rightarrow$ Forderung: $\left[s''(x) h'(x) \right]_{x=a}^b = s''(b)h'(b) - s''(a)h'(a) \overset{!}{=} 0$

\begin{theorem}
Seien $f, s \in \mathcal{C}^2 ([a,b], \mathbb{R})$ zwei Funktionen, die in $a=x_0 < x_1 < ... < x_n = b$ dieselben Werte annehmen, d.h. 
\begin{align*}
f(x_i) = s(x_i) \medspace &(i=0,...,n)
\quad \text{und } \quad 
s\,\rule[-2mm]{0.1mm}{5mm}_{\thinspace [x_{i-1}, x_i]} &\in \mathcal{P}_3 \quad \text{für } i=1,...,n
\intertext{Falls}
s''(a) [f'(a) - s'(a)] &= s''(b) [f'(b) - s'(b)], \quad (*) 
\intertext{so gilt:}
\int_a^b (s''(x))^2dx &\leq \int_a^b(f''(x))^2dx
\end{align*}
\begin{proof}[Beweis]
Obige Rechnung für $h=f-s$ und $\varepsilon = 1$, $h(x_i) = 0$ \\
$[s''(x) h'(x)]_{x=a}^b = 0 \Leftrightarrow (*)$
\end{proof}
\end{theorem}

\begin{comment}
Die Bedingung (*) kann erreicht werden durch 
\begin{enumerate}
  \item[a)] Vorgabe von $s'(a) = f'(a)$, $s'(b) = f'(b)$ \\
  Der dadurch bestimmte Spline heißt \textbf{eingespannter} Spline.
  \item[b)] Vorgabe von $s''(a) = 0 = s''(b)$ \\
  Der dadurch bestimmte Spline heißt \textbf{natürlicher} Spline. Dieser hat aber schlechtere Approximationseigenschaften. 
\end{enumerate}
\end{comment}

\begin{nothing}[Konstruktion des Splines]
Gegeben sind $(x_i, y_i)$ $i=0,...,n$, $a = x_0 < x_1 < ... < x_n = b$, $s\,\rule[-2mm]{0.1mm}{5mm}_{\thinspace [x_{i-1}, x_i]} =: s_i \in \mathcal{P}_3$. \\
Hermite-Interpolation:
\begin{align*}
&s_i(x_i) = y_i &\\
&s_i(x_{i-1}) = y_{i-1} &\\
&s_i'(x_i) = \tau_i &\\
&s_i'(x_{i-1}) = \tau_{i-1}
\end{align*}
Dabei sind $\tau$ unbekannte Steigungen. \\
Ansatz: 
\begin{align*}
s_i(x) &= y_{i-1} + (x-x_{i-1}) \delta y[x_{i-1}, x_i] + (x-x_{i-1})(x-x_i) \left( \alpha (x-x_{i-1}) + \beta (x-x_i) \right) &\\
s_i'(x_{i-1}) &= \delta y[x_{i-1}, x_i] + \beta (x_{i-1} - x_i)^2 = \tau_{i-1} &\\
s_i'(x_i) &= \delta y[x_{i-1}, x_i] + \alpha (x_i - x_{i-1})^2 = \tau_i &\\
\Rightarrow \alpha &= \frac{\tau_i - \delta y[x_{i-1}, x_i]}{(x_i - x_{i-1})^2} &\\
\beta &= \frac{\tau_{i-1} - \delta y[x_{i-1}, x_i]}{(x_{i-1}- x_i)^2} &\\
h_i &= x_i-x_{i-1} &\\
\Rightarrow s_i(x) &= y_{i-1} + (x-x_{i-1}) \delta y [x_{i-1}, x_i] &\\ 
&+ \frac{(x-x_{i-1} )(x-x_i)}{h_i^2} \left( (\tau_i - \delta y[x_{i-1},x_i])(x-x_{i-1})+(\tau_{i-1} - \delta y[x_{i-1},x_i])(x-x_i) \right) &\\
\end{align*}
Für beliebige $\tau_0,..., \tau_n$ erhalten wir $s: [a,b] \rightarrow \mathbb{R}$ mit 
\begin{enumerate}
  \item[i)] $s\,\rule[-2mm]{0.1mm}{5mm}_{\thinspace [x_{i-1}, x_i]} \in \mathcal{P}_3$
  \item[ii)] $s(x_i) = y_i$
  \item[iii)] $s \in \mathcal{C}^1([a,b])$
\end{enumerate} 
Bestimme $\tau_0,..., \tau_n$ so, dass $s \in \mathcal{C}^2([a,b])$, d.h. $s_i''(x_i) = s_{i+1}''(x_i)$ für $i=1,...,n-1$. Das sind $(n-1)$ Bedingungen. (\#) \\
Beim eingespannten Spline sind $\tau_0$ und $\tau_n$ bekannt und die $\tau_1, ..., \tau_{n-1}$ sind die Unbekannten. \\
Mit
\begin{align*}
&(fg)'' = f''g + 2f'g' + fg''
\intertext{gilt wegen}
&\frac{d^2}{dx^2} \left( (x-x_{i-1})^2 (x-x_i) \right) \rule[-2mm]{0.1mm}{5mm}_{\thinspace x = x_i} = 4h_i
\intertext{und}
&\frac{d^2}{dx^2} \left( (x-x_{i-1}) (x-x_i)^2 \right) \rule[-2mm]{0.1mm}{5mm}_{\thinspace x = x_i} = 2h_i
\intertext{folgendes:}
s_i''(x_i) &= \frac{1}{h_i^2} \left( (\tau_i - \delta y[x_{i-1},x_i]) 4h_i + (\tau_{i-1} - \delta y[x_{i-1},x_i]) 2h_i \right) &\\
&= \frac{2}{h_i} \left( 2 \tau_i - 3 \delta y[x_{i-1},x_i] + \tau_{i-1} \right)
\intertext{Ebenso zeigt man:}
s_{i+1}''(x_i) &= -\frac{2}{h_{i+1}} \left( 2 \tau_i - 3 \delta y[x_{i},x_{i+1}] + \tau_{i+1} \right)
\end{align*}
Die Bedingung (\#) $s_i''(x_i) = s_{i+1}''(x_i) \quad i=1,...,n-1$ wird damit zu
\begin{align*}
\frac{\tau_{i-1}}{h_i} + 2 \left( \frac{1}{h_i} + \frac{1}{h_{i+1}} \right) \tau_i + \frac{\tau_{i+1}}{h_{i+1}} &= 3 \left( \frac{\delta y[x_{i-1}, x_i]}{h_i} + \frac{\delta y[x_{i}, x_{i+1}]}{h_{i+1}}\right)&\\
\end{align*}
Damit erhalten wir ein LGS für $\tau_1, ..., \tau_{n-1}$ 
\begin{align*}
&\underbrace{
\begin{bmatrix} 
(\frac{2}{h_1} + \frac{2}{h_2}) & \frac{1}{h_2} &0&\dots&0\\ 
\frac{1}{h_2} & (\frac{2}{h_2} + \frac{2}{h_3}) & \frac{1}{h_3} &\ddots&\vdots\\ 
0& & \ddots &&0\\
\vdots&\ddots&&& \frac{1}{h_{n-1}} \\
0&\dots&0& \frac{1}{h_{n-1}} & ( \frac{2}{h_{n-1}} +  \frac{2}{h_{n}})\\
\end{bmatrix}
}_A
\underbrace{
\begin{bmatrix}
\tau_1 \\
\tau_2 \\
\vdots \\
\tau_{n-1} \\
\end{bmatrix}
}_{\tau} 
= 
\underbrace{
\begin{bmatrix}
3 \left( \frac{\delta y[x_0,x_1]}{h_1} + \frac{\delta y[x_1,x_2]}{h_2} \right) - \frac{\tau_0}{h_1} \\
3 \left( \frac{\delta y[x_1,x_2]}{h_2} + \frac{\delta y[x_2,x_3]}{h_3} \right)\\
\vdots \\
3 \left( \frac{\delta y[x_{n-2},x_{n-1}]}{h_{n-1}} + \frac{\delta y[x_{n-1},x_n]}{h_n} \right) - \frac{\tau_n}{h_n} \\
\end{bmatrix}
}_b
\end{align*}
\end{nothing}

\begin{theorem}
Sei $A$ wie in (12.3) und $A\tau = b$, dann gilt
$$\max_i \vert \tau_i \vert \leq \frac{h}{2} \max_i \vert b_i \vert,$$
wobei $\tau = (\tau_1, ..., \tau_{n-1})^T$, $b = (b, ..., b{n-1})^T$, $h = \max_i h_i$.

\begin{proof}[Beweis]
Sei $j \in \{1, ..., n-1\}$ so, dass $\vert \tau_j \vert = \max_i \vert \tau_i \vert$. Dann gilt:
\begin{align*}
2 \left( \frac{1}{h_j} + \frac{1}{h_{j+1}} \right) \tau_j &= -\frac{\tau_{j-1}}{h_j} -\frac{\tau_{j+1}}{h_{j+1}} + b_j &\\
\Rightarrow 2 \left\vert \frac{1}{h_j} + \frac{1}{h_{j+1}} \right\vert & \leq \left\vert \frac{\tau_{j-1}}{h_j} \right\vert + \left\vert \frac{\tau_{j+1}}{h_{j+1}} \right\vert + \vert b_j \vert &\\
& \leq \left( \frac{1}{h_j} + \frac{1}{h_{j+1}} \right) \vert \tau_j \vert + \max_i \vert b_i \vert &\\
\Rightarrow \left( \frac{1}{h_j} + \frac{1}{h_{j+1}} \right) \vert \tau_j \vert &\leq \max_i \vert b_i \vert &\\
\Rightarrow \max_i \vert \tau_i \vert &= \vert \tau_j \vert \leq \frac{h}{2} \max_i \vert b_i \vert 
\end{align*}
\end{proof}
\end{theorem}

\begin{korollar}
Die Matrix A aus (12.4) ist invertierbar.
\begin{proof}[Beweis]
Die einzige Lösung von $A\tau = 0$ ist $\tau = 0$, $0 \in \mathbb{R}^{n-1}$
\end{proof}
\end{korollar}

\begin{korollar}
Der eingespannte Spline existiert und ist eindeutig.
\begin{proof}[Beweis]
Folgt aus (12.5)
\end{proof}
\end{korollar}

\subsection{Fehler bei der Splineinterpolation}
Vorraussetzungen für diesen Abschnitt: \\
$a = x_0 < x_1 < ... < x_n = b$, \\
$h_i= x_i-x_{i-1}$, \\
$h:= \max_i \vert h_i \vert$

\begin{theorem}
Sei $f \in \mathcal{C}^4([a,b])$, $s$ der eingespannte Spline, d.h. $s'(a) = f'(a)$, $s'(b) = f'(b)$, $s(x_i) = f(x_i)$ für $i=0,...,n$. Dann gilt für $x\in [a,b]$
$$ \vert f(x) - s(x) \vert \leq \frac{5}{384} h^4 \max_{\xi \in[a,b]} \vert f^{(4)}(\xi) \vert $$
\begin{proof}[Beweis] Siehe (13.3)
\end{proof}
\end{theorem}

\begin{lemma}
Unter den Vorraussetzungen von (13.1) gilt für  $s'(x_i) = \tau_i$:
$$ \vert f'(x_i) - \tau_i \vert \leq \frac{h^3}{24} \max_{\xi \in [a,b]} \vert f^{(4)}(\xi) \vert$$
\begin{proof}[Beweis (für den äquidistanten Fall)]
Für $i=1, ..., n-1$ erfüllen die $\tau_i$ 
\begin{align*}
&\frac{1}{h} (\tau_{i-1} + 4 \tau_i + \tau_{i+1}) = \frac{3}{h^2} (f(x_{i+1}) - f(x_{i-1})) = b_i &\\
\intertext{Ersetze nun $\tau_i$ durch $f'(x_i)$, so gilt:}
&\frac{1}{h} (f'(x_{i-1}) + 4f'(x_i) + f'(x_{i+1})) - \frac{3}{h^2} ( f(x_{i+1}) - f(x_{i-1})) =: \delta_j &\\
\intertext{Taylorentwicklung von $f'(x_{i-1}), f'(x_{i+1}), f(x_{i-1}), f(x_{i+1})$ um $x_i$}
&f(x_{i+1}) = f(x_i + h) = f(x_i) + hf'(x_i) + \frac{h^2}{2!} f''(x_i) &\\
& + \frac{h^3}{3!} f'''(x_i) + h^4 \int_0^1 \frac{(1-t)^3}{3!} f^{(4)}(x_i+th) dt &\\
&f'(x_{i+1}) =  f'(x_i) + hf''(x_i) + \frac{h^2}{2!} f''(x_i) &\\
&+ h^3 \int_0^1 \frac{(1-t)^2}{2!} f^{(4)}(x_i+th) dt &\\
\intertext{und analog für $f(x_{i-1}) = f(x_i-h)$ und $f'(x_{i-1})$. $\Rightarrow$}
\delta_j &= \frac{1}{h} (f'(x_i) - hf''(x_i) + \frac{h^2}{2} f''(x_i) + R_{i-}' &\\
&\quad + 4 f'(x_i) + f'(x_i) + hf''(x_i) + \frac{h^2}{2} f'''(x_i) + R_{i+}') &\\
&\quad -\frac{3}{h^2} (f(x_i) + hf'(x_i) + \frac{h^2}{2}f''(x_i) + \frac{h^3}{3!} f'''(x_i) + R_{i+} &\\
&\quad- f(x_i) + hf'(x_i) - \frac{h^2}{2} f''(x_i) + \frac{h^3}{3!} f'''(x_i) + R_{i-}) &\\
&= h^2 \int_0^1 \left( \frac{(1-t)^2}{2!} - 3 \frac{(1-t)^3}{3!} \right) f^{(4)}(x+th) dt &\\
&\quad+ h^2 \int_0^1 \left( \frac{(1-t)^2}{2!} - 3 \frac{(1-t)^3}{3!} \right) f^{(4)}(x-th) dt &\\
&= h^2 \left( f^{(4)}(\xi_i) + f^{(4)}(\eta_i) \right) \int_0^1 \frac{(1-t)^2}{2} - \frac{(1-t)^3}{2} dt &\\
&=\frac{h^2}{24} \left(f^{(4)}(\xi_i) + f^{(4)}(\eta_i)\right), \quad \xi_i \in [x_{i-1},x_i], \medspace \eta_i \in [x_i, x_{i+1}] &\\
\Rightarrow \vert \delta_i \vert &\leq \frac{h^2}{12} \max_{\xi \in [a,b]} \vert f^{(4)}(\xi) \vert
\intertext{Definiere nun $e_i := f'(x_i) - \tau_i$ für $i=0,...,n$. Diese erfüllen die Bedingung $e_0 = 0, \medspace e_n = 0$ vom eingespannten Spline. Für $f' = (f'(x_1),..., f'(x_{n-1}))^T$, $\delta = (\delta_1, ..., \delta_{n-1})^T$ und $e = (e_1, ..., e_{n-1})$ gilt: $A \tau = b$ und $Af' = b+ \delta$. Mit (12.4) gilt dann}
\max_i \vert e_i \vert & \leq \frac{h}{2} \max_i \vert \delta_i \vert \leq \frac{h^3}{24} \max_{\xi \in [a,b]} \vert f^{(4)}(\xi) \vert 
\end{align*}
\end{proof}
\end{lemma}

\begin{nothing}[Beweis von (13.1)]
Für $x \in [x_{i-1}, x_i]$ ist $f(x) - s_i(x) = f(x) - p_i(x) + p_i(x) - s_i(x)$, \\
wobei $p_i$ das kubische Hermiteinterpolationspolynom zu $f$ ist mit \\
$p_i(x_i) = f(x_i)$, $p_i(x_{i-1}) = f(x_{i-1})$, $p_i'(x_i) = f'(x_i)$, $p_i'(x_{i-1}) = f'(x_{i-1})$\\
Nach Satz (11.2) gilt für ein $\xi \in [x_{i-1}, x_i]$:
\begin{align*}
\vert f(x) - p_i(x) \vert &= \vert (x-x_i)^2(x-x_{i-1})^2 \vert \left\vert \frac{f^{(4)}(\xi)}{24} \right\vert &\\
& \leq \frac{h^4}{16*24} \vert f^{(4)}(\xi) \vert = \frac{h^4}{384} \vert f^{(4)}(\xi) \vert
\end{align*}
Weiter gilt:
\begin{align*}
s_i(x) - p_i(x) &= (x-x_{i-1})(x-x_i) ((\tau_i - f'(x_i)) (x-x_{i-1}) &\\
& \quad +  (\tau_{i-1} - f'(x_{i-1})) (x-x_{i}) ) \frac{1}{h^2}
\intertext{Da $\frac{(x-x_{i-1})(x-x_i)}{h^2} \leq \frac{1}{4}$ für $x \in [x_{i-1}, x_i]$ gilt mit (13.2)}
\vert s_i(x) - p_i(x) \vert  &\leq \frac{1}{4} \frac{h^3}{24} \max_{\xi \in [a,b]} \left\vert f^{(4)}(\xi) \right\vert \underbrace{\left( \vert x-x_{i-1} \vert + \vert x-x_i \vert \right)}_{= h} &\\
& = \frac{h^4}{96} \max_{\xi \in [a,b]} \left\vert f^{(4)}(\xi) \right\vert &\\
\intertext{Ingesamt gilt also:}
\vert f(x) - s_i(x) \vert & \leq h^4 \frac{1+4}{384} \max_{\xi \in [a,b]} \left\vert f^{(4)}(\xi) \right\vert
\end{align*}
\qed
\end{nothing}

\begin{comment*}
Wie wirken sich Störungen/Fehler in den Daten auf den interpolierenden Spline aus? \\
Gegeben seien $(x_i, y_i)_{i=0}^n$ und $(y_0', y_n')$. Dadurch erhält man einen Spline $s(x)$. \\
Für Daten $(x_i, \tilde{y}_i)_{i=0}^n$ und $(y_0', y_n')$ erhält man einen Spline $\tilde{s}(x)$. Der Einfachheit halber sind $y_0'$ und $y_n'$ fehlerfrei. \\
Nun gilt:
$$s(x) - \tilde{s}(x) = \sum_{i=0}^n (y_i - \tilde{y}_i) l_i(x),$$
wobei $l_i(x)$ ein kubischer Spline mit 
$$l_i(x_j) =
\begin{cases}
  1 & \text{, falls } i=j \\
  0 &\text{, sonst} \\
\end{cases}  $$
und $l_i'(a) = 0 = l_i'(b)$ ist ("Lagrange-Spline"). \\
Diese zeigen keine Oszillationen wie Lagrangepolynome auf äquidistanten Stützstellen. Es gilt 
$$\max_{x \in [a,b]} \vert s(x) - \tilde{s}(x) \vert \leq \Lambda_n \max_i \vert y_i - \tilde{y}_i \vert $$
mit der Spline Lebesguekonstante 
$$\Lambda_n = \max_{x \in [a,b]} \sum_{i=0}^n \vert l_i(x) \vert $$
\underline{Ohne Beweis:} Für äquidistante Verteilungen gilt für Splines $\forall n \in \mathbb{N}: \medspace \Lambda_n \leq 2$
\end{comment*}

\subsection{Numerische Differentiation}
\begin{description}
  \item[Problemstellung:] Zu $f: [a,b] \rightarrow \mathbb{R}$ berechne näherungsweise $f'(x)$ für $x \in [a,b]$:
  $$f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}$$
  Falls $f \in \mathcal{C}^2([a,b])$ gilt:
  \begin{align*}
  f(x+h) &= f(x) + hf'(x) + \frac{h^2}{2} f''(\xi), \quad \text{für } \xi \in [a,b] &\\
  \Rightarrow \frac{f(x+h) - f(x)}{h} &= f'(x) + \frac{h}{2} f''(\xi)
  \end{align*}
  Allerdings ist ein Grenzübergang $h \rightarrow 0$ auf einem Computer problematisch, da statt $\frac{f(x+h) - f(x)}{h}$ nur $\frac{f(x+h) - f(x) + \varepsilon}{h}$ berechnet werden kann für ein $\varepsilon < \text{eps}$ (Maschinengenauigkeit) \\
  $\text{eps} \approx 10^{-16}$
  \item[Idee:] Um $f'$ zu approximieren, ersetze $f$ durch ein Polynom $p$ oder ein Spline $s$ und approximiere $f'$ durch $s'$ oder $p'$.
  \item[Berechnung von $p'(x)$:]  Dividierte Differenzen:\\
  \begin{tabular}{lllllll}
    $x$ & $p(x) = b_0$ \\
    &&$b_1$\\
	$x_0$ & $y_0 = f(x_0)$ && $b_2$\\
	 & & $\delta^1y[x_0, x_1]$&& $b_3$\\
	$x_1$ & $y_1 = f(x_1)$ & &$\delta^2y[x_0, x_1, x_2]$  &&$\ddots$\\
	 & & $\delta^1y[x_1, x_2]$ & & $\delta^3y[x_0, x_1, x_2, x_3]$&& $b_n = \delta^ny[x_0,...,x_n]$\\
	$x_2$ & $y_2 = f(x_2)$ & & $\delta^2y[x_1, x_2, x_3]$\\
	 & & $\delta^1y[x_2, x_3]$\\
	$x_3$ & $y_3 = f(x_3)$ \\
	$\vdots$ & $\vdots$\\
	$x_n$ & $y_n = f(x_n)$
  \end{tabular}\\\\
  Interpolationspolynom $p \in \mathcal{P}_n$:
  \begin{align*}
    p(x) &= \sum_{i=0}^n \prod_{j=0}^{i-1} (x-x_i) \delta^iy[x_0,..., x_i] &\\
    &= x^n \delta^ny[x_0,..., x_n] + r, \quad \text{für }r \in \mathcal{P}_{n-1} &\\
    p^{(n)} &= n! \delta^ny[x_0,..., x_n]
  \end{align*}
  Füge weitere Diagonale zu Knoten $x$ in obigem Schema hinzu mit $b_0 = p(x)$ und $b_k = \delta^k y[x, x_0, x_1, ..., x_{k-1}]$. Nach Definition ist 
  $$b_{k+1} = \frac{b_k - \delta^ky[x_0,..., x_k]}{x-x_k}$$
  Rechne nun im Newtonschema von rechts nach links (da $b_n = \delta^ny[x_0,..., x_n]$).\\
  $b_n = \delta^ny[x_0,...,x_n]$ für $k= n-1, ..., 0$.\\
  $b_k = b_{k+1} (x-x_k) + \delta^ky[x_0,...,x_k]$ \\
  $p(x) = b_0$\\
  Nach dem Hornerschema.\\
  Berechne nun die Ableitungen: \\
  Füge weitere Diagonale zu Knoten $x+ \varepsilon$ hinzu und lasse $\varepsilon \rightarrow 0$ laufen \\
  \begin{tabular}{lllllll}
    $x+ \varepsilon$ & $p(x + \varepsilon) = c_0$ \\
    && $c_1 = p'(x)$ \\
    $x$ & $p(x) = b_0$ \\
    &&$b_1$\\
	$x_0$ & $y_0 = f(x_0)$ && $b_2$&&$\ddots$\\
	 & & $\delta^1y[x_0, x_1]$&& $b_3$&&$c_n$\\
	$x_1$ & $y_1 = f(x_1)$ & &$\delta^2y[x_0, x_1, x_2]$  &&$\ddots$ & =\\
	 & & $\delta^1y[x_1, x_2]$ & & $\delta^3y[x_0, x_1, x_2, x_3]$&& $b_n$\\
	$x_2$ & $y_2 = f(x_2)$ & & $\delta^2y[x_1, x_2, x_3]$&&& =\\
	 & & $\delta^1y[x_2, x_3]$&&&& $\delta^ny[x_0,...,x_n]$\\
	$x_3$ & $y_3 = f(x_3)$ \\
	$\vdots$ & $\vdots$\\
	$x_n$ & $y_n = f(x_n)$
  \end{tabular}\\\\
  \underline{Algorithmus zur Berechnung von $p'(x)$:}
  \begin{algorithmic}
  \STATE $c_n = b_n$
  \FOR{$k = n-1, ...,1$}
  	\STATE $c_k = b_k + (x-x_{k-1})c_{k+1}$
  \ENDFOR
  \STATE $p'(x) = c_1$
  \end{algorithmic}
\end{description}

\begin{theorem}
Sei $f \in \mathcal{C}^{n+2}([a,b])$, $p$ Interpolationspolynom zu $f$ in $x_0,..., x_n \in [a,b]$ paarweise verschieden ($p\in \mathcal{P}_n$). \\
$\forall x \in [a,b] \medspace \exists \xi, \xi' \in [a,b]:$ 
$$f'(x) - p'(x) = \left( \sum_{i=0}^n \prod{j=0, \medspace j \neq i}^n (x-x_j) \frac{f^{(n+1)(\xi)}}{(n+1)!}\right) + \prod_{j=0}^n (x-x_j) \frac{f^{(n+2)}(\xi')}{(n+2)!}$$
\begin{proof}[Beweisskizze] (vgl. 9.1) \\
Sei \={x} fest aber beliebig, $\bar{p}$ das Hermiteinterpolationspolynom zu 
\begin{description}
  \item $\bar{p}(x_i) = f(x_i), \quad i=0,...,n$
  \item $\bar{p}(x) = f(x),$
  \item $\bar{p}'(x) = f'(x)$
\end{description}
Newtonschema und Newtoninterpolationspolynome liefert das Ergebnis.
\end{proof}
\end{theorem}

\section{Lineare Gleichungssysteme und lineare Ausgleichsrechnung}
%
\begin{description}
  \item[Ziele:]
    \begin{itemize}
      \item Berechne $x \in \mathbb{R}^n$, welches Lösung von $Ax = b$ ist, wobei $A \in \mathbb{R}^{n\times n}$ invertierbar und $b \in \mathbb{R}^n$.
      \item Berechne $x \in \mathbb{R}^m$, welches Lösung von $\min_{x\in \mathbb{R}^m} \Vert Ax - b \Vert _2$ ist, wobei $A \in \mathbb{R}^{n\times m}$, $b \in \mathbb{R}^n$ und $n>m$.
    \end{itemize}
\end{description}

\subsection{Gaußelimination}
%
\begin{example}\leavevmode
\begin{enumerate}
  \item[a)] Splineinterpolation $A\tau = b$, $A$ tridiagonal und symmetrisch.
  \item[b)] Computertomographie \\
%    \includegraphics[width=11cm]{Bild1.png}\\
    $\Delta I$: gemessener Intensitätsunterschied zwischen Quelle und Detektor \\
    $\Delta I = \int_{[a,b]} \alpha(x) dx$\\
    Dabei ist $\alpha(x)$ der Absorptionskoeffizient\\
    Annahme: $\alpha$ ist konstant in jeder Volumenzelle (Voxel) $\Rightarrow$\\
    $\Delta I = \sum_{j \in \text{Voxel}} \alpha_j l_j$\\
    $l_j$: Länge des Weges $[a,b]$ in Voxel $j$\\
    Viele Strahlen: $L_j(t) = \omega ( \varphi_j ) s_j + \omega^{\perp} ( \varphi_j ) t$\\
%    \includegraphics[width=8cm]{Bild2.png}\\
    $\omega(\varphi_j) = (\cos(\varphi_j), \sin(\varphi_j))$\\
    $\omega^{\perp}(\varphi_j) = (-\sin(\varphi_j), \cos(\varphi_j))$ \\\\
    $\begin{bmatrix} 
    l_{11} & l_{12} & \dots & l_{1M} \\
    \vdots & \vdots & & \vdots \\
    l_{N1} & l_{N2} & \dots & l_{NM} \\
    \end{bmatrix}
    \begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_M \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    \Delta I_1 \\
    \vdots \\
    \Delta I_N \\
    \end{bmatrix}$\\\\
    $M$ ist dabei die Anzahl der Voxel.\\
    $l_{ij}$: Länge des i-ten Strahl im j-ten Voxel\\
    $\alpha_j$: Absorption im j-ten Voxel \\
    $\Delta I_i$: Intensitätsunterschied entlang vom Strahl i
\end{enumerate}
\end{example}

\begin{nothing}[Herleitung des Verfahrens (Wdh. LA)]
$Ax = b$, $A = (a_{ij})_{i,j = 1}^n$, $b = (b_i)_{i=1}^n$\\\\
%
$a_{11} x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1$\\
$a_{21} x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2$\\
$\vdots$ \\
$a_{n1} x_1 + a_{n2}x_2 + \dots + a_{nn}x_n = b_n$\\\\
%
Ohne Einschränkungen sei $a_{11} \neq 0 $. Da $A$ invertierbar ist, ist mindestens ein Element aus $\{a_{i1}, i=1,...,n\}$ ungleich 0. Man kann also Zeilen/Gleichungen so vertauschen, dass $a_{11} \neq 0$. Für $i=2,3,...,n$ multipliziere die 1-te Zeile mit $l_{i1} := \frac{a_{i1}}{a_{11}}$ und ersetze die i-te Zeile durch $\text{(i-te Zeile)} - l_{i1} * \text{(1-ste Zeile)}$. Dann ergibt sich folgendes Gleichungssystem: \\\\
%
$a_{11}^{(1)} x_1 + a_{12}^{(1)}x_2 + \dots + a_{1n}^{(1)}x_n = b_1^{(1)}$\\
$0 + a_{22}^{(1)}x_2 + \dots + a_{2n}^{(1)}x_n = b_2^{(1)}$\\
$\vdots$ \\
$0+ a_{n2}^{(1)}x_2 + \dots + a_{nn}^{(1)}x_n = b_n^{(1)}$\\\\
%
Dabei ist\\
$a_{1j}^{(1)} = a_{1j}$ für $j=1,...,n$, \\
$b_1^{(1)} = b_1$, \\
$(a_{i1}^{(1)} = 0)$\\
$a_{ij}^{(1)} = a_{ij} - l_{i1} a_{1j}$,\\
$b_i^{(1)} = b_i - l_{i1} b_1$ für $i=2,...,n$, $j=1,...,n$.\\\\
%
Da die $(n-1)\times (n-1)$ Untermatrix $A^{(1)} (2:n, 2:n)$ ebenfalls invertierbar ist, wiederholt man den eben beschriebenen Schritt.\\
Nach eventuellem Zeilentausch ist $a_{22}^{(1)} \neq 0$
%
\begin{align*}
l_{i2} &:= \frac{a_{i2}^{(1)}}{a_{22}^{(1)}}, \quad i=3,...,n &\\
b_2^{(2)} &= b_2^{(1)}, \quad a_{2j}^{(2)} = a_{2j}^{(1)}, \quad j=2,...,n &\\
b_i^{(2)} &= b_i^{(1)} - l_{i2} b_2^{(1)}, \quad i=3,...,n &\\
a_{ij}^{(2)} &= a_{ij}^{(1)} - l_{i2} a_{2j}^{(1)}, \quad i=3,...,n, \medspace j=2,...,n &\\
\end{align*}
%
Damit entsteht eine Folge\\ 
$(A, b), (A^{(1)}, b^{(1)}), (A^{(2)}, b^{(2)}),..., (A^{(n-1)}, b^{(n-1)}) =: (R,c)$\\
für eine obere Dreiecksmatrix $R$ (d.h. alle Einträge unter der Diagonalen sind 0).\\
Das Gleichungssystem mit ($r_{ii} \neq 0$)\\
%
\begin{align*}
r_{11}x_1 + r_{12}x_2 + \dots + r_{1n}x_n &= c_1 &\\
r_{22}x_2 + \dots + r_{2n}x_n &= c_2 &\\
\vdots& &\\
r_{nn}x_n &= c_{n} &\\
\end{align*}
%
Dabei ist\\
$x_n = \frac{c_n}{x_{nn}}$\\
$x_i = \frac{1}{r_{ii}}(c_i - \sum_{j=i+1}^n r_{ij}x_j)$ für $i=n-1,...,1$
\end{nothing}

\begin{theorem}
Für eine invertierbare Matrix $A \in \mathbb{R}^{n\times n}$ liefert das in (15.2) beschriebene Verfahren 
$$PA = LR,$$
wobei 
$$L = 
\begin{bmatrix} 
1 &&& 0 \\
l_{21} & \ddots \\
\vdots &\ddots& \ddots \\
l_{n1} &\dots& l_{n(n-1)}& 1 \\
\end{bmatrix}$$
$$R = 
\begin{bmatrix} 
r_{11} & r_{12} & \dots & r_{1n} \\
& \ddots &  & \vdots \\
\\
0&&& r_{nn} \\
\end{bmatrix}$$
und $P$ eine Permutationsmatrix ist.
\begin{proof}[Beweis]
Nehme an, dass die notwendige Zeilenvertauschungen bereits durchgeführt wurden, d.h. ersetze $A$ durch $PA$ (Zeilen und Spalten von $P$ bestehen aus kanonischen Einheitsvektoren z.B. $P = \left(\begin{smallmatrix} 0&1&0&\\1&0&0&\\0&0&1&\\ \end{smallmatrix}\right)$\Big). \\
Bezeichne mit $L_i \in \mathbb{R}^{n\times n}$:
$$L_i = 
\begin{bmatrix}
1 &&&&&0\\
& \ddots \\
&& 1 \\
&& -l_{i+1,i} &  \\
&& \vdots &&\ddots\\
&& \underbrace{-l_{n,i}}_{\text{i-te Spalte}} &&&1 \\
\end{bmatrix}$$
Damit ist
\begin{align*}
A^{(1)} &= L_1 A, \quad a_{ij}^{(1)} = a_{ij} - l_{i1} a_{1j} &\\
A^{(k)} &= L_k A^{(k-1)}, \quad k = 2,...,n-1 &\\
R &= A^{(n-1)} = L_{n-1}L_{n-2} ... L_1 A &\\
\Rightarrow A &= \underbrace{L_1^{-1}...L_{n-2}^{-1}L_{n-1}^{-1}}_{=L} R
\end{align*}
Setzt man 
$$V_i = 
\begin{bmatrix}
0 \\
& \ddots \\
&& 0 \\
&& l_{i+1,i} &  \\
&& \vdots &&\ddots&\\
&& \underbrace{l_{n,i}}_{\text{i-te Spalte}} &&&0 \\
\end{bmatrix}
\in \mathbb{R}^{n\times n}$$
so ist $L_i = I_n - V_i$, da $V_iV_k = 0$ für $i \leq k$.\\\\
$\underbrace{(I_n - V_i)}_{=L_i} (I_n + V_i) = I_n + V_i - V_i + \underbrace{V_iV_i}_{=0} = I_n$\\\\
d.h. $L_i^{-1} = I_n + V_i$.\\
Damit folgt $L = L_1^{-1}L_2^{-1} ... L_{n-1}^{-1} = (I_n + V_1)(I_n + V_2) ... (I_n + V_{n-1}) = I_n + V_1 + V_2 +... + V_{n-1} + \underbrace{V_1V_2 + ...}_{=0} = L$\\
Das schließende $L$ ist dabei das $L$ aus (15.2).
\end{proof}
\end{theorem}

\begin{comment*}
\begin{align*}
\det(PA) &= \det(P)\det(A) = (-1)^{\text{\# Vertauschungen}} \det(A) &\\
\det(PA) &= \det(LR) = \underbrace{\det(L)}_{=1} \det(R) = \prod_{i=1}^n r_{ii}
\end{align*}
\end{comment*}

\begin{nothing}[Vorwärts- und Rückwärts-Substitution]
Sobald man die LR-Zerlegung (lu-decomposition) von $A$ kennt, löst man $Ax = b$ wie folgt:
$$Pb = PAx = L\underbrace{Rx}_{=: c}$$
Löse $Lc = Pb$ ("Vorwärtssubstitution"):
%
\begin{algorithmic}
\STATE $c_1 = (Pb)_1$
\FOR{$i=2,...,n$}
\STATE $c_i = (Pb)_i - \sum_{j=1}^{i-1} l_{ij}c_j$
\ENDFOR
\end{algorithmic}
%
und anschließend:\\
Löse $Rx = c$ wie in (15.2) angegeben ("Rückwärtssubstitution").
\end{nothing}

\begin{nothing}[Aufwand]
Beim Schritt $A \rightarrow A^{(1)}$ benötigt man 
\begin{itemize}
  \item $(n-1) \in \mathcal{O}(n)$ Divisionen 
  \item $(n-1)^2 \in \mathcal{O}(n^2)$ Multiplikationen
  \item $(n-1)^2 \in \mathcal{O}(n^2)$ Additionen
\end{itemize}
Also insgesamt Operationen aus $\mathcal{O}(n^2)$.\\
$A^{(1)} \rightarrow A^{(2)}$: $(n-1)^2$ Operationen. \\
$A^{(2)} \rightarrow A^{(3)}$: $(n-2)^2$ Operationen. \\
$\vdots$\\
$A \rightarrow L,R$: $\sum_{j=1}^n j^2 \approx \frac{1}{3}n^3 \medspace \left(\in \mathcal{O}(n^3)\right)$, da $\underbrace{\frac{1}{n} \sum_{j=0}^n \left(\frac{j}{n}\right)^2}_{ \approx \int_0^1 x^2 = \frac{1}{3}} n^3$\\
Die Lösung von $Lc = Pb$ kostet ebenso wie die Lösung von $Rx = c$ \\
$1+2+...+(n-1) \approx \frac{1}{2} n^2$ Operationen.\\
Der Hauptaufwand steckt also in der Berechnung der Zerlegung $PA = LR$. 
\end{nothing}

\begin{comment*}[Einschub zur Gleitkommarechnung (floating point arithmetic)]
Jeder reelle Zahl $0 \neq x$ kann für festes $B \in \mathbb{N}$, $B \geq 2 $ eindeutig durch
$$x = \pm \thinspace m \thinspace B^e$$
dargestellt werden, wobei $m \in [1, B)$, die Mantisse, $e \in \mathbb{Z}$, der Exponent und $B$ die Basis ist. \\
Durch den Computer kommen folgende Einschränkungen hinzu:
\begin{itemize}
  \item Es stehen nur $l$ Ziffern für die Mantisse $m$ zur Verfügung $\rightarrow$ $m$ wird gerundet.
  \item Es stehen nur $r$ Ziffern für den Exponenten zur Verfügung
\end{itemize}
%Beginne mit den Sätzen, Definitionen,...
\begin{description}
  \item[Definition] $\thinspace$\\
    Eine $l$-stellige-Basis-$B$-Gleitkommazahl mit Exponentialbereich $[e_{\min}, e_{\max}]$ ist ein Tripel $(\sigma, m, e)$. Dabei ist 
    \begin{itemize}
      \item $\sigma$ das Vorzeichen
      \item $m$ eine $l$-stellige Zahl zur Basis $B$ mit festgelegter Kommastelle
      \item $e$ die ganze Zahl in $[e_{\min}, e_{\max}]$
    \end{itemize}
    Der Wert von $(\sigma, m, e)$ ist $\sigma * m * B^e$.
  %
  \item[Beispiel] $\thinspace$\\
    Betrachte den Standard IEEE 754.\\
    Dieser stellt eine Zahl mit einfacher Genauigkeit dar zur Basis $B=2$ mit $l=32$ und $[e_{\min}, e_{\max}] = [-128, 127]$.
    \begin{figure}[!htb]
      \centering
%      \includegraphics[width=10cm]{Bild3.png}
      \caption{From https://de.wikipedia.org/wiki/IEEE\_754}
    \end{figure}\\
    Der Wert lässt sich berechnen aus
    $$(-1)^{\sigma} * (1,m) * 2^{\sum_{j} e_j^{2^j} - 127}$$
    Dabei ist $m$ binär dargestellt.
  %
  \item[Definition] $\thinspace$ \\
    Für eine reelle Zahl $x$ bezeichnen wir mit $fl(x)$ eine $l$-stellige-Basis-10-Darstellung von $x$ mit unbeschränktem Exponenten $e$, sodass 
    $$fl(x) = \pm \thinspace m \thinspace 10^e,$$
    wobei $m$ eine Zahl mit $l$ Stellen ist.\\
    Die \underline{Maschinengenauigkeit} $\text{eps}$ ist die kleinste positive Zahl, sodass $fl(1+ \text{eps}) > 1$. Also ist $\text{eps}$ der Abstand zwischen zwei benachbarten Mantissen.
  %
  \item[Beispiele] $\thinspace$ 
    \begin{description}
      \item Dezimalsystem ($B=10$) $\Rightarrow \quad \text{eps} = 5 * 10^{-e}$
      \item Binärsystem ($B=2$) $\quad \medspace \medspace  \thinspace \Rightarrow \quad \text{eps} = 2^{-e}$
    \end{description}
\end{description}
\end{comment*}

\subsection{Wahl des Pivotelements}

\begin{example}
\begin{align*}
10^{-4}x_1 + x_2 &= 1 &\\
x_1 + x_2 &= 2 &\\
\end{align*}
exakte Lösung:
\begin{align*}
x_1 = \frac{1}{0.9999} &= 1.0001\overline{0001} &\\
x_2 = \frac{0.9998}{0.9999} &= 0.9998\overline{9998} &\\
\end{align*}
bei dreistelliger dezimaler Gleitkommaartihmetik (Mantissenlänge 3, Basis 10)
\begin{align*}
0.100 * 10^{-3} x_1 + 0.100 * 10^1 x_2 &= 0.199 * 10^1 &\\
0.100 * 10^1 x_1 + 0.100 * 10^1 x_2 &= 0.200 * 10^1
\end{align*}
und damit erhält man für 
\begin{enumerate}
  \item [a)] $a_{11} = 10^{-4}$ (Pivot) \\
    \begin{align*}
    l_{21} &= \frac{a_{21}}{a_{11}} = 10^4 = 0.100 * 10^5 &\\
    a_{22}^{(1)} &= 0.100 * 10^1 - 0.100 * 10^5 = -0.100 * 10^5 &\\
    b_2^{(1)} &= 0.200 * 101 - 0.100 * 10^5 = -0.100 *10^5 &\\
    \end{align*}
    Aus $-0.100 * 10^5 x_2 = -0.100 *10^5$ folgt $x_2 = 0.100 * 10^1 = 1$ \\
    $\Rightarrow$ $x_1 = \frac{b_1 - a_{12}x_2}{a_{11}} = \frac{0.100 * 10^1 - 0.100 * 10^1 x_2}{0.100 * 10^1} = 0$
  \item [b)] Wähle Pivot $a_{21} = 1$:
    \begin{align*}
    x_1 + x_2 &= 2 &\\
    10^{-4} x_1 + x_2 &= 1 &\\
    \end{align*}
    ... $l_{21} = 10^{-4} \Rightarrow x_2 = 1, \medspace x_1 = 1$
\end{enumerate} 
\begin{description}
  \item[Erläuterung:] $\thinspace$ \\
    Falls $\vert l_{21} \vert$ groß ist, ergibt sich
    \begin{align*}
    a_{22}^{(1)} &= a_{22} - l_{21} a_{12} \approx l_{21} a_{12} &\\
    b_2^{(1)} &= b_2 - l_{21} b_1 \approx l_{21}b_1 &\\
    x_2 &= \frac{b_2^{(1)}}{a_{22}^{(1)}} \approx \frac{b_1}{a_{21}} &\\
    \end{align*}
    Bei der Berechnung von $x_1$ kommt es zu einer Stellenauslöschung $x_1 = {(b_1 - \underbrace{a_{12} x_2}_{b_1})}:{a_{11}} \approx 0$\\
    \underline{Ausweg:} Zeilentausch, sodass $\vert a_{21} \vert \leq \vert a_{11} \vert $. Dann ist $\vert l_{21} \vert \leq 1$.\\
    Spaltenpivotsuche: \\
    Nehme Pivotelement im $(k+1)$-ten Schritt
    $$a_{j(k+1)}^{(k)} \quad \text{mit} \quad \vert a_{j(k+1)}^{(k)} \vert = \max_{i=k+1,...,n} \vert a_{i(k+1)}^{(k)} \vert, $$
    d.h. das betragsmäßig größte Element der $(k+1)$-ten Spalte von $A^{(k)}$ unterhalb der Diagonalen inklusive des Diagonalelements. \\
%    \begin{figure}[!htb]
%    \centering
%    \includegraphics[width=6cm]{Bild4.png}
%    \end{figure}\\
    Damit erreicht man
    $$ \vert l_{i, (k+1)} \vert = \frac{\vert a_{i, k+1}^{(k)} \vert }{ \vert a_{k+1, k+1}^{(k)} \vert} \leq 1, \quad i=k+2,...,n$$
\end{description}
\end{example}

\subsection{Cholesky-Zerlegung für symmetrische positiv definite Matrizen}

\begin{definition}
Eine Matrix $A = (a_{ij})_{i,j=1}^n \in \mathbb{R}^{n\times n}$ heißt symmetrisch, falls $\forall i,j = 1,...,n: \medspace a_{ij} = a_{ji}$, d.h. $A = A^T$.\\
Eine Matrix $A$ ist positiv definit, falls 
$$\forall x \in \mathbb{R}^n: \medspace x^T Ax > 0$$
\end{definition}

\begin{theorem}
Sei $A$ symmtrisch positiv definit (kurz: spd), $A \in \mathbb{R}^{n\times n}$. Dann gilt:
\begin{enumerate}
  \item[i)] Die Gaußelimination kann ohne Zeilenvertauschungen durchgeführt werden
  \item[ii)] Für die Zerlegung $A=LR$ gilt $R = DL^T$ für eine Diagonalmatrix 
  $$D =
  \begin{bmatrix}
  r_1 && 0 \\
  & \ddots \\
  0 && r_{nn} \\
  \end{bmatrix}, \quad \text{wobei }\forall i = 1,...,n: \medspace r_{ii} > 0$$ 
\end{enumerate}
\begin{proof}[Beweis]
Es gilt: $a_{11} = e_{1}^T A e_1 > 0$, da $A$ spd, wobei $e_1 = (1,0,...,0)^T \in \mathbb{R}^n$ der 1. kanonische Basisvektor ist. Also ist $a_{11}$ ein möglicher Pivot. Schreibe $A$ nun als:
$$\left[
\begin{array}{c|c}
a_{11} & z^T \\
\hline
z & C \\
\end{array}
\right]$$
wobei $z = (a_{21}, ..., a_{n1})^T \in \mathbb{R}^{n-1}$ und $C$ eine symmetrische $(n-1)\times(n-1)$-Matrix ist. \\
Nun ist 
$$A^{(1)} = \left[
\begin{array}{c|c}
a_{11} & z^T \\
\hline
0 & C^{(1)} \\
\end{array}
\right]$$
$C^{(1)}$ ist symmetrisch, da 
$$c_{ij}^{(1)} = a_{ij} - \underbrace{\frac{a_{i1}}{a_{11}}}_{= l_{i1}} a_{1j} = a_{ji} - \underbrace{\frac{a_{j1}}{a_{11}}}_{= l_{j1}} a_{1i} = c_{ji}^{(1)}$$
Also ist $C^{(1)}$ insbesondere spd, da für 
$\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right) \neq 0 $ gilt:
$$ 0 < 
\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right)^T
A
\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right) 
=
\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right) 
\left[
\begin{array}{c|c}
a_{11} & z^T \\
\hline
z & C \\
\end{array}
\right]
\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right) 
= a_{11} x_1^2 + \underbrace{y^Tzx_1 + x_1 z^Ty}_{=2x_1y^Tz} +y^T Cy$$
Weiter ist:
$$ y^T C^{(1)} y = y^TCy - \frac{1}{a_{11}} y^Tzz^Ty = y^TCy - \frac{1}{a_{11}}(y^Tz)^2$$
Wählt man nun $x_1 = -\frac{y^Tz}{a_{11}}$, so gilt:
\begin{align*}
0 &< a_{11} \left(- \frac{y^Tz}{a_{11}} \right)^2 - 2 \frac{y^Tz}{a_{11}} y^Tz + y^TCy &\\
&= -\frac{(y^Tz)^2}{a_{11}} + y^TCy &\\
&= y^TC^{(1)}y
\end{align*}
für beliebiges $y \in \mathbb{R}^{n-1}\setminus \{0\}$. \\
Induktiv folgt dann $a_{22}^{(1)} > 0$ $C^{(2)}$ spd, ...\\\\
Zeige nun noch ii):\\
Es gilt 
$$l_{i1} = \frac{a_{i1}}{a_{11}} = \frac{a_{i1}}{r_{11}} = \frac{r_{1i}}{r_{11}}$$
da $r_{1i} = a_{1i} = a_{i1}$ für $i=2,...,n$.\\
Außerdem gilt
$$l_{i2} = \frac{a_{i2}}{a_{22}} = \frac{a_{i2}}{r_{22}} = \frac{r_{2i}}{r_{22}}$$
da $r_{2i} = a_{2i}^{(1)} = a_{i2}^{(1)}$ für $i=3,...,n$.\\
Allgemein gilt also
$$\forall i > j: \medspace l_{ij} = \frac{r_{ji}}{r_{jj}},$$
wobei $r_{ii} = a_{ii}^{(i-1)} > 0$ und $r_{ij} = l_{ij} * r_{jj} = r_{jj} * l_{ij}$, d.h. $R = DL^T$ für 
$$D =
  \begin{bmatrix}
  r_1 && 0 \\
  & \ddots \\
  0 && r_{nn}\\
  \end{bmatrix}
$$
Es wird die i-te Zeile von $L^T$ mit $r_{ii}$ skaliert.
\end{proof}
\end{theorem}

\begin{comment*}
Wegen $R_{ii} > 0$ ist $D = D^{\nicefrac{1}{2}}D^{\nicefrac{1}{2}}$ mit 
$$D^{\nicefrac{1}{2}} =
  \begin{bmatrix}
  \sqrt{r_1} && 0 \\
  & \ddots \\
  0 && \sqrt{r_{nn}} \\
  \end{bmatrix}
$$
Damit erhält man für $\tilde{L} = LD^{\nicefrac{1}{2}}$ (Spaltenskalierung)
$$A = LDL^T = LD^{\nicefrac{1}{2}}D^{\nicefrac{1}{2}}L^T = (LD^{\nicefrac{1}{2}})(LD^{\nicefrac{1}{2}})^T = \tilde{L}\tilde{L}^T$$
Wir bezeichnen die Elemente von $\tilde{L}$ wieder mit $l_{ij}$:
$$\tilde{L} =
  \begin{bmatrix}
  l_{11} && 0 \\
  \vdots& \ddots \\
  l_{1n} &\dots& l_{nn} \\
  \end{bmatrix}
$$
Diese $l_{ij}$'s lassen sich direkt aus der Gleichung $A = \tilde{L}\tilde{L}^T$ berechnen.
\begin{align*}
\begin{bmatrix}
  l_{11} && 0 \\
  \vdots& \ddots \\
  l_{1n} &\dots& l_{nn} \\
\end{bmatrix}
\begin{bmatrix}
  l_{11} &\dots& l_{n1} \\
  & \ddots &\vdots\\
  0 && l_{nn}\\
\end{bmatrix}
=
\begin{bmatrix}
  a_{11}&a_{12} \\
  a_{12}&a_{22}&a_{32} \\
  &&\ddots \\
  &&&a_{nn} \\
\end{bmatrix}
\end{align*}
Nun folgt:
\begin{align*}
&l_{11}^2 = a_{11} > 0 \quad \Rightarrow \quad l_{11} = \sqrt{a_{11}} &\\
&l_{11}l_{i1} = a_{i1} \quad \Rightarrow \quad l_{i1} = \frac{a_{i1}}{l_{11}} &\\
\intertext{allgemein gilt:}
&a_{kk} = l_{k1}^2 + l_{k2}^2 + ... + l_{kk-1}^2 + l_{kk}^2 &\\
&l_{kk} = \left( a_{kk} - \sum_{j=1}^{k-1}l_{kj}^2 \right) ^{\nicefrac{1}{2}}
\intertext{und für $i > k$:}
& a_{ik} = l_{i1}l_{k1} + l_{i2}l_{k2} + ... + l_{ik-1}l_{kk-1} + l_{ik}l_{kk} \\
&l_{ik} = \frac{\left(a_{ik} - \sum_{j=1}^{k-1} l_{ij}l_{kj} \right)}{l_{kk}}
\end{align*}
\underline{Choleski-Verfahren:}
\begin{algorithmic}
\FOR{$k = 1,...,n$}
  \STATE $l_{kk} = \left( a_{kk} - \sum_{j=1}^{k-1}l_{kj}^2 \right) ^{\nicefrac{1}{2}}$
  \FOR{$i=k+1,...,n$}
    \STATE $l_{ik} = \nicefrac{\left(a_{ik} - \sum_{j=1}^{k-1} l_{ij}l_{kj} \right)}{l_{kk}}
$
  \ENDFOR
\ENDFOR
\end{algorithmic}
Nun stellen sich folgende zwei Fragen:
\begin{itemize}
  \item Wie wirken sich Störungen in $A$ und $b$ auf die Lösung von $Ax=b$ aus?
  \item Wie wirken sich Rundungsfehler im Verfahren auf die berechnete Lösung aus?
\end{itemize}
\end{comment*}

\subsection{Matrixnormen}

\begin{definition}
Die Abbildung $\Vert \cdot \Vert: \mathbb{R}^n \rightarrow \mathbb{R}^n, \medspace x \mapsto \Vert x \Vert$ ist eine Norm auf dem $\mathbb{R}$- Vektorraum $\mathbb{R}^n$, falls gilt:
\begin{enumerate}
  \item[i)] $\forall x \in \mathbb{R}^n: \medspace \Vert x \Vert \geq 0$
  \item[ii)] $\Vert x \Vert = 0 \Rightarrow \medspace x = 0 \in \mathbb{R}^n$
  \item[iii)] $\forall \alpha \in \mathbb{R} \forall x \in \mathbb{R}^n: \medspace \Vert \alpha x \Vert = \vert \alpha \vert \Vert x \Vert $
  \item[iv)] $\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert $
\end{enumerate}
\end{definition}

\begin{example} \leavevmode
\begin{enumerate}
  \item[i)] $ \Vert x \Vert_1 = \sum_{j=1}^n \vert x_j \vert$ für $x=(x_1, ...,x_n)^T$
  \item[ii)] $ \Vert x \Vert_2 = \left( \sum_{j=1}^n \vert x_j \vert ^2 \right)^{\nicefrac{1}{2}}$ oder allgemein $\Vert x \Vert_p = \left( \sum_{j=1}^n \vert x_j \vert ^2p\right)^{\nicefrac{1}{p}}$ für $ 1\leq p < \infty$
  \item[iii)] $\Vert x \Vert_{\infty} = \max_{i=1,...,n} \vert x_i\vert$
\end{enumerate}
\end{example}

\begin{definition}
Sei $A\in \mathbb{R}^{m \times n}$, d.h. $A: \mathbb{R}^n \rightarrow \mathbb{R}^m$ linear. Nun heißt
$$\Vert A \Vert_{a \rightarrow b} := \sup_{x \notin \mathcal{O}_n, \thinspace x \in \mathbb{R}^n} \frac{\Vert Ax \Vert_a}{\Vert x\Vert_b}$$
die von den Vektorraumnormen induzierte Norm. Schreibe einfach nur $\Vert \cdot \Vert$.
\end{definition}

\begin{comment}
Sei $A \in \mathbb{R}^{n\times m}$, $\alpha \in \mathbb{R}$ Es gilt für die in (18.3) definierte Matrixnorm
\begin{enumerate}
  \item[i)] $\forall x \in \mathbb{R}^n: \medspace \Vert Ax \Vert \leq \Vert A \Vert \Vert x \Vert$ \\
  $\Vert A \Vert$ ist die kleinste Zahl mit dieser Eigenschaft.
  \item[ii)] Es gilt $\Vert A \Vert \geq 0$. Weiter gilt $\Vert A \Vert = 0 \Rightarrow \medspace A = 0$
  \item[iii)] $\Vert \alpha A \Vert = \vert \alpha \vert \Vert A \Vert$
  \item[iv)] $\Vert A + B \Vert \leq \Vert A \Vert + \Vert B \Vert$.\\
	Damit ist $\Vert \cdot \Vert$ tatsächlich eine Norm.
  \item[v)] $\Vert I \Vert = 1$ falls $m=n$, $\Vert \cdot \Vert_{\mathbb{R}^m} = \Vert \cdot \Vert_{\mathbb{R}^n}$
  \item[vi)] $\Vert AB \Vert \leq \Vert A \Vert \thinspace \Vert B \Vert$ (Submultiplikativität)
\end{enumerate}
\end{comment}

\begin{theorem}
Sei $A = (a_{ij})_{i,j=1}^{m,n} \in \mathbb{R}^{m \times n}$. Es gilt für $\Vert A \Vert_p = \sup_{x \neq 0} \frac{\Vert Ax \Vert_p}{\Vert x \Vert_p}$ für $p \geq 1$
\begin{enumerate}
  \item[i)] $\Vert A \Vert_1 = \max_{j=1,...,n} \sum_{i=1}^m \vert a_{ij} \vert$ ist die maximale Spaltenbetragssumme
  \item[ii)] $\Vert A \Vert_2$ ist die Wurzel des größten Eigenwerts von $A^TA$
  \item[iii)] $\Vert A \Vert_{\infty} = \max_{i=1,...,m} \sum_{j=1}^n \vert a_{ij} \vert$ ist die maximale Zeilenbetragssumme
\end{enumerate}
\begin{proof}[Beweis]\leavevmode
\begin{enumerate}
  \item[i) + iii)] Übungsaufgabe
  \item[ii)] $A^TA$ ist symmetrisch und positiv semidefinit. Es gilt nämlich 
  \begin{align*}
  &(A^TA)^T = A^T A^{T^T} = A^TA &\\
  \intertext{und}
  &x^TA^TAx = (Ax)^T Ax = {\Vert Ax \Vert_2}^2 \geq 0 &\\
  \end{align*} 
  Damit ist $A^TA$ orthogonal diagonalisierbar, d.h. es ex. $Q$ mit $Q^TQ = I$ sodass $Q^TA^TAQ = D$ mit 
  $$D = 
  \begin{bmatrix}
  \lambda_1 \\
  & \ddots \\
  && \lambda_m
  \end{bmatrix}$$
  wobei $\lambda_j \geq 0 \medspace(j=1,...,m)$ die Eigenwerte von $A^TA$ sind.\\
  Damit ist
  \begin{align*}
  {\Vert Ax \Vert_2}^2 &= x^TA^TAx \underset{x = Qy}{=} y^TQ^TA^TAQy = \sum_{j=1}^m \lambda_j y_j^2 \\
  & \leq \lambda_{\max} \sum_{j=1}^m y_j^2 = \lambda_{\max} y^Ty = \lambda_{\max} {\Vert y \Vert_2}^2 = \lambda_{\max} {\Vert x \Vert_2}^2
  \end{align*}
  $\Rightarrow \medspace \Vert A \Vert_2 \leq \sqrt{\lambda_{\max}}$ für den größten Eigenwert $\lambda_{\max}$ von $A^TA$.\\
  Sei $\tilde{x} = Q\tilde{y}$ mit $\tilde{y} = (0,...,0,\underset{\text{$j_0$-ter Eintrag}}{1},0,...,0)^T$ mit $\lambda_{j_0} = \lambda_{\max}$. Dann ist ${\Vert A\tilde{x} \Vert_2}^2 = \lambda_{\max} {\Vert \tilde{x} \Vert_2}^2 \Rightarrow \medspace \Vert A \Vert_2 = \sqrt{\lambda_{\max}}$.
\end{enumerate}
\end{proof}
\end{theorem}

\subsection{Kondition eines Problems}

\begin{definition}
Seien $X,Y$ normierte Vektorräume $(X, \Vert \cdot \Vert_X)$, $(Y, \Vert \cdot \Vert_Y)$. Ein Problem bzw. eine Problemstellung ist eine Abbildung $f:X \rightarrow Y$, wobei $X$ die Eingaben und $Y$ die Ausgaben enthält.
\end{definition}

\begin{example}
Sei $X = Y = \mathbb{R}^2$ und $\Vert \cdot \Vert_X = \Vert \cdot \Vert_Y = \Vert \cdot \Vert_2$.
\begin{enumerate}
  \item[i)] $f: (x_1, x_2) \mapsto A \left(\begin{matrix} x_1 \\x_2 \end{matrix} \right)$ für eine $2 \times 2$ Matrix $A$. "Anwendung der linearen Abbildung $A$"
  \item[ii)] $f: (p,q) \mapsto \text{Wurzeln von $z^2 + pz + q = 0$}$. "Berechnung der Wurzeln eines normierten quadratischen Polynoms"
\end{enumerate}
\end{example}

\begin{definition}[Absolute Kondition]
Seien $X, Y$ normierte Vektorräume $f: X \rightarrow Y$ ein Problem. Die \textbf{absolute Kondition} von $f$ in $x \in X$ ist 
$$\kappa_{\text{abs}}(f,x) := \lim_{\delta \rightarrow 0} \sup_{\Vert z \Vert_X \leq \delta} \frac{\Vert f(x+z) - f(x) \Vert_Y}{\Vert z \Vert_X} $$
%    \begin{figure}[!htb]
%    \centering
%    \includegraphics[width=16cm]{Bild5.png}
%    \end{figure}\\
\end{definition}

\begin{lemma}
Sei $f: (\mathbb{R}, \Vert \cdot \Vert) \rightarrow (\mathbb{R}, \Vert \cdot \Vert)$ differenzierbar, so gilt
$$\kappa_{\text{abs}}(f,x) = \vert f'(x) \vert$$
\begin{proof}[Beweis]
Übungsaufgabe.
\end{proof}
\end{lemma}

\begin{definition}[Relative Kondition]
Seien $X, Y$ normierte Räume, $f: X \rightarrow Y$ ein Problem. Die \textbf{relative Kondition} von $f$ in $x \in X$ ist
$$\kappa_{\text{rel}}(f,x) := \lim_{\delta \rightarrow 0} \sup_{\Vert z \Vert_X \leq \delta} \frac{\frac{\Vert f(x+z) - f(x) \Vert_Y}{\Vert f(x) \Vert_Y}}{\frac{\Vert z \Vert_X}{\Vert x \Vert_X}} $$
d.h. $\kappa_{\text{rel}}(f,x)$ ist die kleinste Zahl sodass 
$$\underbrace{\frac{\Vert f(x) - f(x+z) \Vert_Y}{\Vert f(x) \Vert_Y}}_{\underset{\text{statt f(x) das Problem f(x+z) gelöst hat}}{\text{relativer Fehler in der Ausgabe, wenn man}}} \leq \kappa_{\text{rel}}(f,x) \underbrace{\frac{\Vert z \Vert_X}{\Vert x \Vert_X}}_{\text{relativer Fehler in der Eingabe}}$$
\end{definition}

\begin{example}
Kondition der Addition:
\begin{align*}
&f: (\mathbb{R}^2, \Vert \cdot \Vert_1) \rightarrow (\mathbb{R}, \vert \cdot \vert), \medspace (a, b) \mapsto a+b &\\
&\kappa_{\text{abs}}(f,(a,b)) = \lim_{\delta \rightarrow 0} \sup_{\vert \alpha \vert + \vert \beta \vert \leq \delta, \medspace z = (\alpha, \beta)} \frac{\vert a + \alpha + b + \beta - a - b \vert}{\vert \alpha \vert + \vert \beta \vert} = 1 &\\
&\kappa_{\text{rel}}(f,(a,b)) = ... = \frac{\vert a \vert + \vert b \vert}{\vert a+b \vert} &\\
\end{align*}
d.h. für die Addition zweier Zahlen mit gleichem Vorzeichen ist $\kappa_{\text{rel}} = 1$. Für Subtraktion zweier annähernd gleich großer Zahlen ist $\kappa_{\text{rel}}$ groß.
\end{example}

\begin{definition}
Ein Problem heißt \textbf{gut konditioniert}, falls $\kappa_{\text{rel}}$ klein ist ($< 10^3$) und \textbf{schlecht konditioniert}, falls $\kappa_{\text{rel}}$ groß ist ($>10^8$).
\end{definition}

\subsection{Konditionszahl einer Matrix}
Gegeben sei $Ax = b$. Welchen Einfluss haben Fehler in $A$ und in $b$ auf die Lösung $x$?\\
In Form von $\S 19$: $f: (A, b) \mapsto x$. Statt $a_{ij}$ stehen nun $\tilde{a}_{ij}= a_{ij}(1+ \varepsilon_{ij})$ und statt $b_i$ nun $b_i(1+ \varepsilon_{i})$ zur Verfügung. Also $\tilde{A} \tilde{x} = \tilde{b}$. 

\begin{theorem}
Sei $A$ invertierbar, $A \in \mathbb{R}^{n \times n}$, $Ax = b$, $\tilde{A} \tilde{x} = \tilde{b}$, $x \neq 0$. \\
Falls 
$$ \frac{\Vert A - \tilde{A} \Vert}{\Vert A \Vert} \leq \varepsilon_A, \quad \frac{\Vert b - \tilde{b} \Vert}{\Vert b \Vert} \leq \varepsilon_b$$
so gilt für die Lösung des LGS:
$$\frac{\Vert \tilde{x} - x \Vert }{\Vert x \Vert} \leq \frac{\text{cond($A$)}}{1- \varepsilon_A \text{cond($A$)}}(\varepsilon_A + \varepsilon_b)$$
falls $\varepsilon_A *\text{cond($A$)} < 1$. Hierbei ist $\text{cond($A$)} = \Vert A \Vert \Vert A^{-1} \Vert$ die Konditionszahl von $A$ und die Matrixnorm wird von der Vektorraumnorm induziert (vgl. 18.3)
\begin{proof}[Beweis]
\begin{align*}
b - \tilde{b} &= Ax - \tilde{A}\tilde{x} &\\
&= Ax - A\tilde{x} + A\tilde{x} - \tilde{A}\tilde{x} &\\
&= A(x-\tilde{x}) + (A- \tilde{A})\tilde{x} &\\
\Rightarrow x- \tilde{x} &= A^{-1} (b -\tilde{b} - (A - \tilde{A})\tilde{x}) &\\
\Vert x - \tilde{x} \Vert & \leq \Vert A^{-1} \Vert ( \Vert b \Vert \varepsilon_b + \varepsilon_A \Vert A \Vert \Vert \tilde{x} \Vert ) &\\
& \underset{b=Ax}{\leq} \text{cond($A$)} ( \Vert x \Vert \varepsilon_b + \varepsilon_A (\Vert \tilde{x} - x \Vert + \Vert x \Vert) &\\
\Rightarrow (1- \text{cond($A$)} \varepsilon_A) \Vert - \tilde{x} \Vert & \leq \text{cond($A$)} \Vert x \Vert (\varepsilon_b + \varepsilon_A)
\end{align*}
\end{proof}
\end{theorem}

\begin{comment}
Die Abschätzung aus (20.1) ist scharf, d.h. es gibt $\tilde{A}$ und $\tilde{b}$, sodass Gleichheit gilt. Aber sie ist oft zu pessimistisch für Rundungsfehlerabschätzungen.
\begin{description}
  \item[Beispiel:] Betrachte folgendes Gleichungssystem: 
    \begin{align*}
    \underbrace{\left( \begin{matrix}
    1 & 1 \\
    0 & 10^{-8} \\
    \end{matrix} \right)}_A
    \left( \begin{matrix}x_1\\x_2\end{matrix} \right)
    =
    \left( \begin{matrix}b_1\\b_2\end{matrix} \right)
    \end{align*}
    Sei des Weiteren $\vert \varepsilon_j \vert < eps$. \\
    Es gilt $\text{cond}_{\infty}(A) = \Vert A \Vert_{\infty} \Vert A^{-1} \Vert_{\infty} = 2 * 10^8$. Das gestörte System ist nun:
    \begin{align*}
    (1+ \varepsilon_1) \tilde{x}_1 + (1 + \varepsilon_2) \tilde{x}_2 &= \overbrace{b_1 ( 1+ \varepsilon_3)}^{= \tilde{b}_1} &\\
    (1+\varepsilon_4) 10^{-8} \tilde{x}_2 &= b_2(1+\varepsilon_5)
    \end{align*}
    
    \begin{align*}
    \intertext{Mit $\frac{1}{1+\varepsilon} = 1 - \varepsilon + \varepsilon^2 - \varepsilon^3 + \varepsilon^4 - ...$ folgt}
    &\Rightarrow \tilde{x}_2 = 10^8 b_2 \frac{1+\varepsilon_5}{1+\varepsilon_4} = 10^8 b_2(1+\varepsilon_5 - \varepsilon_4) + \mathcal{O}(eps^2) &\\
    &\Rightarrow \frac{\vert x_2 - \tilde{x}_2 \vert}{\vert x_2 \vert} \leq 2 eps 
    \end{align*}
    
    \begin{alignat*}{2}
    &\tilde{x}_1 &&= [b_1(1+\varepsilon_3) - x_2(1+\varepsilon_5 - \varepsilon_4 + \varepsilon_3)] (1-\varepsilon_1) + \mathcal{O}(eps^2) \\
    & &&= [x_1 + \underbrace{b_1}_{=x_1 + x_2} \varepsilon_3 - x_2(\varepsilon_5 - \varepsilon_4 + \varepsilon_2)] (1-\varepsilon_1) + \mathcal{O}(eps^2)
    \end{alignat*}
    
    \begin{align*}
    &\tilde{x}_1-x_1 = x_1(-\varepsilon_1 + \varepsilon_3) - x_2(-\varepsilon_3 + \varepsilon_5 - \varepsilon_4 + \varepsilon_2) + \mathcal{O}(eps^2)&\\
    &\frac{\vert \tilde{x}_1 - x_1 \vert}{\vert x_1 \vert} \leq ( 2 + 4 \frac{\vert x_2 \vert}{\vert x_1 \vert} ) eps
    \end{align*}
    Dieser Wert kann sehr groß werden für $\frac{\vert x_2 \vert}{\vert x_1 \vert} \rightarrow \infty$, aber $\frac{\vert \tilde{x}_1 - x_1 \vert}{\Vert x_1 \Vert_{\infty}} \leq 6eps$.
\end{description} 
\end{comment}

\begin{lemma}
Sei $A \in \mathbb{R}^{n \times n}$ invertierbar. Dann gilt:
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
  \item $\text{cond}(A) \geq 1$
  \item $\forall \alpha \in \mathbb{R}\setminus \{0\}: \medspace \text{cond}(\alpha A) = \text{cond}(A)$
  \item $ \text{cond}(A) = \frac{\max_{\Vert y \Vert = 1} \Vert Ay \Vert}{\min_{\Vert x \Vert = 1} \Vert A^{-1}x \Vert}$
\end{enumerate}
\begin{proof}[Beweis]
Übungsaufgabe.
\end{proof}
\end{lemma}

\begin{example}\leavevmode
\renewcommand{\labelenumi}{\theenumi) }
\begin{enumerate}
  \item Matrizen mit kleiner Konditionszahl:
    \begin{itemize}
      \item $I$ mit $\text{cond}(I) = 1$
      \item orthogonale Matrizen $Q$ ($Q^TQ = I$)
        \begin{align*}
        &{\Vert Qx \Vert_2}^2 = x^TQ^T Q x = x^Tx = {\Vert x \Vert_2}^2 \Rightarrow \Vert Q \Vert_2 = 1 &\\
        & Q^{-1} = Q^T \Rightarrow \Vert Q^{-1} \Vert_2 = 1 \Rightarrow \text{cond}_2(Q) = 1
        \end{align*}
      \item Splineinterpolationsmatrix ($h_i = h$)
        \begin{align*}
        &A = \frac{1}{h} \left[ \begin{matrix} 4 & 1 & & 0 \\ 1 & \ddots & \ddots \\  & \ddots & & 1 \\ 0 & & 1 & 4\end{matrix} \right], \quad \Vert A \Vert_{\infty} = 6 &\\
        &A = 4(I+N) \quad \text{mit } N = \left[ \begin{matrix} 0 & \nicefrac{1}{4} & & 0 \\ \nicefrac{1}{4} & \ddots & \ddots \\  & \ddots & & \nicefrac{1}{4} \\ 0 & & \nicefrac{1}{4} & 0 \end{matrix} \right] &\\
        &A^{-1} = \nicefrac{1}{4}(I+N)^{-1} = \nicefrac{1}{4} (I-N+N^2-N^3 + ...), \quad \Vert N \Vert_{\infty} = \nicefrac{1}{2} &\\
        &\Vert A ^{-1} \Vert \leq \nicefrac{1}{4}(\Vert I \Vert + \Vert N \Vert + \Vert N \Vert ^2 + ...) = \nicefrac{1}{2}
        \end{align*}
    \end{itemize}
  
  \item Matrizen mit großer Konditionszahl: 
    \begin{itemize}
      \item Hilbertmatrix $H = \left(\frac{1}{i+j-1}\right)_{i,j=1}^n$
        $$ H = \left[ \begin{matrix}1 & \nicefrac{1}{2} & \nicefrac{1}{3} & \nicefrac{1}{4} & \dots \\ \nicefrac{1}{2} & \nicefrac{1}{3} & \nicefrac{1}{4} \\ \nicefrac{1}{3} & \nicefrac{1}{4} \\ \nicefrac{1}{4} \\ \vdots \end{matrix} \right]$$
        Für $n \in \{1,..., 10\}$ ergibt sich folgende Tabelle:\\
        \begin{tabular}{c|c}
        $n$ & $\text{cond}_2(A)$ \\
        \hline
        1 & 1 \\
        2 & 27 \\
        3 & 740 \\
        4 & 2300 \\
        \vdots & \vdots \\
        10 & $35 * 10^{13}$
        \end{tabular}
    \end{itemize}
\end{enumerate}
\end{example}

\subsection{Stabilität von Verfahren}
\begin{definition}
Ein Verfahren zur Auswertung eines Problems $f$ ist die Hintereinanderausführung von elementaren Operationen $\tilde{f}_k$ \\
$\tilde{f} = \tilde{f}_n \circ \tilde{f}_{n-1} \circ ... \circ \tilde{f}_1, \quad \tilde{f}_k \in \{ +, -, *, /, fl, \sqrt{\cdot}, ...\} $
\end{definition}

\begin{definition}
Ein Verfahren zur Auswertung des Problems $f$ ist stabil im Sinne der Vorwärtsanalysis, falls 
$$\tilde{f}(x) - f(x) \Vert < C * eps *  \Vert f(x) \Vert$$
für eine nicht zu große Konstante $C$.
\end{definition}

\begin{example}
Berechnung von $\frac{1}{x(x-1)}$ für $x = 10^4$.
\renewcommand{\labelenumi}{\theenumi) }
\begin{enumerate}
  \item
    \begin{tabular}{ccccccc}
    &\rotatebox[origin=r]{30}{$\rightarrow$} & $fl(x)$ & \rotatebox[origin=l]{-30}{$\rightarrow$} \\
    x &&&&$fl(x(x-1))$& $\rightarrow$ & $fl(\frac{1}{x(x-1)})$ \\
    &\rotatebox[origin=r]{-30}{$\rightarrow$} & $fl(x-1)$ & \rotatebox[origin=l]{30}{$\rightarrow$} 
    \end{tabular}
    
  \item
    \begin{tabular}{cccccccc}
    &\rotatebox[origin=r]{30}{$\rightarrow$} & $fl(x)$ & $\rightarrow$ & $fl(\frac{1}{x})$ & \rotatebox[origin=l]{-30}{$\rightarrow$} \\
    x &&&&&& $fl(-\frac{1}{x} + \frac{1}{x-1})$ \\
    &\rotatebox[origin=r]{-30}{$\rightarrow$} & $fl(x-1)$ & $\rightarrow$ & $fl(\frac{1}{x-1})$ & \rotatebox[origin=l]{30}{$\rightarrow$} 
    \end{tabular}
\end{enumerate}
Verfahren 2) ist nicht stabil, da $\frac{1}{x} \approx \frac{1}{x-1}$, falls $x=10^4$ gilt und die Subtraktion im letzten Schritt schlecht konditioniert ist. 
\end{example}

\begin{definition}
Ein Verfahren $\tilde{f}$ zur Auswertung eines Problems $f$ ist stabil im Sinne der Rückwärtsanalysis, falls für jedes $x \in X$ ein $\tilde{x} \in X$ existiert, sodass
$$ \tilde{f}(x) = f(\tilde{x}) \quad \text{mit } \frac{\Vert \tilde{x} - x \Vert }{\Vert x \Vert} \leq C*eps$$
für eine nicht zu große Konstante $C$. Die berechnete Lösung $\tilde{f}(x)$ kann als exakte Lösung eines benachbarten Problems $f(\tilde{x})$ aufgefasst werden.
\end{definition}

\begin{example}
Zur Berechnung von $x_1x_2 + x_3x_4$ verwendet man:\\
\begin{tabular}{ccccc}
&\rotatebox[origin=c]{30}{$\underrightarrow{\cdot}$} & $x_1x_2$ & \rotatebox[origin=l]{-30}{$\rightarrow$} \\
$(x_1, x_2, x_3, x_4)$ & & & + & $x_1x_2 + x_3x_4$ \\
&\rotatebox[origin=c]{-30}{$\overrightarrow{\cdot}$} & $x_1x_2$ & \rotatebox[origin=l]{30}{$\rightarrow$} \\
\end{tabular}\\
und erhält unter Berücksichtigung von Rundungsfehlern\\
$$\left[(x_1 ( 1 + \varepsilon_1) x_2(1+\varepsilon_2))(1+\eta_1) + (x_3(1+\varepsilon_3)x_4(1+\varepsilon_4))(1+\eta_2)\right](1+\eta_3) \quad \text{für $\vert \varepsilon_j \vert, \vert \eta_j \vert \leq eps$}$$
Das ist das exakte Ergebnis für 
\begin{flalign*}
&\tilde{x}_1 = x_1(1+ \varepsilon_1)(1+\eta_1)(1+\eta_3)&\\
&\tilde{x}_2 = x_2(1+ \varepsilon_2)(1+\eta_1)(1+\eta_3)&\\
&\tilde{x}_3 = x_3(1+ \varepsilon_3)(1+\eta_2)(1+\eta_3)&\\
&\tilde{x}_4 = x_4(1+ \varepsilon_4)(1+\eta_2)(1+\eta_3)
\end{flalign*}
Die Konstante $C$ in (21.4) ist etwa 3, wenn man Produkte von $\varepsilon_j$ und $\eta_j$ vernachlässigt. Das Verfahren ist also rückwärtsstabil, auch wenn evtl. das Problem schlecht konditioniert ist.
\end{example}

\begin{theorem}[Stabilität der Gaußelimination (LR-Zerlegung)]
Sei $A \in \mathbb{R}^{n \times n}$ invertierbar und $\hat{L}\hat{R}$ das rundungsfehlerbehaftete Ergebnis der Gaußelimination mit Pivotisierung, sodass $\vert \hat{l}_{ij} \vert \leq 1$ für alle $i,j \in \{1,..., n\}$. \\
Dann gilt für $\hat{A} = (\hat{a}_{ij})_{i,j=1}^n = \hat{L}\hat{R}$:
$$\vert a_{ij} - \hat{a}_{ij} \vert \leq 2 \max_{i,j,k} \vert a_{ij}^{(k)} \vert * \min\{i-1, j\} * eps$$
für Maschinengenauigkeit $eps$.
\begin{proof}[Beweis]
Im k-ten Schritt berechnet man ausgehend von $\hat{a}_{ij}^{(k-1)}$
\begin{align*}
\hat{a}_{ij}^{(k)} &= \left( \hat{a}_{ij}^{(k-1)} - \hat{l}_{ik} \hat{a}_{kj}^{(k-1)} ( 1+ \varepsilon_{ijk}) \right) ( 1+ \eta_{ijk}) &\\
&= \hat{a}_{ij}^{(k-1)} - \hat{l}_{ik} \hat{a}_{kj}^{(k-1)} + \mu_{ijk} \quad (*)
\end{align*}
mit $\vert \varepsilon_{ijk} \vert, \vert \eta_{ijk} \vert \leq eps$ und $\mu_{ijk} \leq \vert \hat{a}_{ij}^{(k-1)} \vert \vert \eta_{ijk} \vert + \vert \hat{l}_{ik} \vert \vert \hat{a}_{kj}^{(k-1)} \vert \vert \varepsilon_{ijk} \vert + \mathcal{O}(eps^2). \quad (**)$\\
Nach Definition von $\hat{A}$ ist 
$$\hat{a}_{ij} = \sum_{k=1}^{\min\{i,j\}} \hat{l}_{ik} \hat{r}_{kj} = \sum_{k=1}^{\min\{i,j\}} \hat{l}_{ik} \hat{a}_{kj}^{(k-1)}$$ 
Verwendet man für $i > j$ (*), so erhält man
$$\hat{a}_{ij} = \sum_{k=1}^j \left( \hat{a}_{ij}^{(k-1)} - \hat{a}_{ij}^{(k)} + \mu_{ijk} \right) = a_{ij} + \sum_{k=1}^j \mu_{ijk}, \quad \text{da } a_{ij}^{(j)} = 0$$
Für $i \leq j $ erhält man
$$\hat{a}_{ij} = \sum_{k=1}^{i-1} \left( \hat{a}_{ij}^{(k-1)} - \hat{a}_{ij}^{(k)} + \mu_{ijk} \right) + \hat{l}_{ii} a_{ij}^{(i-1)}= a_{ij} + \sum_{k=1}^{i-1} \mu_{ijk}, \quad \text{da } \hat{l}_{ii} = 1$$
Zusammen mit (**) folgt die Behauptung.
\end{proof}
\end{theorem}

\begin{comment*}
Aus dem Satz (21.6) kann man entnehmen, dass die Gaußelimination im Sinne der Rückwärtsanalysis stabil ist, falls 
$$\frac{\max_{i,j,k} \vert a_{ij}^{(k)}}{\max_{i,j} \vert a_{ij} \vert }$$
nicht zu groß wird. Dieser Quotient ist meistens klein. Mehr kann man nicht beweisen.
\end{comment*}

\subsection{QR-Zerlegung mit Hilfe der Householdertransformationen}
\underline{Ziel:} Konstruiere zu einer gegebenen Matrix $A \in \mathbb{R}^{m \times n}$ ($m \geq n$) eine Zerlegung $A=QR$ mit einer orthogonalen Matrix $Q \in \mathbb{R}^{m \times m}$ ($Q^TQ = I_m$) und 
$$R = \underbrace{\left[ \begin{matrix} \tilde{R} \\ \hline 0\end{matrix} \right]}_{n}\begin{array}{l} \}n \\ \} m-n\end{array}$$
Dabei ist $\tilde{R} \in \mathbb{R}^{n \times n}$ eine obere Dreiecksmatrix.\\
\underline{Anwendungen:}
\renewcommand{\labelenumi}{\alph{enumi})}
\begin{enumerate}
  \item $m=n$, $Ax = b$, $Qc = b$, $Rx=c$, $Q^{-1} = Q^T$. Besonders stabiler Algorithmus (stabiler als Gauß). Dafür doppelt so teuer.
  \item $m>n$ $\rightarrow$ lineare Ausgleichsrechnung (siehe \S 23)
  \item QR-Algorithmus zur Berechnung von Eigenwerten (siehe Numerik II)
\end{enumerate}

\begin{definition}
Für $v \in \mathbb{R}^m$, $\Vert v \Vert_2 = 1$ heißt
$$Q = I - 2 v v^T$$
\textbf{Householderreflexion} zum Vektor $v$.
\end{definition}

\begin{theorem}
Für eine Householderreflexion $v \in \mathbb{R}^m$, $\Vert v \Vert_2 = 1$, $Q = I - 2 v v^T$ gilt:
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
  \item $Q$ ist symmetrisch
  \item $Q$ ist orthogonal
  \item $Qv = -v$
  \item $Qw = w$ für alle $w \in \mathbb{R}^{m}$ mit $w^Tv = 0$
\end{enumerate}
Mit iii) und iv) erhält man, dass $Q$ eine Spiegelung an der Hyperebene $\{x\in \mathbb{R}^m: x^Tv = 0 \}$ ist.
\begin{proof}[Beweis]\leavevmode
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
  \item $Q^T = (I-2vv^T)^T = I-2vv^T = Q$
  \item $Q^TQ \overset{i)}{=} (I-2vv^T)(I-2vv^T) = I - 4 v v^T + t v \underbrace{(v^Tv)}_{=1}v^T = I$ 
  \item $Qv = (I-2vv^T)v = v - 2v \underbrace{v^Tv}_{=1} = - v$
  \item $Qw = (I-2vv^T)w = w - 2v\underbrace{v^Tw}_{=0} = w$
\end{enumerate}
\end{proof}
\end{theorem}

\begin{nothing}[Algorithmus der QR-Zerlegung]
Sei $A = \left[ \begin{matrix} \vdots & \vdots & & \vdots \\ a_1 & a_2 & \dots & a_n \\ \vdots & \vdots & & \vdots \end{matrix} \right]$ mit $a_j$ als die j-te Spalte von $A$.
\renewcommand{\labelenumi}{\theenumi) }
\begin{enumerate}
  \item Suche $Q_1a_1 = \left[ \begin{matrix} \alpha_1 \\ 0 \\ \vdots \\ 0 \end{matrix} \right] = \alpha_1 e_1$\\
  Da $Q_1$ orthogonal ist, gilt ${\Vert a_1 \Vert_2}^2 = {\Vert Q_1a_1 \Vert_2}^2 = \vert \alpha_1 \vert ^2$\\
  $\Rightarrow \quad \alpha_1 = \pm {\Vert a_1 \Vert _2}$ (Vorzeichen noch nicht fest)\\
  $Q_1a_1 = \alpha_1e_1$ \\
  $Q_1a_1 = (I-2u_1u_1^T)a_1 = a_1 - 2u_1 \underbrace{u_1^Ta_1}_{\in \mathbb{R}}$\\
  $\Rightarrow$ $u_1$ ist ein Vielfaches von $a_1 - \alpha_1 e_1$\\
  Mit der Forderung $\Vert u_1 \Vert_2. = 1 $ ergibt sich
  $$u_1 = \frac{a_1 - \alpha_1e_1}{\Vert a_1 - \alpha_1e_1 \Vert_2}$$
  Dabei ist ${\Vert a_1 - \alpha_1e_1 \Vert_2}^2 = \underbrace{{\Vert a_1 \Vert_2}^2}_{=\alpha_1^2} - 2 \alpha_1 \underbrace{e_1^Ta_1}_{a_{11}} + \alpha_1^2 = 2 \alpha_1 ( \alpha_1 - a_{11})$
  Man wählt das Vorzeichen von $\alpha_1$ so, dass keine Stellenauslöschung bei der Berechnung von $\alpha_1 - a_{11}$ auftritt
  $$\alpha_1 = -\text{sgn}(a_{11}) \Vert a_1 \Vert_2$$
  Es ist dann 
  $$Q_1A= A^{(1)} = \left[ \begin{array}{c|c} \alpha_1 & * \\ \hline 0 \\ \vdots & B \\ 0 \end{array} \right]$$
  wobei die j-te Spalte von $A^{(1)}$ (Nenne diese $a_j^{(1)}$) durch
  $$a_j^{(1)} = Q_1a_j = a_j - \frac{2 v_1^Ta_j}{v_1^Tv_1} v_1, \quad j = 2,..,n$$
  für $v_1 = a_1 - \alpha_1e_1$ gegeben ist. Weiter gilt
  $$ \frac{v_1^Tv_1}{2} = \frac{1}{2} {\Vert a_1 - \alpha_1 e_1 \Vert_2}^2 = \alpha_1(\alpha_1 - a_{11})$$
  Bemerkung: Zur Berechnung von $Q_1A$ reicht es $v_1$ und $\alpha$ zu kennen. Die Matrix $Q_1 = I-2u_1u_1^T$ wird nicht aufgestellt.
  
  \item Suche nun $\tilde{Q}_2 = I_{m-1} - 2 u_2 u_2^T$ mit $u_2 \in \mathbb{R}^{m-1}$, sodass $\tilde{Q}_2 b_1 = \alpha_2 e_1$ ($e_1 \in \mathbb{R}^{m-1}$).\\
  Für $Q_2 = \left[ \begin{matrix} 1 & 0 & \cdots & 0 \\ 0 \\ \vdots & & \tilde{Q}_2 \\0 \end{matrix} \right]$ ist dann $Q_2A^{(1)} = \left[ \begin{array}{cc|} \alpha_1 & *  \\ 0 & \alpha_2 \\ \vdots & 0 \\ & \vdots \\ 0 & 0 \end{array} \begin{array}{ccc} &* \\ \\ \hline \\  &C &\\ \\ \\ \end{array}\right]$\\
  usw.\\
  im k-ten Schritt hat man dann
  $$Q_k = \left[ \begin{matrix} I_{k-1} \\ & \tilde{Q}_k \end{matrix} \right]$$
  mit dem Ergebnis $Q_n Q_{n-1} \cdots Q_1 A = R$ mit 
  $$R = \underbrace{\left[ \begin{matrix} \alpha_1 & & * \\ & \ddots \\ 0 && \alpha_n \\ \\ & 0 \end{matrix} \right]}_n\begin{matrix} \left. \begin{matrix} \\\\\\\end{matrix} \right\} &n \\ \left. \begin{matrix} \\\\\end{matrix} \right\} & m-n\end{matrix}$$
\end{enumerate}
\end{nothing}

\begin{nothing}[Rechenaufwand] \leavevmode
\begin{description}
  \item[Schritt 1:] $\approx \medspace 2*m*n$ (*,+) Operationen
\end{description}
Gesamtaufwand: $2 (m*n + (m-1)(n-1) \cdots (m-n+1) 1 )$\\
Falls $m=n \medspace \rightarrow \medspace \frac{2}{3}\thinspace n^3$\\
Falls $m>>n \medspace \rightarrow \medspace 2m ( n + (n-1) + \cdots + 1 ) \approx mn^2)$
\end{nothing}

\begin{nothing}[Stabilität]
$A=QR$\\
Da $Q$ orthogonal ist, gilt $\Vert A \Vert_2 = \Vert QR \Vert_2 = \Vert R \Vert_2$\\
$\Rightarrow$ Einträge von $R$ können nicht groß werden. \\
Für das berechnete $\hat{Q}$ gilt $\Vert \hat{Q}^T \hat{Q} - I \Vert < c * eps$ für eine kleine Konstante $c$, d.h. $\hat{Q}$ ist fast orthogonal.\\
Man kann zeigen, dass 
$$\Vert A - \hat{Q} \hat{R} \Vert_2 < c\thinspace \Vert A \Vert_2 \thinspace eps$$
\end{nothing}

\begin{comment}
QR-Zerlegung ist bis zum Ende durchführbar, falls rang($A$) $= n$ ( dann ist $\alpha_1, \alpha_2, ..., \alpha_n \neq 0$, da rang(A) = rang(R)) \\
Falls rang(A) $= k < n$ wäre bei der Rechnung ohne Rundungsfehler $\alpha_l = 0$ für ein $l$ und das Verfahren bricht ab. Modifiziere den Algorithmus deswegen:
\begin{description}
  \item[1. Schritt:] Berechne $\Vert a_1 \Vert_2, ..., \Vert a_n \Vert_2$ die Spaltennormen und vertausche die Spalten, sodass $\Vert a_1 \Vert_2$ maximal wird:
  $$Q_1AP_1 = \left[ \begin{matrix} \alpha_1 \\ 0 \\ \vdots && * & \\ 0 \end{matrix} \right], \quad \vert \alpha_1 \vert = \Vert a_1 \Vert_2$$
  usw. mit Spaltenvertauschungen in weiteren Schritten.\\
  $\Rightarrow \medspace \vert \alpha_1 \vert \geq \vert \alpha_2 \vert \geq ... \geq \vert \alpha_n \vert > 0$. Falls rang(A) $=k < n$ erhält man $\alpha_{k+1} = 0$ und
  $$\left[ \begin{array}{c|c}
    \begin{matrix} \alpha_1 & & R_1 \\ & \ddots \\ 0 && \alpha_k \end{matrix}& R_2 \\
    \hline
    0 & 0
  \end{array}\right]$$
  $AP = QR$ (numerische Rangentscheidung) \\
  falls $\frac{\vert \alpha_{k+1} \vert}{\vert \alpha_1 \vert } < 100 eps$ setze Rang($A$) $= k$.
\end{description}
\end{comment}

\subsection{Lineare Ausgleichsrechnung}

\begin{problem}
Zu gegebenen Messdaten $(t_j, y_j)$ $j=1,...,m$ suche $y = f(t)$
$$f(t) = \sum_{i=1}^n x_i \varphi_i(t),$$
sodass $y_j \approx f(t_j)$ für $j=1,...,m$.\\
Hierbei sind die $\varphi_1, ..., \varphi_n$ gegebene Funktionen und $x_1, ..., x_n$ unbekannte Parameter, wobei $m>>n$.\\
Genauer: $\sum_{j=1}^m (y_j - f(t_j))^2$ soll minimal werden.
\end{problem}

\begin{nothing}[Matrix-Vektor Formulierung]
Es sei 
\begin{align*}
A &= \left[ \begin{matrix} \varphi_1(t_1) & \cdots & \varphi_n(t_1) \\ \varphi_1(t_2) & \cdots & \varphi_n(t_2) \\ \vdots && \vdots \\ \varphi_1(t_m) & \cdots & \varphi_n(t_m) \end{matrix} \right] \in \mathbb{R}^{m \times n} &\\
b &= \left[ \begin{matrix} y_1 \\ \vdots \\ y_n \end{matrix} \right] \in \mathbb{R}^n 
\intertext{Setze}
x &= \left[ \begin{matrix} x_1 \\ \vdots \\ x_n \end{matrix} \right] &\\
\intertext{Nun gilt es $\Vert Ax - b \Vert_2$ zu minimieren}
Ax = b & \Leftrightarrow 
\left[ \begin{matrix} \varphi_1(t_1) & \cdots & \varphi_n(t_1) \\ \varphi_1(t_2) & \cdots & \varphi_n(t_2) \\ \vdots && \vdots \\ \varphi_1(t_m) & \cdots & \varphi_n(t_m) \end{matrix} \right] 
\left[ \begin{matrix} x_1 \\ \vdots \\ x_n \end{matrix} \right] = 
\left[ \begin{matrix} y_1 \\ \vdots \\ y_n \end{matrix} \right] 
\end{align*}
\end{nothing}

\begin{example}[Ausgleichsproblem] \leavevmode
\includegraphics[width=6cm]{Bild6.png}\\
Wähle $n=2$ mit $\varphi_1(t) = 1$ und $\varphi_2(t) = t$. Dann ergibt sich
$$f(t) = x_2t + x_1.$$
Bezeichne nun $x_2$ mit $m$ und $x_1$ mit $c$, um die übliche Geradedarstellung zu erhalten:
$$f(t) = mt + c,$$
wobei $c$ der y-Achsenabschnitt und $m$ die Steigung ist.
Dann ergibt sich:
\begin{align*}
&Ax = b \quad \Leftrightarrow &\\
&\left[ \begin{matrix} t_1 & 1 \\ t_2 & 1 \\ \vdots & \vdots \\ t_m & 1 \end{matrix} \right] \left[ \begin{matrix} m \\ c \end{matrix} \right] = \left[ \begin{matrix} y_1 \\ \vdots \\ y_m \end{matrix} \right]
\end{align*}
\end{example}

\begin{theorem}[von Gauß]
Sei $A \in \mathbb{R}^{m \times n}$, $b \in \mathbb{R}^m$, $m \geq n$. Äquivalent sind:
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
  \item $\Vert Ax - b \Vert_2 = \min_{v \in \mathbb{R}^n} \Vert Av - b \Vert_2$
  \item $A^TAx = A^Tb$ "Normalengleichung"
\end{enumerate}

\begin{comment*}
$V = \{Ax: x \in \mathbb{R}^n \} \leq \mathbb{R}^n$ Unterraum\\
Das gesuchte $Ax$ ist die orthogonale Projektion von $b$ in $V$.\\
\includegraphics[width=6cm]{Bild7.png}
\end{comment*}
\begin{proof}[Beweis]\leavevmode \\
i) $\Leftrightarrow$ \\
Für beliebiges $y \in \mathbb{R}^n$ gilt:
\begin{align*}
{\Vert Ax - b \Vert_2}^2 &\leq {\Vert A(x+y) - b \Vert_2}^2 &\\
&= (A(x+y)-b)^T(a(x+y)-b) &\\
&= (A(x-b)+Ay)^T((Ax-b) + Ay) &\\
&= {\Vert Ax - b \Vert_2}^2 + 2\underbrace{(Ay)^T (Ax-b)}_{=0} + \underbrace{{\Vert Ay \Vert_2}^2}_{\geq 0} 
\end{align*}
$\Leftrightarrow$ $(Ay)^T(Ax-b) = 0$ für alle $y \in \mathbb{R}^n$ \\
$\Leftrightarrow$ $Ax-b$ ist orthogonal auf $V$ \\
$\Leftrightarrow$ $y^TA^TAx -y^TA^Tb = 0$ für alle $y \in \mathbb{R}^n$ \\
$\Leftrightarrow$ $A^TAx - A^Tb = 0$ 
\end{proof}
\end{theorem}

\begin{properties}\leavevmode
\begin{itemize}
  \item $A^TA \in \mathbb{R}^{n \times n}$ symmetrisch positiv semidefinit [$\forall x \in \mathbb{R}^n: {\Vert Ax \Vert_2}^2 = x^TA^TAx \geq 0$]
  \item $A^TA$ ist positiv definit $\Leftrightarrow$ Rang($A$) $= n$  [$x^TA^TAx = {\Vert Ax \Vert_2}^2 = 0 \Leftrightarrow Ax = 0 \overset{\text{falls Rang($A$) $=n$}}{ \Leftrightarrow} x = 0$]
\end{itemize}
\end{properties}

\begin{algorithm}\leavevmode
\begin{description}
  \item[1. Algorithmus] \leavevmode 
    \begin{algorithmic}
    \STATE Berechne $A^TA$ ($\frac{1}{2}mn^2$ Operationen)
    \STATE und $A^Tb$ ($mn$ Operationen)
    \STATE Löse $A^TAx = A^Tb$ mit der Choleskyzerlegung ($\frac{1}{6} n^3$ Operationen)
    \end{algorithmic}
    
  \item[2. Algorithmus] \leavevmode
    \begin{algorithmic}
    \STATE Berechne QR-Zerlegung von $A$ ($mn^2$ Operationen)
    \STATE Nun gilt $A = QR$ und damit lässt sich ${\Vert Ax-b \Vert_2}^2 = {\Vert QRx-b \Vert_2}^2 = {\Vert Rx - Q^Tb \Vert_2}^2 = {\Vert \tilde{R}x - c \Vert_2}^2 + {\Vert d \Vert_2}^2$ mit $R = \left[ \begin{matrix} \tilde{R} \\ 0 \end{matrix} \right]$ und $Q^Tb = \left[ \begin{matrix} c \\ d \end{matrix} \right]$ umschreiben.
    \STATE Dabei ist $\tilde{R} = (r_{ij})_{i,j=1}^n$ eine quadratische $n\times n$ rechte obere Dreiecksmatrix und $c$ der Vektor aus den ersten $n$ Einträgen von $Q^Tb$.
    \STATE $\Vert Ax-b \Vert_2 = \min! \Leftrightarrow \tilde{R}x = c$, dann ist ${\Vert Ax-b \Vert_2}^2 = {\Vert d \Vert_2}^2$
    \STATE Berechne $Q^Tb = Q_n Q_{n-1}...Q_1b$ ($2nm$ Operationen)
    \STATE Löse $\tilde{R}x = c$ ($\frac{1}{2}n^2$ Operationen)
    \end{algorithmic}
\end{description}
Der 2. Algorithmus mit der QR-Zerlegung ist etwa doppelt so teuer wie der 1. Algorithmus dafür aber deutlich stabiler.
\end{algorithm}

\begin{example}
Sei $A = \left[ \begin{matrix} 1 & 1 \\ \varepsilon & 0 \\ 0 & \varepsilon \end{matrix} \right]$, $b= \left[ \begin{matrix} 1 \\ 0 \\ 0 \end{matrix} \right]$ und $\varepsilon^2 < eps$.\\
Es gilt $A^TA = \left[ \begin{matrix} 1+ \varepsilon^2 & 1 \\ 1 & 1 + \varepsilon^2 \end{matrix} \right]$ und $A^Tb = \left[ \begin{matrix} 1 \\ 1 \end{matrix} \right]$.\\
Die exakte Lösung ist $x_1 = x_2 = \frac{1}{2+\varepsilon^2} \approx \frac{1}{2}$ \\
In Gleitkommaarithmetik ist $A^TA = \left[ \begin{matrix} 1 & 1 \\ 1 & 1 \end{matrix} \right]$ singulär\\
Die QR-Zerlegung mit Householder (in Gleitkommaarithmetik) liefert das exakte Ergebnis:
$\alpha_1 = 1$, $v_1 = \left[ \begin{matrix} 2 \\ \varepsilon \\ 0 \end{matrix} \right]$, $R = \left[ \begin{matrix} -1 & -1 \\ 0 & \sqrt{2} \varepsilon \\ 0 & 0 \end{matrix} \right]$, $Q^Tb = \left[ \begin{matrix} 1 \\ \frac{\sqrt{2}}{2} \varepsilon \\ \frac{\sqrt{2}}{2} \varepsilon \end{matrix} \right]$
\begin{description}
  \item[Alg. 1:] Lösung von $A^TAx = A^Tb$ $\text{cond}_2(A^TA) = \text{cond}_2(A)^2 \geq \text{cond}_2(A) = \frac{\max_y \Vert Ay \Vert}{\min_z \Vert Az \Vert}$ Der abschließende Bruch funktioniert auch für nicht inv. Matrizen.
  
  \item[Alg. 2:] Lösung von $\tilde{R}x = c$, $\text{cond}_2(\tilde{R}) = \text{cond}_2(R) = \text{cond}_2(A)$
\end{description}
\end{example}

\section{Nichtlineare Gleichungssysteme}
\underline{Problemstellung:}\\
Zu einer Funktion $f: U \subset \mathbb{R}^n \rightarrow \mathbb{R}^n$, $U$ offen, suche $x \in U$ mit $f(x) = 0$, d.h. $\left\{ \begin{matrix} f_1(x_1, ..., x_n) = 0 \\ f_2(x_1, ..., x_n) = 0 \\ \vdots \\ f_n(x_1, ...,x_n) = 0 \end{matrix} \right.$.\\
Eventuell ex. keine Lösungen
$$f(x) = e^x$$
oder es ex. mehrere Lösungen
$$f(x) = x^2-1 \quad \text{oder} \quad f(x) = tan(x) - x$$
%
\underline{\textbf{Erinnerung/Wiederholung:}}
%
\begin{description}
  \item[Definition:] \leavevmode \\
  Sei $\Omega \subset \mathbb{R}^n$ abgeschlossen. Eine Abbildung $\Phi: \Omega \rightarrow \mathbb{R}^n$ heißt \underline{kontrahierend}, falls
  $$ \Vert \Phi(x) - \Phi(y) \Vert \leq \Theta \Vert x-y \Vert \quad \text{für } \Theta \in (0,1) \text{ und alle } x, y \in \Omega.$$
  Eine Abbildung heißt \underline{Selbstabbildung}, falls $\Phi(x) \in \Omega$ für alle $x \in \Omega$.
  
  \item[Satz: (Spezialfall von BFS)] \leavevmode \\
  Sei $\Omega \subset \mathbb{R}^n$ abgeschlossen, $\Phi: \Omega \rightarrow \Omega$ eine kontrahierende Selbstabbildung. Dann gilt:
  \renewcommand{\labelenumi}{\roman{enumi})}
  \begin{enumerate}
    \item Es existiert ein Fixpunkt $X^*$ von $\Phi$, d.h. $\Phi(X^*) = X^*$
    \item Für alle $x^{(0)} \in \Omega$ konvergiert die Folge $(x^{(k)})_{k \in \mathbb{N}}$ definiert durch $x^{(k+1)} = \Phi(x^{(k)})$ gegen $x^*$ mit 
    \begin{align*}
      &\Vert x^{(k+1)} - x^{(k)} \Vert \leq L \Vert x^{(k)} - x^{(k-1)} \Vert \quad \text{und} \quad \text{(a priori Schranke)}  &\\
      &\Vert x^* - x^{(k)} \Vert \leq \frac{L^k}{1-L} \Vert x^{(1)} - x^{(k)} \Vert \quad \text{für ein } L < 1 \quad \text{(a posteriori Schranke)}
    \end{align*}
  \end{enumerate}
  \begin{proof}[Beweis] 
  Ana II
  \end{proof}
  
  \item[Bemerkung:] \leavevmode \\
  $f(x) \overset{!}{=} 0$ \\
  Kann man das nichtlineare Gleichungssystem $f(x) = 0$ in eine äquivalente Fixpunktgleichung umwandeln?\\
  $x = \Phi(x)$ \\
  Mit Hilfe der Fixpunktgleichung konstruiert man eine Folge $(x_n)_n$ ausgehend von $x_0$ durch $x_{n+1} = \Phi(x_n)$, die hoffentlich gegen $x^*$ mit $x^* = \Phi(x^*)$ konvergiert.
  
  \item[Beispiel:] \leavevmode \\
  $f(x) = 2x - \tan(x) \overset{!}{=} 0$ \\
  Fixpunktgleichungen:\\
  $x = \frac{1}{2} \tan(x) \medspace \Rightarrow \medspace \Phi_1(x) = \frac{1}{2} \tan(x)$ \\
  $x = \arctan(2x) \medspace \Rightarrow \medspace \Phi_2(x) = \arctan(2x)$ 
  $$\left( \begin{array}{l} 2x - \tan(x) - x = -x \\ x = \tan(x) -x \end{array} \right)$$
\end{description}

\subsection{Newton-Verfahren}
\begin{nothing}[Illustration]
\includegraphics[height=5cm]{Bild8.png}
\includegraphics[height=5cm]{Bild9.png}\\
Startwert $x_0$, $x_1$ Schnitt der Tangente von $f$ im Punkt $(x_0, f(x_0))$ mit der x-Achse, $x_2$ Schnitt der Tangente in $(x_1, f(x_1))$ mit x-Achse, usw.
\end{nothing}

\begin{nothing}[Herleitung Newton-Verfahren]
Sei $x_0$ in der Nähe einer Nullstelle $x^*$ von $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$. Taylorentwicklung liefert 
\begin{align*}
0 = f(x^*) = f(x_0 + (x^* - x_0)) = f(x_0) + f'(x_0)(x^* - x_0) + \mathcal{O}(\Vert x^* - x_0 \Vert ^2)
\end{align*}
Dabei ist $f'(x_0)$ die Jacobimatrix an der Stelle $x_0$. \\
Näherungsweise gilt damit, falls $f'(x_0)$ invertierbar ist
\begin{align*}
&x^* - x_0 \approx - f'(x_0)^{-1} f(x_0)
\intertext{Setze nun}
&x_1 = x_0 - f'(x_0)^{-1} f(x_0)
\end{align*}
\underline{Gewöhnliches Newtonverfahren:}
\begin{algorithmic}
\STATE $x_0$ ist gegeben
\FOR{$k = 0,1,...$}
\STATE Löse $f'(x_k)\Delta x_k = -f(x_k)$ LGS (z.B. mit LR-Zerlegung)
\STATE $x_{k+1} = x_k + \Delta x_k$
\ENDFOR
\end{algorithmic}
\end{nothing}

\begin{theorem}
Sei $f$ dreimal stetig differenzierbar, $f(x^*) = 0$, $f'$ invertierbar in einer Umgebung von $x^*$ und die Folge $(x_k)_k$ definiert durch das gewöhnliche Newtonverfahren. Dann gilt für den Fehler $e_k = x_k - x^*$
$$e_k = \frac{1}{2} f'(x_k)^{-1} f''(x_k)[e_k, e_k] + \mathcal{O}( \Vert e_k \Vert ^3)$$
Insbesondere gilt $\Vert e_{k+1} \Vert \leq C \Vert e_k \Vert ^2$, d.h. das Newtonverfahren konvergiert quadratische (Ordnung 2), falls $\Vert e_k \Vert$ genügend klein ist.\\
Dabei ist 
$$f''(x) [y,z] = \sum_{k=1}^n z_k \sum_{j=1}^n \frac{\delta^2}{\delta_{x_k} \delta_{x_j}} f(x) y_j \medspace \in \mathbb{R}^n$$ 
\begin{proof}[Beweis]\leavevmode \\
\begin{align*}
0 &= f(x^*) = f(x_k - e_k) &\\
&\overset{\text{Taylor}}{=} f(x_k) - f'(x)_ke_k + \frac{1}{2} f''(x_k) [e_k, e_k] + \mathcal{O}(\Vert e_k \Vert ^3) &\\
&= -f''(x_k) (x_{k+1} + x^* - x^* - x_k) - f'(x_k) e_k + \frac{1}{2} f''(x_k) [e_k, e_k] + \mathcal{O}(\Vert e_k \Vert ^3) &\\
&= -f'(x_k) (e_{k+1}) + \frac{1}{2} f''(x_k)[e_k, e_k] + \mathcal{O}(\Vert e_k \Vert ^3) &\\
\Rightarrow \medspace &e_{k+1} = \frac{1}{2} f'(x_k)^{-1} f''(x_k)[e_k, e_k] + \mathcal{O}(\Vert e_k \Vert ^3) &\\
\Rightarrow \medspace &\Vert e_{k+1} \Vert \leq \frac{1}{2} C \Vert e_k \Vert ^2
\end{align*}
\end{proof}
\end{theorem}

\begin{definition}
Eine Folge $(x_k)_k$ konvergiert mit Ordnung $p$ für $p \geq 1$ gegen $x^*$ falls ein $C \geq 0$ exisitert mit
$$ \Vert x_{k+1} - x^* \Vert \leq C \Vert x_k - x^* \Vert ^p,$$
wobei $C < 1$, falls $p=1$.
\end{definition}

\begin{theorem}[Newton Mysovskii]
$f \in \mathcal{C}^1(D, \mathbb{R}^n)$, $D \subset \mathbb{R}^n$ offene Teilmenge, $f'(x)$ invertierbar für bel. $x \in D$ und es gelte:
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
  \item $\Vert \Delta x_0 \Vert \leq \alpha$ (Definition von $\alpha$)
  \item $\Vert f'(x)^{-1} (f'(y) - f'(z)) (y-z) \Vert \leq \omega \Vert y-z \Vert ^2$ (Definiton von $\omega$) für bel. $x, z \in D$ und festes $y \in \overline{xz}$
  \item $y := \frac{1}{2} \alpha \omega < 1$
  \item Für $\rho := \frac{\alpha}{1-y}$ ist $B_{\rho}(x_0) \subset D$
\end{enumerate}
Dann gilt für die Folge der Iteration des Newtonverfahrens $(x_k)_k$:
\begin{itemize}
  \item $(x_k)_k \subset B_{\rho}(x_o)$
  \item $x_k \rightarrow x^*$ für $k \rightarrow \infty$ mit $f(x^*) = 0$, genauer:
  \item $\Vert x_{k+1} - x_k \Vert \leq \frac{\omega}{2} \Vert x_k - x_{k-1} \Vert^2$
\end{itemize}
Hierbei ist $\overline{xz}$ die Strecke zwischen $x$ und $z$ und 
$B_{\rho}(x_0) = \{x \in \mathbb{R}^n, \Vert x-x_0 \Vert < \rho \}$ die offene Kugel um $x_0$ mit Radius $\rho$.
\begin{proof}[Beweis] mühsam \end{proof}
\end{theorem}

\begin{nothing}[Praktische Durchführung]\leavevmode
\begin{algorithmic}
\STATE $x_0$ sei gegeben.
\FOR{$k= 0,1,2,...$}
  \STATE Löse $f'(x_k) \Delta x_k = -f(x_k)$ mit LR-Zerlegung
  \STATE $x_{k+1} = x_k + \Delta x_k$
  \IF{$ \Vert \Delta_k \Vert \leq \text{TOL}$ or $k \geq k_{\max}$}
    \STATE $x_{k+1}$ ist die Lösung
    \STATE Warnung, falls $k \geq k_{\max}$
  \ENDIF
\ENDFOR
\end{algorithmic}
$\Vert f(x_k) \Vert \leq \text{TOL}$ ist \textbf{kein} geeignetes Abbruchkriterium. Ersetzt man die nichtlineare Gleichung $f(x) = 0$ durch $\tilde{f}(x) = Af(x) = 0$ für $A$ invertierbare Matrix, so ändern sich die Iterierten nicht. \\
$f \mapsto Af \medspace \Rightarrow \medspace f'(x)^{-1}f(x) \mapsto f'(x)^{-1}A^{-1}Af(x)$ \\
Man sagt, das Newtonverfahren ist \underline{affin invariant}. Deswegen sollte sich auch das Abbruchkriterium nicht ändern. \\
$\Vert f(x) \Vert \mapsto \Vert af(x) \Vert$ statt $\Vert x_k \Vert \leq \text{TOL}$ oft $\Vert x_k \Vert \leq \frac{\text{TOL}}{1- \frac{\Vert \Delta_k \Vert}{\Vert \Delta_k - 1 \Vert}}$
\end{nothing}

\begin{nothing}[Vereinfachtes Newtonverfahren] \leavevmode
\begin{description}
  \item[Algorithmus:]\leavevmode
  \begin{algorithmic}
  \STATE $A \approx f'(x_0)$ ($LR = A$)
  \FOR{$k=0,1,...$}
    \STATE Löse $A \Delta x_k = -f(x_k)$ (mit LR von A)
    \STATE $x_{k+1} = x_k + \Delta x_k$
  \ENDFOR
  \end{algorithmic}
  
  \item[Satz 1:]\leavevmode\\
  Sei $f \in \mathcal{C}^2(D, \mathbb{R}^n)$, $f(x^*) = 0$ und $A$ invertierbar. Dann gilt für $e_k = x_k - x^*$
  $$e_{k+1} = (I-A^{-1}f'(x_k)) e_k + \mathcal{O}( \Vert e_k \Vert^2)$$
  \begin{proof}[Beweis]
  \begin{align*}
  0 &= f(x^*) = f(x_k-e_k) \overset{\text{Taylor}}{=} f(x_0) - f'(x_k)e_k + \mathcal{O}(\Vert e_k \Vert^2) &\\
  f(x_k) &= -A( x_{k+1} - x^* + x^* - x_k) = -A(e_{k+1} - e_k) &\\
  0 &= -A(e_{k+1} - e_k) - f'(x_k)e_k + \mathcal{O}(\Vert e_k \Vert^2) &\\
  \Rightarrow \medspace &Ae_{k+1} = (A-f'(x_k))e_k + \mathcal{O}(\Vert e_k \Vert^2)
  \end{align*}
  \end{proof}
  
  \item[Satz 2:] \leavevmode \\
  Sei $f \in \mathcal{C}^1(D, \mathbb{R}^n)$, $A$ invertierbar, $x_0 \in D$ mit 
  \renewcommand{\labelenumi}{\roman{enumi})}
  \begin{enumerate}
    \item $\Vert \Delta x_0 \Vert \leq \alpha$
    \item $\Vert I - A^{-1}f'(x) \Vert \leq y < 1$ für bel. $x \in D$
    \item $B_{\rho}(x_0) \subset D$ mit $\rho = \frac{\alpha}{1-y}$
  \end{enumerate}
  Dann konvergiert $x_k$ aus dem Algorithmus gegen $x^*$ mit $f(x^*) = 0$ $\Vert x_{k+1} -x_k \Vert \leq y \Vert x_k - x_{k-1} \Vert$, d.h. das vereinfachte Newtonverfahren konvergiert lokal linear.
  \begin{proof}[Beweis]
  Das vereinfachte Newtonverfahren ist Fixpunktiteration zu $\Phi(x) = x-A^{-1}f(x)$
  \begin{itemize}
    \item $\Phi$ ist kontrahierend wegen ii)
    \item $\Phi$ ist eine Selbstabbildung
  \end{itemize}
  $\Rightarrow$ BFS liefert die Behauptung.
  \end{proof}
\end{description}
\end{nothing}

\section{Gewöhnliche Differentialgleichungen}
\underline{Problem:}\\
Suche Lösung  $y: [t_0, T] \rightarrow \mathbb{R}^d$ der Anfangswertaufgabe
$$\frac{d}{dt} y(t) = y'(t) = f(t, y(t)) \quad \text{für $t \in (t_0, T)$}$$
d.h. eine Funktion, die die obige Gleichung erfüllt. \\
$f: \mathcal{U} \rightarrow \mathbb{R}^d$, $\mathcal{U} \subset \mathbb{R}\times\mathbb{R}^d$ offen, $y(t_0) = y_0$ Anfangswert, $(t_0, y_0) \in \mathcal{U}$.

\subsection{Beispiele für gewöhnliche Differentialgleichungen}
\begin{example}[Harmonischer Oszillator] \leavevmode
\begin{align*}
\frac{d}{dt}p(t) &= -q(t) &\\
\frac{d}{dt}q(t) &= p(t)
\end{align*}
Dabei ist $\frac{d}{dt}p(t)$ die Änderungsrate zur Zeit $t$ von $p$, \\
$q(t)$ die Position zur Zeit $t$ und \\
$p(t)$ die Geschwindigkeit zur Zeit $t$.
\end{example}

\begin{example}[Pendel]
\begin{align*}
&ms''(t) = -mg\sin(\phi(t)), \quad s(t) = l\phi(t) &\\
&\phi''(t) = - \frac{g}{l} \sin(\phi(t)) &\\
&y(t) := \left( \begin{matrix} \phi(t) \\ \phi'(t) \end{matrix} \right), \quad y'(t) = \underbrace{\left( \begin{matrix} \phi'(t) \\ -\frac{g}{l}\sin(\phi(t)) \end{matrix} \right)}_{= f(t, y(t))}
\end{align*}
Dabei ist $\phi(t)$ der Winkel zur Zeit $t$, $s(t)$ die Position zur Zeit $t$, $l$ die Länge des Pendels, $m$ die Masse und $g$ die Erdbeschleunigung.
\end{example}

\begin{example}[Chemische Reaktionen]
Hier möchte man den Verlauf chemischer Reaktionen simulieren. Weiß man etwa, dass die Substanzen $A, B, C$ gemäß
\begin{align*}
A &\overset{k_1}{\rightarrow} B &\\
B + C &\overset{k_2}{\rightarrow} A + C &\\
B + B &\overset{k_3}{\rightarrow} B + C
\end{align*}
mit Reaktionskonstanten $k_1, k_2, k_3$ reagieren, dann liefert das Massenwirkungsgesetz für die Konzentrationen $a(t), b(t), c(t)$ der Substanzen $A, B, C$ zur Zeit $t$.
\begin{align*}
&a' = -k_1a + k_2 bc &\\
&b' = k_1a - k_2 bc - k_3 b^2 &\\
&c' = k_3b^2
\end{align*}
Zusätzlich müssen Anfangskonzentrationen $a(0), b(0)$ und $c(0)$ gegeben sein.
\end{example}

\begin{example}[Räuber Beute Modell]
Die Anzahl $y(t)$ von Speisefischen zur Zeit $t$ und die Anzahl $z(t)$ von Raubfischen mit Hilfe des Populationsmodells
\begin{align*}
&y' = ay - byz &\\
&z' = -cz
\end{align*}
berechnet werden.\\
Hierbei ist $a$ die Geburtenrate der Speisefische, $b$ die Effizienz der Raubfische, $c$ die Sterberate der Raubfische und $d$ die nahrungsabhängige Geburtenrate der Raubfische.
\end{example}

\subsection{Erinnerung an die Theorie gewöhnlicher DGLs}

\begin{comment}
Jeder Differentialgleichung $k$-ter Ordnung
$$y^{(k)} = f(t, y, y',..., y^{(k-1)})$$
kann in ein System erster Ordnung umgeschrieben werden:\\
Mit der Setzung
\begin{alignat*}{3}
y_1 &= y \quad &&y_1' &&= y_2 \\
y_2 &= y' \quad  &&y_2' &&= y_3 \\
&\medspace\medspace\vdots  &&&&\medspace\medspace\vdots \\
y_{k-1} &= y^{(k-2)} \quad\quad &&y_{k-1}' &&= y_k \\
y_k &= y^{(k-1)} \quad &&y_k' && = f(t, y_1, ..., y_k) \\
\end{alignat*}
erhält man für $Y = \left[ \begin{matrix} y_1 \\ \vdots \\ y_k \end{matrix} \right]$das System $Y' = F(t, Y)$, wenn man $F$ durch $F(t, Y) = \left[ \begin{matrix} y_2 \\ y_3 \\ \vdots \\ y_k \\ f(t, y_1) \end{matrix} \right]$ setzt. 
\end{comment}

\begin{definition}
Hängt die rechte Seite $f$ nicht explizit von $t$ ab, so heißt die Differentialgleichung autonom.
\end{definition}

\begin{comment*}
Jede nichtautonome Differentialgleichung 
$$y' = f(t, y) \quad y(t_0) = y_0$$
ist äquivalent zu einem autonomen System
$$Y' = F(Y) \text{ mit } Y = \left[\begin{matrix}y \\ t \end{matrix} \right] \text{ und } F(Y) = \left[ \begin{matrix} f(t,y) \\ 1 \end{matrix} \right]$$
\end{comment*}

\begin{comment*}[Allgemeine Vorraussetzungen]
Als nächstes wiederholen wir einige theoretische Resultate zur Existenz, Eindeutigkeit und Stabilität von Lösungen von Anfangswertproblemen. Dazu sei ab jetzt
\begin{itemize}
  \item $U \subset \mathbb{R} \times \mathbb{R}^d$ offen und zusammenhängend
  \item $f: U \rightarrow \mathbb{R}^d$ stetig und erfülle folgende lokale Libschit-Bedingung:
  $$\exists L \thinspace \forall K \subset U \text{ kompakt } \forall (t, y), (t,z) \in K: \Vert f(t, y) - f(t,z) \Vert \leq L \Vert y- z \Vert$$
\end{itemize}
Die lokale Lipschitz-Bedingung ist erfüllt, falls $f$ stetig differenzierbar ist. Dann kann $L = \max_{(t, y) \in K} \Vert f_y(t, y) \Vert$ gewählt werden. 
\end{comment*}

\begin{theorem}[Satz von Picard-Lindelöf zur lokalen Existenz und Eindeutigkeit)]
Unter obigen Vorraussetzungen gilt: Es gibt ein offenes Intervall $I$ mit $t_0 \in I$, sodass genau eine Lösung $y: I \rightarrow \mathbb{R}^d$ exisitert, mit
$$y'(t) = f(t, y(t)), \text{ für $t\in I$ und $y(t_0)=y_0$}$$
Diese Lösung kann bis an den Rand von $U$ fortgesetzt werden.
\begin{proof}[Beweis] Ana II \end{proof}
\end{theorem}

\begin{example}
Die rechte Seite der Differentialgleichung
$$y' = y^2, \quad y(0) = 1, \quad U = \mathbb{R} \times \mathbb{R}$$
ist lokal Lipschitz-stetig, erfüllt also die Vorraussetzungen von Picard-Lindelöf. Die eindeutige Lösung $y(t) = (1-t)^{-1}$ existiert auf dem offenen Intervall $I = (-\infty, 1)$ und $t_0 = 0 \in I$. Es ist jedoch $\lim_{t \nearrow 1} y(t) = + \infty$. 
\end{example}

Für numerische Verfahren ist es wichtig wie sich Störungen der Anfangswerte auf die Lösung auswirken.

\begin{theorem}
Zusätzlich zu den allgemeinen Vorraussetzungen erfülle für $\langle \cdot, \cdot \rangle$ ein Skalarprodukt auf $\mathbb{C}^d$ und $\Vert \cdot \Vert$ die induzierte Norm die rechte Seite $f: [t_0, T] \rightarrow \mathbb{C}^d$ $f$ für ein $l$ folgende einseitige Lipschitz-Bedingung
$$\Re \langle f(t, y) - f(t, z), y-z \rangle \leq l \Vert y-z \Vert^2 \quad \text{für alle } y, z \in U $$
Sind $y$ und $z$ zwei Lösungen von $y' = f(t, y)$ zu verschiedenen Anfangswerten $y_0$ bzw. $z_0$, so gilt
$$ \Vert y(t) - z(t) \Vert \leq e^{l(t-t_0)} \Vert y_0 - z_0 \Vert \quad \text{für alle } t \in [t_0, T].$$
%
\begin{proof}[Beweis]
\begin{align*}
\frac{d}{dt} \Vert y(t) - z(t) \Vert ^2 &= 2 \Re \langle y'(t) - z'(t), y(t)-z(t) \rangle \\
&= 2 \Re \langle f(t, y(t)) - f(t, z(t)), y(t) - z(t)\rangle \\
&\leq 2l \Vert y(t) - z(t) \Vert ^2
\end{align*}
Falls $y(t_0) \neq z(t_0)$ so gilt wegen der Eindeutigkeit der Lösung auch $y(t) \neq z(t)$ für alle $t$.
\begin{align*}
\intertext{Mit}
& \varphi(t) := \Vert y(t) - z(t) \Vert^2 \neq 0
\intertext{erhalten wir}
&\frac{\varphi'(t)}{\varphi(t)} = \frac{d}{dt} \log \varphi(t) \leq 2l.
\intertext{Integration ergibt}
&\log(\varphi (t)) - \log( \varphi(t_0)) = \int_{t_0}^t \frac{d}{ds} \log(\varphi(s))ds \leq \int_{t_0}^t 2lds = 2l(t-t_0)  \\
& \Rightarrow \log \left(\frac{\varphi(t)}{\varphi(t_0)}\right) \leq 2l(t-t_0)
\end{align*}
"Exponieren" liefert $\phi(t) \leq e^{2l(t-t_0)} \varphi(t_0)$
\end{proof}
\end{theorem}

\begin{comment}\leavevmode
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
  \item Fehler in den Anfangsdaten können maximal mit dem Faktor $e^{l(t-t_0)}$ verstärkt werden.
  \item Da $f$ lokal Libschitz-stetig ist, ist die Vorraussetzung des Satzes mit $l = L$ erfüllt. Für das bestmögliche (kleinste) $l$ kann aber $l<<L$ gelten.
  \item $l<0$ ist möglich, hingegen ist immer $L>0$
\end{enumerate}
\end{comment}

\begin{example}[Testgleichung]\leavevmode
$$ y' = \lambda y, \quad \lambda \in \mathbb{C}, \quad y(t_0) = y_0$$
Lösung
$$ y(t) = e^{(t-t_0)\lambda}y_0$$
$l = \Re \lambda$, $L = \vert \lambda \vert$
\begin{itemize}
  \item $\Re \lambda < 0$: Fehler werden gedämpft 0, ist asymptotisch stabil
  \item $\Re \lambda = 0$: keine Fehlerverstärkung
  \item $\Re \lambda > 0$: Fehler wachsen exponentiell
\end{itemize}
\end{example}

\subsection{Euler-Verfahren}
Einfachstes und ältestes Verfahren zur näherungsweisen Lösung von
$$y'(t) = f(t, y(t)), \quad y(t_0) = y_0$$
Idee: Ersetze lokal die (unbekannte) Lösung durch die bekannte Tangente an der Stelle $t_0$, so erhält man $y_1 = y_0 + hf(t_0,y_0)$, usw. \\
Allgemeine Iterationsvorschrit (explizites Eulerverfahren):\\
$t_n = t_0 + nh$ \\
$y_i = y_{i-1} + hf(t_{i-1}, y_{i-1})$ für $i\in \mathbb{N}$.\\
\\
Ersetzt man lokal die (unbekannte) Lösung durch die Tangente an der ebenfalls unbekannten Stelle $(t_1, y_1)$, so erhält man $y_1 = y_0 + hf(t_1, y_1)$, usw.\\
Allgemein Iterationsvorschrift (implizites Eulerverfahren):\\
$t_n = t_0 + nh$ \\
$y_i = y_{i-1} + hf(t_{i}, y_{i})$ für $i\in \mathbb{N}$.\\
\\
Hierbei muss in jedem Schritt ein nicht-lineares Gleichungssystem gelöst werden (etwa mit Newton-Verfahren oder Fixpunktiteration).\\
\\
\underline{Approximationsfehler beim expliziten Eulerverfahren:}\\
Sei $I = [t_0, T]$ ein Intervall und $f: I \times \mathbb{R}^d \rightarrow \mathbb{R}^d$ stetig differenzierbar und (global) Lipschit-stetig, d.h.:
$$\forall y, z \in \mathbb{R}^d \medspace \forall t \in I:
\Vert f(t, y) - f(t, z) \Vert \leq L \Vert y-z \Vert$$
Ist $y: I \rightarrow \mathbb{R}^d$ Lösung des Anfangswertproblems $y' = f(t, y), y(t_0) = y_0$, dann ist $y$ jetzt zweimal stetig differenzierbar, denn
$$y'' = \delta_tf + D_yf \cdot y' = \delta_tf + D_yf \cdot f$$
$D_yf = D_yf(t, y)$ bezeichnet die Ableitung ($d \times d$ Matrix) nach y. Die Folge $(y_n)_{n \in \mathbb{N}}$ ist mit $t_n = t_0 + nh \in I$ durch das explizite Eulerverfahren definiert.

\begin{theorem}[Fehlerabschätzung für das explizite Eulerverfahren]
Mit den eben gemachten Vorraussetzungen gilt für den Fehler des expliziten Euler-Verfahrens
$$\Vert y_n - y(t_n) \Vert \leq M \cdot h,$$
mit
$$ M = \frac{e^{L(T-t_0)} - 1}{L} \frac{1}{2} \max_{t \in I} \Vert y''(t) \Vert $$
Insbesondere gilt
$$\lim_{h \rightarrow 0} \max_{n \in \mathbb{N}} \Vert y_n - y(t_n) \Vert = 0,$$
d.h. die Näherungslösung konvergiert gleichmäßig gegen die exakte Lösung der Anfangswertaufgabe, falls $h$ gegen Null geht.

\begin{proof}[Beweis]\leavevmode
Beweis erfolgt in 3 Schritten:
\renewcommand{\labelenumi}{\theenumi)}
\begin{enumerate}
  \item Abschätzung für den lokalen Fehler:
  \item Fehlerfortpflanzung
  \item Fehlerakkumulation
\end{enumerate}
\renewcommand{\labelenumi}{\theenumi)}
\begin{enumerate}
  \item Abschätzung für den lokalen Fehler\\
  Fehler nach einem Schritt des Verfahrens mit Startwert auf der exakten Lösung:
  \begin{align*}
  \underbrace{y(t_{n+1})}_{\substack{\text{exakte Lsg.}\\ \text{bei $t_{n+1}$}}} - 
  \underbrace{(y(t_n) + hf(t_n, y(t_n)))}_{\substack{\text{expl. Eulerverfahren mit} \\ \text{Startwert $y(t_n)$}}} &= y(t_{n+1}) - y(t_n) - hy'(t_n) \\
  \overset{\substack{\text{Taylorentwicklung}\\\text{von $y(t_{n+1})$ im $t_n$}}}{=}& h^2 \int_0^1 (1- \theta) y'' (t_n + \theta h) d\theta \\
  \Rightarrow  \Vert y(t_{n+1}) - (y(t_n) + hf(t_n, y(t_n))) \Vert \leq Ch^2
  \end{align*}
  für $C = \frac{1}{2} \max_{t \in I} \Vert y''(t) \Vert$.
  \item Fehlerfortpflanzung
  \item Fehlerakkumulation
\end{enumerate}
\end{proof}
\end{theorem}
%entklammere Graphiken mit \includegraphics wieder 
\end{document}
